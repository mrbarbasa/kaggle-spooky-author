_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 300)          8329800   
_________________________________________________________________
spatial_dropout1d_1 (Spatial (None, 128, 300)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128, 128)          187392    
_________________________________________________________________
spatial_dropout1d_2 (Spatial (None, 128, 128)          0         
_________________________________________________________________
bidirectional_2 (Bidirection (None, 128, 128)          99328     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 128)               0         
_________________________________________________________________
output_layer (Dense)         (None, 3)                 387       
=================================================================
Total params: 8,616,907
Trainable params: 287,107
Non-trainable params: 8,329,800
_________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.63297; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.63297 to 0.60044; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.60044 to 0.55912; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.55912 to 0.53752; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.53752 to 0.52158; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.52158 to 0.49313; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.49313 to 0.46468; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.46468 to 0.46094; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.46094 to 0.43771; runtime 0:00:03; BEST YET
Epoch 010: val_loss did not improve from 0.43771; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.43771; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.43771; runtime 0:00:04
Fold 1 training runtime: 0:00:44

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.75      0.92      0.82       790
        HPL       0.95      0.67      0.79       564
        MWS       0.84      0.81      0.82       605

avg / total       0.83      0.81      0.81      1959

            ----- Confusion Matrix -----
True Labels  EAP  [727  10  53]
             HPL  [144 379  41]
             MWS  [104  12 489]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.61605; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.61605 to 0.55282; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.55282 to 0.54065; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54065 to 0.49682; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.49682 to 0.48777; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.48777 to 0.44212; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.44212 to 0.43138; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.43138 to 0.42858; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.42858 to 0.39834; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.39834; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.39834; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.39834; runtime 0:00:04
Fold 2 training runtime: 0:00:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.77      0.91      0.84       790
        HPL       0.90      0.82      0.86       564
        MWS       0.88      0.75      0.81       605

avg / total       0.84      0.83      0.83      1959

            ----- Confusion Matrix -----
True Labels  EAP  [720  30  40]
             HPL  [ 82 460  22]
             MWS  [130  22 453]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.62427; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.62427 to 0.58074; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.58074 to 0.56544; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.56544 to 0.53420; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.53420 to 0.50615; runtime 0:00:04; BEST YET
Epoch 006: val_loss did not improve from 0.50615; runtime 0:00:04
Epoch 007: val_loss improved from 0.50615 to 0.49709; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.49709 to 0.48914; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.48914 to 0.47441; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.47441; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.47441; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.47441; runtime 0:00:04
Fold 3 training runtime: 0:00:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.87      0.82       790
        HPL       0.86      0.77      0.81       564
        MWS       0.83      0.78      0.80       605

avg / total       0.82      0.82      0.82      1959

            ----- Confusion Matrix -----
True Labels  EAP  [687  46  57]
             HPL  [ 85 437  42]
             MWS  [104  27 474]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.60871; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.60871 to 0.56035; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.56035 to 0.54524; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54524 to 0.50126; runtime 0:00:04; BEST YET
Epoch 005: val_loss did not improve from 0.50126; runtime 0:00:04
Epoch 006: val_loss improved from 0.50126 to 0.45728; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.45728 to 0.45190; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.45190 to 0.44543; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.44543 to 0.42328; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.42328 to 0.40547; runtime 0:00:04; BEST YET
Epoch 011: val_loss improved from 0.40547 to 0.39750; runtime 0:00:04; BEST YET
Epoch 012: val_loss did not improve from 0.39750; runtime 0:00:04
Epoch 013: val_loss improved from 0.39750 to 0.38483; runtime 0:00:04; BEST YET
Epoch 014: val_loss did not improve from 0.38483; runtime 0:00:04
Epoch 015: val_loss did not improve from 0.38483; runtime 0:00:04
Epoch 016: val_loss did not improve from 0.38483; runtime 0:00:04
Fold 4 training runtime: 0:01:00

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.87      0.81      0.84       790
        HPL       0.84      0.86      0.85       564
        MWS       0.82      0.88      0.85       605

avg / total       0.85      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [636  69  85]
             HPL  [ 45 483  36]
             MWS  [ 49  22 534]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.58098; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.58098 to 0.54170; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.54170 to 0.51328; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.51328 to 0.47849; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.47849 to 0.46173; runtime 0:00:04; BEST YET
Epoch 006: val_loss did not improve from 0.46173; runtime 0:00:04
Epoch 007: val_loss improved from 0.46173 to 0.43132; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.43132 to 0.43089; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.43089 to 0.40321; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.40321; runtime 0:00:04
Epoch 011: val_loss improved from 0.40321 to 0.39019; runtime 0:00:04; BEST YET
Epoch 012: val_loss did not improve from 0.39019; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.39019; runtime 0:00:04
Epoch 014: val_loss did not improve from 0.39019; runtime 0:00:04
Fold 5 training runtime: 0:00:52

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.86      0.81      0.84       790
        HPL       0.90      0.87      0.88       564
        MWS       0.80      0.89      0.84       604

avg / total       0.85      0.85      0.85      1958

            ----- Confusion Matrix -----
True Labels  EAP  [639  44 107]
             HPL  [ 48 488  28]
             MWS  [ 52  12 540]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.63454; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.63454 to 0.57972; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.57972 to 0.55209; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.55209 to 0.54316; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.54316 to 0.50845; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.50845 to 0.48714; runtime 0:00:04; BEST YET
Epoch 007: val_loss did not improve from 0.48714; runtime 0:00:04
Epoch 008: val_loss improved from 0.48714 to 0.45759; runtime 0:00:04; BEST YET
Epoch 009: val_loss did not improve from 0.45759; runtime 0:00:04
Epoch 010: val_loss improved from 0.45759 to 0.45424; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.45424; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.45424; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.45424; runtime 0:00:04
Fold 6 training runtime: 0:00:48

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.83      0.82       790
        HPL       0.89      0.80      0.84       563
        MWS       0.80      0.85      0.83       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [659  39  92]
             HPL  [ 77 449  37]
             MWS  [ 74  14 516]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.63844; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.63844 to 0.60294; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.60294 to 0.56591; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.56591 to 0.55462; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.55462 to 0.51734; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.51734 to 0.50326; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.50326 to 0.48704; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.48704 to 0.47548; runtime 0:00:04; BEST YET
Epoch 009: val_loss did not improve from 0.47548; runtime 0:00:04
Epoch 010: val_loss improved from 0.47548 to 0.46750; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.46750; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.46750; runtime 0:00:04
Epoch 013: val_loss improved from 0.46750 to 0.46669; runtime 0:00:04; BEST YET
Epoch 014: val_loss did not improve from 0.46669; runtime 0:00:04
Epoch 015: val_loss did not improve from 0.46669; runtime 0:00:04
Epoch 016: val_loss did not improve from 0.46669; runtime 0:00:04
Fold 7 training runtime: 0:01:00

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.87      0.84       790
        HPL       0.84      0.83      0.84       563
        MWS       0.85      0.79      0.82       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [686  48  56]
             HPL  [ 71 466  26]
             MWS  [ 88  39 477]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.61382; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.61382 to 0.57535; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.57535 to 0.55924; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.55924 to 0.50638; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.50638 to 0.49834; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.49834 to 0.48268; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.48268 to 0.45428; runtime 0:00:04; BEST YET
Epoch 008: val_loss did not improve from 0.45428; runtime 0:00:04
Epoch 009: val_loss improved from 0.45428 to 0.42977; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.42977; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.42977; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.42977; runtime 0:00:04
Fold 8 training runtime: 0:00:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.86      0.84       790
        HPL       0.89      0.80      0.84       563
        MWS       0.81      0.83      0.82       604

avg / total       0.84      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [680  32  78]
             HPL  [ 69 451  43]
             MWS  [ 78  22 504]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.61694; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.61694 to 0.58866; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.58866 to 0.54934; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54934 to 0.52145; runtime 0:00:04; BEST YET
Epoch 005: val_loss did not improve from 0.52145; runtime 0:00:04
Epoch 006: val_loss improved from 0.52145 to 0.48884; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.48884 to 0.47334; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.47334 to 0.46488; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.46488 to 0.43914; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.43914 to 0.42306; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.42306; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.42306; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.42306; runtime 0:00:04
Fold 9 training runtime: 0:00:48

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.83      0.81      0.82       790
        HPL       0.90      0.74      0.81       563
        MWS       0.75      0.90      0.82       604

avg / total       0.82      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [639  38 113]
             HPL  [ 81 418  64]
             MWS  [ 53  10 541]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.58951; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.58951 to 0.55164; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.55164 to 0.54121; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54121 to 0.51387; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.51387 to 0.49674; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.49674 to 0.47379; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.47379 to 0.46055; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.46055 to 0.43770; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.43770 to 0.43180; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.43180; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.43180; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.43180; runtime 0:00:04
Fold 10 training runtime: 0:00:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.83      0.85      0.84       790
        HPL       0.92      0.78      0.84       563
        MWS       0.78      0.88      0.83       604

avg / total       0.84      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [668  27  95]
             HPL  [ 70 439  54]
             MWS  [ 62  13 529]
                    EAP  HPL  MWS
                  Predicted Labels

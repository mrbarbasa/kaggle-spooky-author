__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_layer (InputLayer)        (None, 128)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 128, 300)     8329800     input_layer[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 128, 256)     330240      spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 512)          0           global_average_pooling1d_1[0][0] 
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
output_layer (Dense)            (None, 3)            1539        concatenate_1[0][0]              
==================================================================================================
Total params: 8,661,579
Trainable params: 331,779
Non-trainable params: 8,329,800
__________________________________________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.64263; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.64263 to 0.58724; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.58724 to 0.54771; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54771 to 0.50388; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.50388 to 0.48684; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.48684 to 0.46984; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.46984 to 0.43986; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.43986 to 0.43873; runtime 0:00:04; BEST YET
Epoch 009: val_loss did not improve from 0.43873; runtime 0:00:04
Epoch 010: val_loss did not improve from 0.43873; runtime 0:00:04
Epoch 011: val_loss improved from 0.43873 to 0.40674; runtime 0:00:04; BEST YET
Epoch 012: val_loss did not improve from 0.40674; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.40674; runtime 0:00:04
Epoch 014: val_loss did not improve from 0.40674; runtime 0:00:04
Fold 1 training runtime: 0:00:58

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.88      0.83       790
        HPL       0.95      0.74      0.84       564
        MWS       0.81      0.86      0.83       605

avg / total       0.84      0.83      0.83      1959

            ----- Confusion Matrix -----
True Labels  EAP  [695  16  79]
             HPL  [102 419  43]
             MWS  [ 82   4 519]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.65387; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.65387 to 0.56491; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.56491 to 0.50849; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.50849 to 0.49638; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.49638 to 0.46535; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.46535 to 0.44120; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.44120 to 0.42512; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.42512 to 0.39359; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.39359 to 0.37976; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.37976 to 0.37725; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.37725; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.37725; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.37725; runtime 0:00:04
Fold 2 training runtime: 0:00:54

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.83      0.87      0.85       790
        HPL       0.95      0.77      0.85       564
        MWS       0.80      0.89      0.84       605

avg / total       0.86      0.85      0.85      1959

            ----- Confusion Matrix -----
True Labels  EAP  [690  17  83]
             HPL  [ 81 432  51]
             MWS  [ 61   5 539]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.63313; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.63313 to 0.59455; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.59455 to 0.54562; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54562 to 0.52677; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.52677 to 0.49576; runtime 0:00:04; BEST YET
Epoch 006: val_loss did not improve from 0.49576; runtime 0:00:04
Epoch 007: val_loss improved from 0.49576 to 0.49242; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.49242 to 0.46490; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.46490 to 0.44323; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.44323 to 0.43311; runtime 0:00:04; BEST YET
Epoch 011: val_loss improved from 0.43311 to 0.41297; runtime 0:00:04; BEST YET
Epoch 012: val_loss did not improve from 0.41297; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.41297; runtime 0:00:04
Epoch 014: val_loss did not improve from 0.41297; runtime 0:00:04
Fold 3 training runtime: 0:00:58

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.88      0.84       790
        HPL       0.87      0.81      0.84       564
        MWS       0.85      0.80      0.83       605

avg / total       0.84      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [697  42  51]
             HPL  [ 75 456  33]
             MWS  [ 93  27 485]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.61384; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.61384 to 0.56852; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.56852 to 0.55403; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.55403 to 0.49243; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.49243 to 0.45812; runtime 0:00:04; BEST YET
Epoch 006: val_loss did not improve from 0.45812; runtime 0:00:04
Epoch 007: val_loss did not improve from 0.45812; runtime 0:00:04
Epoch 008: val_loss improved from 0.45812 to 0.45021; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.45021 to 0.42581; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.42581; runtime 0:00:04
Epoch 011: val_loss did not improve from 0.42581; runtime 0:00:04
Epoch 012: val_loss improved from 0.42581 to 0.41370; runtime 0:00:04; BEST YET
Epoch 013: val_loss did not improve from 0.41370; runtime 0:00:04
Epoch 014: val_loss improved from 0.41370 to 0.38182; runtime 0:00:04; BEST YET
Epoch 015: val_loss did not improve from 0.38182; runtime 0:00:04
Epoch 016: val_loss did not improve from 0.38182; runtime 0:00:04
Epoch 017: val_loss did not improve from 0.38182; runtime 0:00:04
Fold 4 training runtime: 0:01:10

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.91      0.84       790
        HPL       0.95      0.70      0.81       564
        MWS       0.82      0.86      0.84       605

avg / total       0.85      0.83      0.83      1959

            ----- Confusion Matrix -----
True Labels  EAP  [716  16  58]
             HPL  [114 394  56]
             MWS  [ 79   3 523]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.59407; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.59407 to 0.57083; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.57083 to 0.51229; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.51229 to 0.46894; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.46894 to 0.45365; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.45365 to 0.43532; runtime 0:00:04; BEST YET
Epoch 007: val_loss did not improve from 0.43532; runtime 0:00:04
Epoch 008: val_loss improved from 0.43532 to 0.41450; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.41450 to 0.39977; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.39977 to 0.37580; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.37580; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.37580; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.37580; runtime 0:00:04
Fold 5 training runtime: 0:00:54

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.89      0.85       790
        HPL       0.89      0.85      0.87       564
        MWS       0.88      0.81      0.84       604

avg / total       0.86      0.85      0.85      1958

            ----- Confusion Matrix -----
True Labels  EAP  [703  36  51]
             HPL  [ 73 477  14]
             MWS  [ 93  22 489]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.60390; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.60390 to 0.56785; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.56785 to 0.51846; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.51846 to 0.51556; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.51556 to 0.45796; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.45796 to 0.45782; runtime 0:00:04; BEST YET
Epoch 007: val_loss did not improve from 0.45782; runtime 0:00:04
Epoch 008: val_loss improved from 0.45782 to 0.43921; runtime 0:00:04; BEST YET
Epoch 009: val_loss did not improve from 0.43921; runtime 0:00:04
Epoch 010: val_loss improved from 0.43921 to 0.42397; runtime 0:00:04; BEST YET
Epoch 011: val_loss improved from 0.42397 to 0.41608; runtime 0:00:04; BEST YET
Epoch 012: val_loss improved from 0.41608 to 0.40500; runtime 0:00:04; BEST YET
Epoch 013: val_loss improved from 0.40500 to 0.39714; runtime 0:00:04; BEST YET
Epoch 014: val_loss did not improve from 0.39714; runtime 0:00:04
Epoch 015: val_loss did not improve from 0.39714; runtime 0:00:04
Epoch 016: val_loss did not improve from 0.39714; runtime 0:00:04
Fold 6 training runtime: 0:01:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.89      0.84       790
        HPL       0.91      0.79      0.84       563
        MWS       0.85      0.80      0.83       604

avg / total       0.84      0.83      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [707  26  57]
             HPL  [ 94 443  26]
             MWS  [102  18 484]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.66110; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.66110 to 0.59467; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.59467 to 0.55130; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.55130 to 0.50759; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.50759 to 0.48094; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.48094 to 0.47237; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.47237 to 0.46908; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.46908 to 0.46696; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.46696 to 0.43240; runtime 0:00:04; BEST YET
Epoch 010: val_loss did not improve from 0.43240; runtime 0:00:04
Epoch 011: val_loss improved from 0.43240 to 0.42183; runtime 0:00:04; BEST YET
Epoch 012: val_loss improved from 0.42183 to 0.41497; runtime 0:00:04; BEST YET
Epoch 013: val_loss did not improve from 0.41497; runtime 0:00:04
Epoch 014: val_loss did not improve from 0.41497; runtime 0:00:04
Epoch 015: val_loss did not improve from 0.41497; runtime 0:00:04
Fold 7 training runtime: 0:01:02

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.92      0.85       790
        HPL       0.93      0.74      0.82       563
        MWS       0.85      0.82      0.83       604

avg / total       0.85      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [727  17  46]
             HPL  [104 416  43]
             MWS  [ 98  13 493]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.60983; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.60983 to 0.56417; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.56417 to 0.51638; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.51638 to 0.48272; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.48272 to 0.45259; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.45259 to 0.43449; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.43449 to 0.41406; runtime 0:00:04; BEST YET
Epoch 008: val_loss did not improve from 0.41406; runtime 0:00:04
Epoch 009: val_loss improved from 0.41406 to 0.39519; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.39519 to 0.37776; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.37776; runtime 0:00:04
Epoch 012: val_loss did not improve from 0.37776; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.37776; runtime 0:00:04
Fold 8 training runtime: 0:00:54

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.89      0.86       790
        HPL       0.93      0.79      0.86       563
        MWS       0.84      0.86      0.85       604

avg / total       0.86      0.85      0.85      1957

            ----- Confusion Matrix -----
True Labels  EAP  [705  21  64]
             HPL  [ 82 445  36]
             MWS  [ 72  11 521]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.64479; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.64479 to 0.58164; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.58164 to 0.54021; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.54021 to 0.49547; runtime 0:00:04; BEST YET
Epoch 005: val_loss did not improve from 0.49547; runtime 0:00:04
Epoch 006: val_loss improved from 0.49547 to 0.46676; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.46676 to 0.46350; runtime 0:00:04; BEST YET
Epoch 008: val_loss improved from 0.46350 to 0.43961; runtime 0:00:04; BEST YET
Epoch 009: val_loss improved from 0.43961 to 0.40655; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.40655 to 0.40537; runtime 0:00:04; BEST YET
Epoch 011: val_loss did not improve from 0.40537; runtime 0:00:04
Epoch 012: val_loss improved from 0.40537 to 0.40155; runtime 0:00:04; BEST YET
Epoch 013: val_loss did not improve from 0.40155; runtime 0:00:04
Epoch 014: val_loss improved from 0.40155 to 0.38903; runtime 0:00:04; BEST YET
Epoch 015: val_loss did not improve from 0.38903; runtime 0:00:04
Epoch 016: val_loss did not improve from 0.38903; runtime 0:00:04
Epoch 017: val_loss did not improve from 0.38903; runtime 0:00:04
Fold 9 training runtime: 0:01:10

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.91      0.84       790
        HPL       0.96      0.74      0.83       563
        MWS       0.85      0.84      0.84       604

avg / total       0.85      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [722  13  55]
             HPL  [112 414  37]
             MWS  [ 90   6 508]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.66876; runtime 0:00:05; BEST YET
Epoch 002: val_loss improved from 0.66876 to 0.59154; runtime 0:00:04; BEST YET
Epoch 003: val_loss improved from 0.59154 to 0.52178; runtime 0:00:04; BEST YET
Epoch 004: val_loss improved from 0.52178 to 0.49154; runtime 0:00:04; BEST YET
Epoch 005: val_loss improved from 0.49154 to 0.47488; runtime 0:00:04; BEST YET
Epoch 006: val_loss improved from 0.47488 to 0.44406; runtime 0:00:04; BEST YET
Epoch 007: val_loss improved from 0.44406 to 0.42271; runtime 0:00:04; BEST YET
Epoch 008: val_loss did not improve from 0.42271; runtime 0:00:04
Epoch 009: val_loss improved from 0.42271 to 0.41905; runtime 0:00:04; BEST YET
Epoch 010: val_loss improved from 0.41905 to 0.40303; runtime 0:00:04; BEST YET
Epoch 011: val_loss improved from 0.40303 to 0.40010; runtime 0:00:04; BEST YET
Epoch 012: val_loss did not improve from 0.40010; runtime 0:00:04
Epoch 013: val_loss did not improve from 0.40010; runtime 0:00:04
Epoch 014: val_loss improved from 0.40010 to 0.37716; runtime 0:00:04; BEST YET
Epoch 015: val_loss did not improve from 0.37716; runtime 0:00:04
Epoch 016: val_loss improved from 0.37716 to 0.37434; runtime 0:00:04; BEST YET
Epoch 017: val_loss did not improve from 0.37434; runtime 0:00:04
Epoch 018: val_loss did not improve from 0.37434; runtime 0:00:04
Epoch 019: val_loss did not improve from 0.37434; runtime 0:00:04
Fold 10 training runtime: 0:01:19

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.92      0.86       790
        HPL       0.91      0.82      0.86       563
        MWS       0.87      0.80      0.83       604

avg / total       0.86      0.85      0.85      1957

            ----- Confusion Matrix -----
True Labels  EAP  [723  24  43]
             HPL  [ 73 460  30]
             MWS  [ 98  23 483]
                    EAP  HPL  MWS
                  Predicted Labels

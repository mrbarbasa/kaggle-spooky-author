__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_layer (InputLayer)        (None, 128)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 128, 300)     8329800     input_layer[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 128, 64)      64128       spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
spatial_dropout1d_2 (SpatialDro (None, 128, 64)      0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 128, 64)      18816       spatial_dropout1d_2[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 64)           0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 64)           0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] 
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
output_layer (Dense)            (None, 3)            387         concatenate_1[0][0]              
==================================================================================================
Total params: 8,413,131
Trainable params: 83,331
Non-trainable params: 8,329,800
__________________________________________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.64563; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.64563 to 0.58839; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.58839 to 0.56978; runtime 0:00:06; BEST YET
Epoch 004: val_loss did not improve from 0.56978; runtime 0:00:07
Epoch 005: val_loss improved from 0.56978 to 0.53196; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.53196 to 0.49142; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.49142 to 0.47472; runtime 0:00:06; BEST YET
Epoch 008: val_loss improved from 0.47472 to 0.45709; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.45709 to 0.45378; runtime 0:00:07; BEST YET
Epoch 010: val_loss did not improve from 0.45378; runtime 0:00:06
Epoch 011: val_loss improved from 0.45378 to 0.45235; runtime 0:00:07; BEST YET
Epoch 012: val_loss improved from 0.45235 to 0.43161; runtime 0:00:07; BEST YET
Epoch 013: val_loss did not improve from 0.43161; runtime 0:00:07
Epoch 014: val_loss improved from 0.43161 to 0.41392; runtime 0:00:07; BEST YET
Epoch 015: val_loss did not improve from 0.41392; runtime 0:00:07
Epoch 016: val_loss did not improve from 0.41392; runtime 0:00:07
Epoch 017: val_loss did not improve from 0.41392; runtime 0:00:07
Fold 1 training runtime: 0:01:53

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.86      0.84       790
        HPL       0.92      0.76      0.83       564
        MWS       0.82      0.88      0.85       605

avg / total       0.84      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [681  33  76]
             HPL  [ 90 431  43]
             MWS  [ 64   7 534]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.61416; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.61416 to 0.58390; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.58390 to 0.54040; runtime 0:00:06; BEST YET
Epoch 004: val_loss improved from 0.54040 to 0.50643; runtime 0:00:06; BEST YET
Epoch 005: val_loss improved from 0.50643 to 0.48777; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.48777 to 0.47171; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.47171 to 0.44892; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.44892 to 0.43875; runtime 0:00:06; BEST YET
Epoch 009: val_loss did not improve from 0.43875; runtime 0:00:07
Epoch 010: val_loss did not improve from 0.43875; runtime 0:00:06
Epoch 011: val_loss improved from 0.43875 to 0.40343; runtime 0:00:06; BEST YET
Epoch 012: val_loss improved from 0.40343 to 0.39389; runtime 0:00:06; BEST YET
Epoch 013: val_loss did not improve from 0.39389; runtime 0:00:06
Epoch 014: val_loss improved from 0.39389 to 0.38884; runtime 0:00:06; BEST YET
Epoch 015: val_loss improved from 0.38884 to 0.38051; runtime 0:00:06; BEST YET
Epoch 016: val_loss did not improve from 0.38051; runtime 0:00:07
Epoch 017: val_loss did not improve from 0.38051; runtime 0:00:06
Epoch 018: val_loss did not improve from 0.38051; runtime 0:00:06
Fold 2 training runtime: 0:01:58

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.88      0.84       790
        HPL       0.92      0.79      0.85       564
        MWS       0.82      0.83      0.83       605

avg / total       0.84      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [695  25  70]
             HPL  [ 79 447  38]
             MWS  [ 90  14 501]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.62492; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.62492 to 0.59973; runtime 0:00:06; BEST YET
Epoch 003: val_loss did not improve from 0.59973; runtime 0:00:06
Epoch 004: val_loss improved from 0.59973 to 0.56526; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.56526 to 0.51137; runtime 0:00:06; BEST YET
Epoch 006: val_loss did not improve from 0.51137; runtime 0:00:06
Epoch 007: val_loss improved from 0.51137 to 0.48413; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.48413 to 0.47158; runtime 0:00:07; BEST YET
Epoch 009: val_loss did not improve from 0.47158; runtime 0:00:06
Epoch 010: val_loss did not improve from 0.47158; runtime 0:00:06
Epoch 011: val_loss did not improve from 0.47158; runtime 0:00:07
Fold 3 training runtime: 0:01:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.84      0.79      0.82       790
        HPL       0.87      0.78      0.82       564
        MWS       0.75      0.88      0.81       605

avg / total       0.82      0.82      0.82      1959

            ----- Confusion Matrix -----
True Labels  EAP  [626  46 118]
             HPL  [ 66 442  56]
             MWS  [ 53  22 530]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.63877; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.63877 to 0.59415; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.59415 to 0.56425; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.56425 to 0.52407; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.52407 to 0.50779; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.50779 to 0.48028; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.48028 to 0.46932; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.46932 to 0.44542; runtime 0:00:06; BEST YET
Epoch 009: val_loss improved from 0.44542 to 0.43502; runtime 0:00:06; BEST YET
Epoch 010: val_loss improved from 0.43502 to 0.42356; runtime 0:00:07; BEST YET
Epoch 011: val_loss improved from 0.42356 to 0.40692; runtime 0:00:06; BEST YET
Epoch 012: val_loss did not improve from 0.40692; runtime 0:00:07
Epoch 013: val_loss improved from 0.40692 to 0.38701; runtime 0:00:07; BEST YET
Epoch 014: val_loss did not improve from 0.38701; runtime 0:00:07
Epoch 015: val_loss improved from 0.38701 to 0.38292; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.38292; runtime 0:00:06
Epoch 017: val_loss did not improve from 0.38292; runtime 0:00:07
Epoch 018: val_loss did not improve from 0.38292; runtime 0:00:06
Fold 4 training runtime: 0:01:59

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.75      0.93      0.83       790
        HPL       0.96      0.69      0.80       564
        MWS       0.86      0.80      0.83       605

avg / total       0.84      0.82      0.82      1959

            ----- Confusion Matrix -----
True Labels  EAP  [738  13  39]
             HPL  [135 391  38]
             MWS  [114   4 487]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.59777; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.59777 to 0.57040; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.57040 to 0.52933; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.52933 to 0.50069; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.50069 to 0.50033; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.50033 to 0.46529; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.46529 to 0.44739; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.44739 to 0.42665; runtime 0:00:06; BEST YET
Epoch 009: val_loss improved from 0.42665 to 0.41059; runtime 0:00:07; BEST YET
Epoch 010: val_loss did not improve from 0.41059; runtime 0:00:07
Epoch 011: val_loss improved from 0.41059 to 0.40280; runtime 0:00:06; BEST YET
Epoch 012: val_loss improved from 0.40280 to 0.39745; runtime 0:00:06; BEST YET
Epoch 013: val_loss did not improve from 0.39745; runtime 0:00:06
Epoch 014: val_loss did not improve from 0.39745; runtime 0:00:06
Epoch 015: val_loss did not improve from 0.39745; runtime 0:00:07
Fold 5 training runtime: 0:01:39

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.92      0.84       790
        HPL       0.92      0.81      0.86       564
        MWS       0.89      0.78      0.83       604

avg / total       0.85      0.84      0.84      1958

            ----- Confusion Matrix -----
True Labels  EAP  [725  26  39]
             HPL  [ 89 455  20]
             MWS  [115  16 473]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.63412; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.63412 to 0.61712; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.61712 to 0.55829; runtime 0:00:06; BEST YET
Epoch 004: val_loss improved from 0.55829 to 0.55599; runtime 0:00:06; BEST YET
Epoch 005: val_loss improved from 0.55599 to 0.51191; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.51191 to 0.49005; runtime 0:00:06; BEST YET
Epoch 007: val_loss improved from 0.49005 to 0.48205; runtime 0:00:06; BEST YET
Epoch 008: val_loss improved from 0.48205 to 0.45840; runtime 0:00:07; BEST YET
Epoch 009: val_loss did not improve from 0.45840; runtime 0:00:06
Epoch 010: val_loss improved from 0.45840 to 0.45828; runtime 0:00:06; BEST YET
Epoch 011: val_loss improved from 0.45828 to 0.43791; runtime 0:00:06; BEST YET
Epoch 012: val_loss did not improve from 0.43791; runtime 0:00:06
Epoch 013: val_loss improved from 0.43791 to 0.43674; runtime 0:00:06; BEST YET
Epoch 014: val_loss did not improve from 0.43674; runtime 0:00:06
Epoch 015: val_loss did not improve from 0.43674; runtime 0:00:06
Epoch 016: val_loss did not improve from 0.43674; runtime 0:00:06
Fold 6 training runtime: 0:01:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.85      0.83       790
        HPL       0.92      0.77      0.84       563
        MWS       0.79      0.85      0.82       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [673  26  91]
             HPL  [ 88 434  41]
             MWS  [ 79  14 511]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.71520; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.71520 to 0.61646; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.61646 to 0.57143; runtime 0:00:06; BEST YET
Epoch 004: val_loss improved from 0.57143 to 0.54836; runtime 0:00:06; BEST YET
Epoch 005: val_loss improved from 0.54836 to 0.53333; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.53333 to 0.50000; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.50000 to 0.49301; runtime 0:00:06; BEST YET
Epoch 008: val_loss improved from 0.49301 to 0.46282; runtime 0:00:06; BEST YET
Epoch 009: val_loss did not improve from 0.46282; runtime 0:00:06
Epoch 010: val_loss improved from 0.46282 to 0.44862; runtime 0:00:06; BEST YET
Epoch 011: val_loss did not improve from 0.44862; runtime 0:00:07
Epoch 012: val_loss improved from 0.44862 to 0.44182; runtime 0:00:06; BEST YET
Epoch 013: val_loss did not improve from 0.44182; runtime 0:00:06
Epoch 014: val_loss did not improve from 0.44182; runtime 0:00:07
Epoch 015: val_loss improved from 0.44182 to 0.43082; runtime 0:00:06; BEST YET
Epoch 016: val_loss improved from 0.43082 to 0.42501; runtime 0:00:07; BEST YET
Epoch 017: val_loss did not improve from 0.42501; runtime 0:00:07
Epoch 018: val_loss did not improve from 0.42501; runtime 0:00:06
Epoch 019: val_loss did not improve from 0.42501; runtime 0:00:07
Fold 7 training runtime: 0:02:05

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.89      0.84       790
        HPL       0.89      0.80      0.84       563
        MWS       0.86      0.80      0.83       604

avg / total       0.84      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [707  34  49]
             HPL  [ 84 448  31]
             MWS  [100  21 483]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.61016; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.61016 to 0.58482; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.58482 to 0.55361; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.55361 to 0.52281; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.52281 to 0.49142; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.49142 to 0.46890; runtime 0:00:06; BEST YET
Epoch 007: val_loss improved from 0.46890 to 0.45643; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.45643 to 0.43740; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.43740 to 0.42007; runtime 0:00:06; BEST YET
Epoch 010: val_loss did not improve from 0.42007; runtime 0:00:06
Epoch 011: val_loss did not improve from 0.42007; runtime 0:00:06
Epoch 012: val_loss did not improve from 0.42007; runtime 0:00:07
Fold 8 training runtime: 0:01:20

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.90      0.83       790
        HPL       0.92      0.77      0.84       563
        MWS       0.85      0.80      0.83       604

avg / total       0.84      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [710  24  56]
             HPL  [102 434  27]
             MWS  [104  15 485]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.68217; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.68217 to 0.59268; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.59268 to 0.57389; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.57389 to 0.53310; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.53310 to 0.51372; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.51372 to 0.50150; runtime 0:00:06; BEST YET
Epoch 007: val_loss did not improve from 0.50150; runtime 0:00:06
Epoch 008: val_loss improved from 0.50150 to 0.46278; runtime 0:00:06; BEST YET
Epoch 009: val_loss did not improve from 0.46278; runtime 0:00:07
Epoch 010: val_loss did not improve from 0.46278; runtime 0:00:06
Epoch 011: val_loss improved from 0.46278 to 0.44035; runtime 0:00:07; BEST YET
Epoch 012: val_loss did not improve from 0.44035; runtime 0:00:06
Epoch 013: val_loss improved from 0.44035 to 0.41457; runtime 0:00:07; BEST YET
Epoch 014: val_loss did not improve from 0.41457; runtime 0:00:06
Epoch 015: val_loss did not improve from 0.41457; runtime 0:00:06
Epoch 016: val_loss did not improve from 0.41457; runtime 0:00:06
Fold 9 training runtime: 0:01:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.86      0.84       790
        HPL       0.89      0.79      0.84       563
        MWS       0.84      0.85      0.84       604

avg / total       0.84      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [683  40  67]
             HPL  [ 86 445  32]
             MWS  [ 74  17 513]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.60440; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.60440 to 0.55814; runtime 0:00:06; BEST YET
Epoch 003: val_loss improved from 0.55814 to 0.54194; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.54194 to 0.50162; runtime 0:00:06; BEST YET
Epoch 005: val_loss improved from 0.50162 to 0.49431; runtime 0:00:06; BEST YET
Epoch 006: val_loss improved from 0.49431 to 0.45965; runtime 0:00:06; BEST YET
Epoch 007: val_loss improved from 0.45965 to 0.44218; runtime 0:00:06; BEST YET
Epoch 008: val_loss improved from 0.44218 to 0.43733; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.43733 to 0.41212; runtime 0:00:06; BEST YET
Epoch 010: val_loss did not improve from 0.41212; runtime 0:00:06
Epoch 011: val_loss did not improve from 0.41212; runtime 0:00:07
Epoch 012: val_loss did not improve from 0.41212; runtime 0:00:07
Fold 10 training runtime: 0:01:19

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.75      0.95      0.84       790
        HPL       0.94      0.75      0.83       563
        MWS       0.90      0.74      0.81       604

avg / total       0.85      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [751  15  24]
             HPL  [112 423  28]
             MWS  [142  14 448]
                    EAP  HPL  MWS
                  Predicted Labels

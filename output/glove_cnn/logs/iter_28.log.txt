_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 300)          8302800   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 128, 32)           67232     
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 64, 32)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 32)            0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 64, 32)            7200      
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 32, 32)            0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32)            0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                32800     
_________________________________________________________________
dropout_3 (Dropout)          (None, 32)                0         
_________________________________________________________________
output_layer (Dense)         (None, 3)                 99        
=================================================================
Total params: 8,410,131
Trainable params: 107,331
Non-trainable params: 8,302,800
_________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.89962; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.89962 to 0.77991; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.77991 to 0.70378; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.70378 to 0.67245; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.67245 to 0.64904; runtime 0:00:00; BEST YET
Epoch 006: val_loss improved from 0.64904 to 0.63405; runtime 0:00:00; BEST YET
Epoch 007: val_loss improved from 0.63405 to 0.62384; runtime 0:00:00; BEST YET
Epoch 008: val_loss improved from 0.62384 to 0.61596; runtime 0:00:00; BEST YET
Epoch 009: val_loss improved from 0.61596 to 0.58397; runtime 0:00:00; BEST YET
Epoch 010: val_loss improved from 0.58397 to 0.55471; runtime 0:00:00; BEST YET
Epoch 011: val_loss did not improve from 0.55471; runtime 0:00:00
Epoch 012: val_loss did not improve from 0.55471; runtime 0:00:00
Epoch 013: val_loss did not improve from 0.55471; runtime 0:00:00
Fold 1 training runtime: 0:00:05

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.79      0.76       790
        HPL       0.68      0.85      0.75       564
        MWS       0.90      0.57      0.70       605

avg / total       0.76      0.74      0.74      1959

            ----- Confusion Matrix -----
True Labels  EAP  [627 129  34]
             HPL  [ 79 479   6]
             MWS  [158 100 347]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.90468; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.90468 to 0.76385; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.76385 to 0.70223; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.70223 to 0.68014; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.68014 to 0.62938; runtime 0:00:00; BEST YET
Epoch 006: val_loss improved from 0.62938 to 0.61283; runtime 0:00:00; BEST YET
Epoch 007: val_loss did not improve from 0.61283; runtime 0:00:00
Epoch 008: val_loss did not improve from 0.61283; runtime 0:00:00
Epoch 009: val_loss improved from 0.61283 to 0.57131; runtime 0:00:00; BEST YET
Epoch 010: val_loss did not improve from 0.57131; runtime 0:00:00
Epoch 011: val_loss improved from 0.57131 to 0.56219; runtime 0:00:00; BEST YET
Epoch 012: val_loss did not improve from 0.56219; runtime 0:00:00
Epoch 013: val_loss improved from 0.56219 to 0.54181; runtime 0:00:00; BEST YET
Epoch 014: val_loss did not improve from 0.54181; runtime 0:00:00
Epoch 015: val_loss improved from 0.54181 to 0.52164; runtime 0:00:00; BEST YET
Epoch 016: val_loss did not improve from 0.52164; runtime 0:00:00
Epoch 017: val_loss improved from 0.52164 to 0.51824; runtime 0:00:00; BEST YET
Epoch 018: val_loss did not improve from 0.51824; runtime 0:00:00
Epoch 019: val_loss did not improve from 0.51824; runtime 0:00:00
Epoch 020: val_loss did not improve from 0.51824; runtime 0:00:00
Fold 2 training runtime: 0:00:08

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.77      0.80      0.79       790
        HPL       0.91      0.67      0.77       564
        MWS       0.71      0.85      0.78       605

avg / total       0.79      0.78      0.78      1959

            ----- Confusion Matrix -----
True Labels  EAP  [635  28 127]
             HPL  [106 378  80]
             MWS  [ 80   9 516]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.86842; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.86842 to 0.76342; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.76342 to 0.75770; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.75770 to 0.71671; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.71671 to 0.66404; runtime 0:00:00; BEST YET
Epoch 006: val_loss did not improve from 0.66404; runtime 0:00:00
Epoch 007: val_loss did not improve from 0.66404; runtime 0:00:00
Epoch 008: val_loss did not improve from 0.66404; runtime 0:00:00
Fold 3 training runtime: 0:00:03

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.64      0.88      0.74       790
        HPL       0.91      0.46      0.61       564
        MWS       0.72      0.71      0.71       605

avg / total       0.74      0.71      0.70      1959

            ----- Confusion Matrix -----
True Labels  EAP  [692  15  83]
             HPL  [218 261  85]
             MWS  [165  11 429]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.91837; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.91837 to 0.75274; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.75274 to 0.68833; runtime 0:00:00; BEST YET
Epoch 004: val_loss did not improve from 0.68833; runtime 0:00:00
Epoch 005: val_loss improved from 0.68833 to 0.64731; runtime 0:00:00; BEST YET
Epoch 006: val_loss did not improve from 0.64731; runtime 0:00:00
Epoch 007: val_loss improved from 0.64731 to 0.60733; runtime 0:00:00; BEST YET
Epoch 008: val_loss did not improve from 0.60733; runtime 0:00:00
Epoch 009: val_loss did not improve from 0.60733; runtime 0:00:00
Epoch 010: val_loss improved from 0.60733 to 0.54916; runtime 0:00:00; BEST YET
Epoch 011: val_loss did not improve from 0.54916; runtime 0:00:00
Epoch 012: val_loss improved from 0.54916 to 0.54524; runtime 0:00:00; BEST YET
Epoch 013: val_loss improved from 0.54524 to 0.53288; runtime 0:00:00; BEST YET
Epoch 014: val_loss did not improve from 0.53288; runtime 0:00:00
Epoch 015: val_loss did not improve from 0.53288; runtime 0:00:00
Epoch 016: val_loss did not improve from 0.53288; runtime 0:00:00
Fold 4 training runtime: 0:00:06

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.78      0.78       790
        HPL       0.89      0.63      0.74       564
        MWS       0.69      0.89      0.78       605

avg / total       0.79      0.77      0.77      1959

            ----- Confusion Matrix -----
True Labels  EAP  [613  33 144]
             HPL  [115 355  94]
             MWS  [ 57   9 539]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.95747; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.95747 to 0.74675; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.74675 to 0.68441; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.68441 to 0.66272; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.66272 to 0.61398; runtime 0:00:00; BEST YET
Epoch 006: val_loss did not improve from 0.61398; runtime 0:00:00
Epoch 007: val_loss improved from 0.61398 to 0.58292; runtime 0:00:00; BEST YET
Epoch 008: val_loss did not improve from 0.58292; runtime 0:00:00
Epoch 009: val_loss did not improve from 0.58292; runtime 0:00:00
Epoch 010: val_loss did not improve from 0.58292; runtime 0:00:00
Fold 5 training runtime: 0:00:04

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.70      0.86      0.78       790
        HPL       0.91      0.61      0.73       564
        MWS       0.75      0.76      0.76       604

avg / total       0.78      0.76      0.76      1958

            ----- Confusion Matrix -----
True Labels  EAP  [682  23  85]
             HPL  [153 345  66]
             MWS  [134  10 460]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.92283; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.92283 to 0.75485; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.75485 to 0.71518; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.71518 to 0.70219; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.70219 to 0.67448; runtime 0:00:00; BEST YET
Epoch 006: val_loss improved from 0.67448 to 0.64555; runtime 0:00:00; BEST YET
Epoch 007: val_loss improved from 0.64555 to 0.62288; runtime 0:00:00; BEST YET
Epoch 008: val_loss improved from 0.62288 to 0.57661; runtime 0:00:00; BEST YET
Epoch 009: val_loss did not improve from 0.57661; runtime 0:00:00
Epoch 010: val_loss did not improve from 0.57661; runtime 0:00:00
Epoch 011: val_loss improved from 0.57661 to 0.54853; runtime 0:00:00; BEST YET
Epoch 012: val_loss improved from 0.54853 to 0.53792; runtime 0:00:00; BEST YET
Epoch 013: val_loss did not improve from 0.53792; runtime 0:00:00
Epoch 014: val_loss did not improve from 0.53792; runtime 0:00:00
Epoch 015: val_loss did not improve from 0.53792; runtime 0:00:00
Fold 6 training runtime: 0:00:06

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.71      0.85      0.78       790
        HPL       0.77      0.83      0.80       563
        MWS       0.86      0.57      0.69       604

avg / total       0.77      0.76      0.76      1957

            ----- Confusion Matrix -----
True Labels  EAP  [675  72  43]
             HPL  [ 81 467  15]
             MWS  [193  65 346]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.91705; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.91705 to 0.78799; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.78799 to 0.71515; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.71515 to 0.70755; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.70755 to 0.64664; runtime 0:00:00; BEST YET
Epoch 006: val_loss did not improve from 0.64664; runtime 0:00:00
Epoch 007: val_loss improved from 0.64664 to 0.61853; runtime 0:00:00; BEST YET
Epoch 008: val_loss improved from 0.61853 to 0.59571; runtime 0:00:00; BEST YET
Epoch 009: val_loss did not improve from 0.59571; runtime 0:00:00
Epoch 010: val_loss improved from 0.59571 to 0.57850; runtime 0:00:00; BEST YET
Epoch 011: val_loss did not improve from 0.57850; runtime 0:00:00
Epoch 012: val_loss improved from 0.57850 to 0.55497; runtime 0:00:00; BEST YET
Epoch 013: val_loss improved from 0.55497 to 0.54839; runtime 0:00:00; BEST YET
Epoch 014: val_loss did not improve from 0.54839; runtime 0:00:00
Epoch 015: val_loss did not improve from 0.54839; runtime 0:00:00
Epoch 016: val_loss improved from 0.54839 to 0.54402; runtime 0:00:00; BEST YET
Epoch 017: val_loss did not improve from 0.54402; runtime 0:00:00
Epoch 018: val_loss did not improve from 0.54402; runtime 0:00:00
Epoch 019: val_loss did not improve from 0.54402; runtime 0:00:00
Fold 7 training runtime: 0:00:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.84      0.68      0.75       790
        HPL       0.64      0.90      0.74       563
        MWS       0.79      0.69      0.74       604

avg / total       0.77      0.75      0.75      1957

            ----- Confusion Matrix -----
True Labels  EAP  [538 164  88]
             HPL  [ 38 504  21]
             MWS  [ 62 124 418]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.88938; runtime 0:00:01; BEST YET
Epoch 002: val_loss did not improve from 0.88938; runtime 0:00:00
Epoch 003: val_loss improved from 0.88938 to 0.71948; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.71948 to 0.64577; runtime 0:00:00; BEST YET
Epoch 005: val_loss improved from 0.64577 to 0.62804; runtime 0:00:00; BEST YET
Epoch 006: val_loss did not improve from 0.62804; runtime 0:00:00
Epoch 007: val_loss improved from 0.62804 to 0.61992; runtime 0:00:00; BEST YET
Epoch 008: val_loss improved from 0.61992 to 0.59737; runtime 0:00:00; BEST YET
Epoch 009: val_loss improved from 0.59737 to 0.56366; runtime 0:00:00; BEST YET
Epoch 010: val_loss did not improve from 0.56366; runtime 0:00:00
Epoch 011: val_loss improved from 0.56366 to 0.55262; runtime 0:00:00; BEST YET
Epoch 012: val_loss did not improve from 0.55262; runtime 0:00:00
Epoch 013: val_loss did not improve from 0.55262; runtime 0:00:00
Epoch 014: val_loss improved from 0.55262 to 0.52242; runtime 0:00:00; BEST YET
Epoch 015: val_loss did not improve from 0.52242; runtime 0:00:00
Epoch 016: val_loss did not improve from 0.52242; runtime 0:00:00
Epoch 017: val_loss did not improve from 0.52242; runtime 0:00:00
Fold 8 training runtime: 0:00:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.93      0.35      0.51       790
        HPL       0.60      0.90      0.72       563
        MWS       0.64      0.86      0.73       604

avg / total       0.74      0.66      0.64      1957

            ----- Confusion Matrix -----
True Labels  EAP  [274 267 249]
             HPL  [  9 508  46]
             MWS  [ 12  74 518]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.88596; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.88596 to 0.79906; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.79906 to 0.70549; runtime 0:00:00; BEST YET
Epoch 004: val_loss improved from 0.70549 to 0.66938; runtime 0:00:00; BEST YET
Epoch 005: val_loss did not improve from 0.66938; runtime 0:00:00
Epoch 006: val_loss improved from 0.66938 to 0.64219; runtime 0:00:00; BEST YET
Epoch 007: val_loss did not improve from 0.64219; runtime 0:00:00
Epoch 008: val_loss improved from 0.64219 to 0.61671; runtime 0:00:00; BEST YET
Epoch 009: val_loss improved from 0.61671 to 0.58068; runtime 0:00:00; BEST YET
Epoch 010: val_loss did not improve from 0.58068; runtime 0:00:00
Epoch 011: val_loss improved from 0.58068 to 0.57004; runtime 0:00:00; BEST YET
Epoch 012: val_loss did not improve from 0.57004; runtime 0:00:00
Epoch 013: val_loss did not improve from 0.57004; runtime 0:00:00
Epoch 014: val_loss did not improve from 0.57004; runtime 0:00:00
Fold 9 training runtime: 0:00:06

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.76      0.76       790
        HPL       0.95      0.49      0.65       563
        MWS       0.61      0.89      0.73       604

avg / total       0.77      0.72      0.72      1957

            ----- Confusion Matrix -----
True Labels  EAP  [597   9 184]
             HPL  [127 277 159]
             MWS  [ 58   7 539]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.89872; runtime 0:00:01; BEST YET
Epoch 002: val_loss improved from 0.89872 to 0.75647; runtime 0:00:00; BEST YET
Epoch 003: val_loss improved from 0.75647 to 0.71898; runtime 0:00:00; BEST YET
Epoch 004: val_loss did not improve from 0.71898; runtime 0:00:00
Epoch 005: val_loss improved from 0.71898 to 0.66354; runtime 0:00:00; BEST YET
Epoch 006: val_loss improved from 0.66354 to 0.64707; runtime 0:00:00; BEST YET
Epoch 007: val_loss did not improve from 0.64707; runtime 0:00:00
Epoch 008: val_loss improved from 0.64707 to 0.61751; runtime 0:00:00; BEST YET
Epoch 009: val_loss improved from 0.61751 to 0.55745; runtime 0:00:00; BEST YET
Epoch 010: val_loss did not improve from 0.55745; runtime 0:00:00
Epoch 011: val_loss did not improve from 0.55745; runtime 0:00:00
Epoch 012: val_loss improved from 0.55745 to 0.54642; runtime 0:00:00; BEST YET
Epoch 013: val_loss did not improve from 0.54642; runtime 0:00:00
Epoch 014: val_loss improved from 0.54642 to 0.53498; runtime 0:00:00; BEST YET
Epoch 015: val_loss did not improve from 0.53498; runtime 0:00:00
Epoch 016: val_loss did not improve from 0.53498; runtime 0:00:00
Epoch 017: val_loss did not improve from 0.53498; runtime 0:00:00
Fold 10 training runtime: 0:00:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.87      0.80       790
        HPL       0.88      0.67      0.76       563
        MWS       0.78      0.77      0.78       604

avg / total       0.79      0.78      0.78      1957

            ----- Confusion Matrix -----
True Labels  EAP  [684  30  76]
             HPL  [128 379  56]
             MWS  [113  24 467]
                    EAP  HPL  MWS
                  Predicted Labels

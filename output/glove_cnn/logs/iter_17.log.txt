_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     (None, 128)               0         
_________________________________________________________________
embedding_161 (Embedding)    (None, 128, 300)          8302800   
_________________________________________________________________
conv1d_491 (Conv1D)          (None, 128, 32)           48032     
_________________________________________________________________
conv1d_492 (Conv1D)          (None, 128, 32)           5152      
_________________________________________________________________
max_pooling1d_211 (MaxPoolin (None, 32, 32)            0         
_________________________________________________________________
conv1d_493 (Conv1D)          (None, 32, 32)            5152      
_________________________________________________________________
conv1d_494 (Conv1D)          (None, 32, 32)            5152      
_________________________________________________________________
max_pooling1d_212 (MaxPoolin (None, 8, 32)             0         
_________________________________________________________________
conv1d_495 (Conv1D)          (None, 8, 32)             5152      
_________________________________________________________________
conv1d_496 (Conv1D)          (None, 8, 32)             5152      
_________________________________________________________________
max_pooling1d_213 (MaxPoolin (None, 2, 32)             0         
_________________________________________________________________
flatten_71 (Flatten)         (None, 64)                0         
_________________________________________________________________
dense_91 (Dense)             (None, 32)                2080      
_________________________________________________________________
dropout_191 (Dropout)        (None, 32)                0         
_________________________________________________________________
output_layer (Dense)         (None, 3)                 99        
=================================================================
Total params: 8,378,771
Trainable params: 75,971
Non-trainable params: 8,302,800
_________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.68655; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.68655 to 0.66921; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.66921 to 0.56826; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.56826; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.56826; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.56826; runtime 0:00:07
Fold 1 training runtime: 0:00:51

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.86      0.66      0.74       790
        HPL       0.74      0.84      0.79       564
        MWS       0.72      0.85      0.78       605

avg / total       0.78      0.77      0.77      1959

            ----- Confusion Matrix -----
True Labels  EAP  [520 121 149]
             HPL  [ 42 473  49]
             MWS  [ 44  45 516]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.62744; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.62744 to 0.61937; runtime 0:00:07; BEST YET
Epoch 003: val_loss did not improve from 0.61937; runtime 0:00:07
Epoch 004: val_loss improved from 0.61937 to 0.55442; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.55442 to 0.54913; runtime 0:00:07; BEST YET
Epoch 006: val_loss did not improve from 0.54913; runtime 0:00:07
Epoch 007: val_loss did not improve from 0.54913; runtime 0:00:07
Epoch 008: val_loss did not improve from 0.54913; runtime 0:00:07
Fold 2 training runtime: 0:01:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.64      0.93      0.76       790
        HPL       0.81      0.73      0.77       564
        MWS       0.90      0.46      0.61       605

avg / total       0.77      0.73      0.72      1959

            ----- Confusion Matrix -----
True Labels  EAP  [731  39  20]
             HPL  [141 411  12]
             MWS  [269  56 280]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.67499; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.67499 to 0.58823; runtime 0:00:07; BEST YET
Epoch 003: val_loss did not improve from 0.58823; runtime 0:00:07
Epoch 004: val_loss did not improve from 0.58823; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.58823; runtime 0:00:07
Fold 3 training runtime: 0:00:45

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.81      0.79       790
        HPL       0.74      0.80      0.77       564
        MWS       0.84      0.72      0.77       605

avg / total       0.78      0.78      0.78      1959

            ----- Confusion Matrix -----
True Labels  EAP  [640  98  52]
             HPL  [ 76 454  34]
             MWS  [109  59 437]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.67768; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.67768 to 0.60561; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.60561 to 0.51044; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.51044; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.51044; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.51044; runtime 0:00:07
Fold 4 training runtime: 0:00:52

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.87      0.80       790
        HPL       0.90      0.69      0.78       564
        MWS       0.80      0.79      0.80       605

avg / total       0.80      0.79      0.79      1959

            ----- Confusion Matrix -----
True Labels  EAP  [687  31  72]
             HPL  [131 387  46]
             MWS  [112  14 479]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.62687; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.62687 to 0.56251; runtime 0:00:07; BEST YET
Epoch 003: val_loss did not improve from 0.56251; runtime 0:00:07
Epoch 004: val_loss did not improve from 0.56251; runtime 0:00:07
Epoch 005: val_loss improved from 0.56251 to 0.53445; runtime 0:00:07; BEST YET
Epoch 006: val_loss did not improve from 0.53445; runtime 0:00:07
Epoch 007: val_loss did not improve from 0.53445; runtime 0:00:07
Epoch 008: val_loss did not improve from 0.53445; runtime 0:00:07
Fold 5 training runtime: 0:01:06

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.78      0.79       790
        HPL       0.75      0.83      0.79       564
        MWS       0.81      0.75      0.78       604

avg / total       0.79      0.78      0.78      1958

            ----- Confusion Matrix -----
True Labels  EAP  [615  97  78]
             HPL  [ 67 469  28]
             MWS  [ 92  59 453]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.79199; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.79199 to 0.56348; runtime 0:00:07; BEST YET
Epoch 003: val_loss did not improve from 0.56348; runtime 0:00:07
Epoch 004: val_loss improved from 0.56348 to 0.54287; runtime 0:00:07; BEST YET
Epoch 005: val_loss did not improve from 0.54287; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.54287; runtime 0:00:07
Epoch 007: val_loss did not improve from 0.54287; runtime 0:00:07
Fold 6 training runtime: 0:00:59

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.80      0.79       790
        HPL       0.81      0.80      0.81       563
        MWS       0.78      0.78      0.78       604

avg / total       0.79      0.79      0.79      1957

            ----- Confusion Matrix -----
True Labels  EAP  [630  68  92]
             HPL  [ 66 453  44]
             MWS  [ 99  35 470]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.71130; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.71130 to 0.61161; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.61161 to 0.60188; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.60188 to 0.59813; runtime 0:00:07; BEST YET
Epoch 005: val_loss did not improve from 0.59813; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.59813; runtime 0:00:07
Epoch 007: val_loss did not improve from 0.59813; runtime 0:00:07
Fold 7 training runtime: 0:00:59

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.72      0.86      0.78       790
        HPL       0.73      0.81      0.77       563
        MWS       0.87      0.55      0.68       604

avg / total       0.77      0.75      0.75      1957

            ----- Confusion Matrix -----
True Labels  EAP  [683  68  39]
             HPL  [ 98 454  11]
             MWS  [170  99 335]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.64475; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.64475 to 0.58181; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.58181 to 0.56037; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.56037 to 0.52441; runtime 0:00:07; BEST YET
Epoch 005: val_loss did not improve from 0.52441; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.52441; runtime 0:00:07
Epoch 007: val_loss did not improve from 0.52441; runtime 0:00:07
Fold 8 training runtime: 0:00:59

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.82      0.79       790
        HPL       0.76      0.81      0.79       563
        MWS       0.83      0.69      0.76       604

avg / total       0.78      0.78      0.78      1957

            ----- Confusion Matrix -----
True Labels  EAP  [650  86  54]
             HPL  [ 76 458  29]
             MWS  [130  55 419]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.72808; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.72808 to 0.58729; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.58729 to 0.57082; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.57082; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.57082; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.57082; runtime 0:00:07
Fold 9 training runtime: 0:00:53

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.68      0.74       790
        HPL       0.65      0.92      0.76       563
        MWS       0.84      0.71      0.77       604

avg / total       0.78      0.76      0.76      1957

            ----- Confusion Matrix -----
True Labels  EAP  [537 187  66]
             HPL  [ 30 516  17]
             MWS  [ 85  91 428]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.63969; runtime 0:00:16; BEST YET
Epoch 002: val_loss improved from 0.63969 to 0.58580; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.58580 to 0.57368; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.57368; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.57368; runtime 0:00:07
Epoch 006: val_loss did not improve from 0.57368; runtime 0:00:07
Fold 10 training runtime: 0:00:53

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.85      0.79       790
        HPL       0.76      0.82      0.79       563
        MWS       0.88      0.63      0.74       604

avg / total       0.79      0.78      0.77      1957

            ----- Confusion Matrix -----
True Labels  EAP  [675  80  35]
             HPL  [ 84 462  17]
             MWS  [155  66 383]
                    EAP  HPL  MWS
                  Predicted Labels

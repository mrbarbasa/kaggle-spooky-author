_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 300)          8302800   
_________________________________________________________________
spatial_dropout1d_1 (Spatial (None, 128, 300)          0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128, 64)           85504     
_________________________________________________________________
global_max_pooling1d_1 (Glob (None, 64)                0         
_________________________________________________________________
output_layer (Dense)         (None, 3)                 195       
=================================================================
Total params: 8,388,499
Trainable params: 85,699
Non-trainable params: 8,302,800
_________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.68909; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.68909 to 0.64886; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.64886 to 0.59370; runtime 0:00:08; BEST YET
Epoch 004: val_loss improved from 0.59370 to 0.54883; runtime 0:00:08; BEST YET
Epoch 005: val_loss improved from 0.54883 to 0.51773; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.51773 to 0.50368; runtime 0:00:08; BEST YET
Epoch 007: val_loss did not improve from 0.50368; runtime 0:00:08
Epoch 008: val_loss improved from 0.50368 to 0.49949; runtime 0:00:08; BEST YET
Epoch 009: val_loss improved from 0.49949 to 0.47188; runtime 0:00:07; BEST YET
Epoch 010: val_loss improved from 0.47188 to 0.45237; runtime 0:00:07; BEST YET
Epoch 011: val_loss improved from 0.45237 to 0.44750; runtime 0:00:07; BEST YET
Epoch 012: val_loss improved from 0.44750 to 0.44623; runtime 0:00:08; BEST YET
Epoch 013: val_loss did not improve from 0.44623; runtime 0:00:07
Epoch 014: val_loss improved from 0.44623 to 0.43771; runtime 0:00:07; BEST YET
Epoch 015: val_loss improved from 0.43771 to 0.43302; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.43302; runtime 0:00:08
Epoch 017: val_loss did not improve from 0.43302; runtime 0:00:07
Epoch 018: val_loss did not improve from 0.43302; runtime 0:00:07
Fold 1 training runtime: 0:02:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.87      0.83       790
        HPL       0.88      0.77      0.82       564
        MWS       0.85      0.82      0.83       605

avg / total       0.83      0.83      0.83      1959

            ----- Confusion Matrix -----
True Labels  EAP  [691  41  58]
             HPL  [101 433  30]
             MWS  [ 93  18 494]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.68935; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.68935 to 0.60507; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.60507 to 0.59842; runtime 0:00:08; BEST YET
Epoch 004: val_loss improved from 0.59842 to 0.51103; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.51103 to 0.50419; runtime 0:00:08; BEST YET
Epoch 006: val_loss improved from 0.50419 to 0.48665; runtime 0:00:08; BEST YET
Epoch 007: val_loss improved from 0.48665 to 0.46225; runtime 0:00:08; BEST YET
Epoch 008: val_loss improved from 0.46225 to 0.45101; runtime 0:00:08; BEST YET
Epoch 009: val_loss did not improve from 0.45101; runtime 0:00:07
Epoch 010: val_loss improved from 0.45101 to 0.44298; runtime 0:00:08; BEST YET
Epoch 011: val_loss improved from 0.44298 to 0.43964; runtime 0:00:08; BEST YET
Epoch 012: val_loss improved from 0.43964 to 0.42351; runtime 0:00:08; BEST YET
Epoch 013: val_loss did not improve from 0.42351; runtime 0:00:08
Epoch 014: val_loss did not improve from 0.42351; runtime 0:00:08
Epoch 015: val_loss improved from 0.42351 to 0.41789; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.41789; runtime 0:00:07
Epoch 017: val_loss improved from 0.41789 to 0.40959; runtime 0:00:08; BEST YET
Epoch 018: val_loss improved from 0.40959 to 0.40495; runtime 0:00:07; BEST YET
Epoch 019: val_loss did not improve from 0.40495; runtime 0:00:08
Epoch 020: val_loss did not improve from 0.40495; runtime 0:00:07
Epoch 021: val_loss did not improve from 0.40495; runtime 0:00:08
Fold 2 training runtime: 0:02:38

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.86      0.84       790
        HPL       0.92      0.75      0.83       564
        MWS       0.79      0.88      0.84       605

avg / total       0.84      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [677  23  90]
             HPL  [ 91 425  48]
             MWS  [ 54  16 535]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.71137; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.71137 to 0.63422; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.63422 to 0.59925; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.59925 to 0.59192; runtime 0:00:08; BEST YET
Epoch 005: val_loss improved from 0.59192 to 0.55653; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.55653 to 0.52753; runtime 0:00:08; BEST YET
Epoch 007: val_loss did not improve from 0.52753; runtime 0:00:07
Epoch 008: val_loss did not improve from 0.52753; runtime 0:00:07
Epoch 009: val_loss improved from 0.52753 to 0.50554; runtime 0:00:07; BEST YET
Epoch 010: val_loss did not improve from 0.50554; runtime 0:00:07
Epoch 011: val_loss did not improve from 0.50554; runtime 0:00:07
Epoch 012: val_loss improved from 0.50554 to 0.48810; runtime 0:00:07; BEST YET
Epoch 013: val_loss improved from 0.48810 to 0.47871; runtime 0:00:07; BEST YET
Epoch 014: val_loss did not improve from 0.47871; runtime 0:00:07
Epoch 015: val_loss improved from 0.47871 to 0.47418; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.47418; runtime 0:00:07
Epoch 017: val_loss did not improve from 0.47418; runtime 0:00:07
Epoch 018: val_loss did not improve from 0.47418; runtime 0:00:08
Fold 3 training runtime: 0:02:14

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.86      0.81       790
        HPL       0.90      0.70      0.79       564
        MWS       0.78      0.82      0.80       605

avg / total       0.81      0.80      0.80      1959

            ----- Confusion Matrix -----
True Labels  EAP  [680  26  84]
             HPL  [117 394  53]
             MWS  [ 93  16 496]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.67270; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.67270 to 0.62518; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.62518 to 0.57768; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.57768 to 0.56673; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.56673 to 0.53227; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.53227 to 0.50875; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.50875 to 0.48432; runtime 0:00:08; BEST YET
Epoch 008: val_loss did not improve from 0.48432; runtime 0:00:07
Epoch 009: val_loss improved from 0.48432 to 0.46242; runtime 0:00:07; BEST YET
Epoch 010: val_loss did not improve from 0.46242; runtime 0:00:07
Epoch 011: val_loss improved from 0.46242 to 0.45719; runtime 0:00:07; BEST YET
Epoch 012: val_loss improved from 0.45719 to 0.45446; runtime 0:00:07; BEST YET
Epoch 013: val_loss did not improve from 0.45446; runtime 0:00:07
Epoch 014: val_loss improved from 0.45446 to 0.43122; runtime 0:00:07; BEST YET
Epoch 015: val_loss improved from 0.43122 to 0.40642; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.40642; runtime 0:00:08
Epoch 017: val_loss did not improve from 0.40642; runtime 0:00:08
Epoch 018: val_loss did not improve from 0.40642; runtime 0:00:08
Fold 4 training runtime: 0:02:14

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.85      0.83       790
        HPL       0.92      0.74      0.82       564
        MWS       0.78      0.86      0.82       605

avg / total       0.83      0.82      0.82      1959

            ----- Confusion Matrix -----
True Labels  EAP  [671  26  93]
             HPL  [ 89 419  56]
             MWS  [ 76   8 521]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.67252; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.67252 to 0.60750; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.60750 to 0.56059; runtime 0:00:08; BEST YET
Epoch 004: val_loss improved from 0.56059 to 0.54348; runtime 0:00:08; BEST YET
Epoch 005: val_loss improved from 0.54348 to 0.51362; runtime 0:00:08; BEST YET
Epoch 006: val_loss improved from 0.51362 to 0.49522; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.49522 to 0.48132; runtime 0:00:08; BEST YET
Epoch 008: val_loss did not improve from 0.48132; runtime 0:00:08
Epoch 009: val_loss improved from 0.48132 to 0.45569; runtime 0:00:08; BEST YET
Epoch 010: val_loss improved from 0.45569 to 0.44722; runtime 0:00:08; BEST YET
Epoch 011: val_loss improved from 0.44722 to 0.44364; runtime 0:00:08; BEST YET
Epoch 012: val_loss did not improve from 0.44364; runtime 0:00:08
Epoch 013: val_loss improved from 0.44364 to 0.42654; runtime 0:00:08; BEST YET
Epoch 014: val_loss did not improve from 0.42654; runtime 0:00:08
Epoch 015: val_loss improved from 0.42654 to 0.42107; runtime 0:00:08; BEST YET
Epoch 016: val_loss did not improve from 0.42107; runtime 0:00:08
Epoch 017: val_loss improved from 0.42107 to 0.41120; runtime 0:00:08; BEST YET
Epoch 018: val_loss did not improve from 0.41120; runtime 0:00:08
Epoch 019: val_loss improved from 0.41120 to 0.40964; runtime 0:00:08; BEST YET
Epoch 020: val_loss did not improve from 0.40964; runtime 0:00:08
Epoch 021: val_loss improved from 0.40964 to 0.40958; runtime 0:00:07; BEST YET
Epoch 022: val_loss did not improve from 0.40958; runtime 0:00:07
Epoch 023: val_loss did not improve from 0.40958; runtime 0:00:08
Epoch 024: val_loss did not improve from 0.40958; runtime 0:00:07
Fold 5 training runtime: 0:03:00

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.85      0.83       790
        HPL       0.85      0.86      0.86       564
        MWS       0.85      0.81      0.83       604

avg / total       0.84      0.84      0.84      1958

            ----- Confusion Matrix -----
True Labels  EAP  [668  55  67]
             HPL  [ 57 486  21]
             MWS  [ 86  28 490]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.67001; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.67001 to 0.61808; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.61808 to 0.58420; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.58420 to 0.57027; runtime 0:00:08; BEST YET
Epoch 005: val_loss improved from 0.57027 to 0.53883; runtime 0:00:08; BEST YET
Epoch 006: val_loss improved from 0.53883 to 0.53687; runtime 0:00:08; BEST YET
Epoch 007: val_loss improved from 0.53687 to 0.52761; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.52761 to 0.49848; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.49848 to 0.48793; runtime 0:00:08; BEST YET
Epoch 010: val_loss improved from 0.48793 to 0.47587; runtime 0:00:07; BEST YET
Epoch 011: val_loss did not improve from 0.47587; runtime 0:00:07
Epoch 012: val_loss improved from 0.47587 to 0.46318; runtime 0:00:08; BEST YET
Epoch 013: val_loss did not improve from 0.46318; runtime 0:00:07
Epoch 014: val_loss improved from 0.46318 to 0.46070; runtime 0:00:07; BEST YET
Epoch 015: val_loss did not improve from 0.46070; runtime 0:00:07
Epoch 016: val_loss did not improve from 0.46070; runtime 0:00:07
Epoch 017: val_loss did not improve from 0.46070; runtime 0:00:07
Fold 6 training runtime: 0:02:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.85      0.82       790
        HPL       0.84      0.85      0.85       563
        MWS       0.86      0.76      0.81       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [675  58  57]
             HPL  [ 64 480  19]
             MWS  [108  35 461]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.72189; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.72189 to 0.65217; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.65217 to 0.60659; runtime 0:00:07; BEST YET
Epoch 004: val_loss improved from 0.60659 to 0.58709; runtime 0:00:08; BEST YET
Epoch 005: val_loss improved from 0.58709 to 0.55145; runtime 0:00:07; BEST YET
Epoch 006: val_loss did not improve from 0.55145; runtime 0:00:08
Epoch 007: val_loss improved from 0.55145 to 0.53692; runtime 0:00:08; BEST YET
Epoch 008: val_loss improved from 0.53692 to 0.51453; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.51453 to 0.51031; runtime 0:00:07; BEST YET
Epoch 010: val_loss did not improve from 0.51031; runtime 0:00:07
Epoch 011: val_loss improved from 0.51031 to 0.49291; runtime 0:00:07; BEST YET
Epoch 012: val_loss improved from 0.49291 to 0.48484; runtime 0:00:07; BEST YET
Epoch 013: val_loss improved from 0.48484 to 0.47741; runtime 0:00:07; BEST YET
Epoch 014: val_loss improved from 0.47741 to 0.47508; runtime 0:00:08; BEST YET
Epoch 015: val_loss did not improve from 0.47508; runtime 0:00:08
Epoch 016: val_loss did not improve from 0.47508; runtime 0:00:07
Epoch 017: val_loss improved from 0.47508 to 0.46965; runtime 0:00:08; BEST YET
Epoch 018: val_loss improved from 0.46965 to 0.46759; runtime 0:00:08; BEST YET
Epoch 019: val_loss did not improve from 0.46759; runtime 0:00:08
Epoch 020: val_loss did not improve from 0.46759; runtime 0:00:08
Epoch 021: val_loss improved from 0.46759 to 0.46499; runtime 0:00:08; BEST YET
Epoch 022: val_loss did not improve from 0.46499; runtime 0:00:08
Epoch 023: val_loss did not improve from 0.46499; runtime 0:00:08
Epoch 024: val_loss did not improve from 0.46499; runtime 0:00:08
Fold 7 training runtime: 0:03:01

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.77      0.89      0.82       790
        HPL       0.88      0.78      0.83       563
        MWS       0.85      0.76      0.80       604

avg / total       0.82      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [700  34  56]
             HPL  [ 95 441  27]
             MWS  [118  26 460]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.67554; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.67554 to 0.64627; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.64627 to 0.57912; runtime 0:00:08; BEST YET
Epoch 004: val_loss improved from 0.57912 to 0.56091; runtime 0:00:07; BEST YET
Epoch 005: val_loss improved from 0.56091 to 0.54525; runtime 0:00:07; BEST YET
Epoch 006: val_loss improved from 0.54525 to 0.52037; runtime 0:00:07; BEST YET
Epoch 007: val_loss improved from 0.52037 to 0.48348; runtime 0:00:08; BEST YET
Epoch 008: val_loss did not improve from 0.48348; runtime 0:00:08
Epoch 009: val_loss did not improve from 0.48348; runtime 0:00:08
Epoch 010: val_loss improved from 0.48348 to 0.46014; runtime 0:00:08; BEST YET
Epoch 011: val_loss improved from 0.46014 to 0.43757; runtime 0:00:07; BEST YET
Epoch 012: val_loss improved from 0.43757 to 0.43643; runtime 0:00:08; BEST YET
Epoch 013: val_loss did not improve from 0.43643; runtime 0:00:08
Epoch 014: val_loss did not improve from 0.43643; runtime 0:00:07
Epoch 015: val_loss improved from 0.43643 to 0.42719; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.42719; runtime 0:00:08
Epoch 017: val_loss did not improve from 0.42719; runtime 0:00:08
Epoch 018: val_loss improved from 0.42719 to 0.41445; runtime 0:00:08; BEST YET
Epoch 019: val_loss did not improve from 0.41445; runtime 0:00:08
Epoch 020: val_loss did not improve from 0.41445; runtime 0:00:07
Epoch 021: val_loss did not improve from 0.41445; runtime 0:00:07
Fold 8 training runtime: 0:02:37

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.85      0.83       790
        HPL       0.82      0.86      0.84       563
        MWS       0.87      0.78      0.82       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [674  65  51]
             HPL  [ 61 483  19]
             MWS  [ 93  41 470]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.73889; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.73889 to 0.63734; runtime 0:00:07; BEST YET
Epoch 003: val_loss improved from 0.63734 to 0.59062; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.59062; runtime 0:00:08
Epoch 005: val_loss improved from 0.59062 to 0.52896; runtime 0:00:08; BEST YET
Epoch 006: val_loss improved from 0.52896 to 0.52539; runtime 0:00:08; BEST YET
Epoch 007: val_loss improved from 0.52539 to 0.51584; runtime 0:00:07; BEST YET
Epoch 008: val_loss improved from 0.51584 to 0.48240; runtime 0:00:07; BEST YET
Epoch 009: val_loss improved from 0.48240 to 0.48072; runtime 0:00:08; BEST YET
Epoch 010: val_loss did not improve from 0.48072; runtime 0:00:07
Epoch 011: val_loss did not improve from 0.48072; runtime 0:00:07
Epoch 012: val_loss improved from 0.48072 to 0.46682; runtime 0:00:08; BEST YET
Epoch 013: val_loss improved from 0.46682 to 0.46206; runtime 0:00:08; BEST YET
Epoch 014: val_loss improved from 0.46206 to 0.45023; runtime 0:00:08; BEST YET
Epoch 015: val_loss improved from 0.45023 to 0.44394; runtime 0:00:07; BEST YET
Epoch 016: val_loss did not improve from 0.44394; runtime 0:00:07
Epoch 017: val_loss improved from 0.44394 to 0.43273; runtime 0:00:08; BEST YET
Epoch 018: val_loss did not improve from 0.43273; runtime 0:00:08
Epoch 019: val_loss did not improve from 0.43273; runtime 0:00:07
Epoch 020: val_loss did not improve from 0.43273; runtime 0:00:08
Fold 9 training runtime: 0:02:30

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.87      0.82       790
        HPL       0.91      0.71      0.79       563
        MWS       0.82      0.83      0.82       604

avg / total       0.82      0.81      0.81      1957

            ----- Confusion Matrix -----
True Labels  EAP  [690  30  70]
             HPL  [122 398  43]
             MWS  [ 90  11 503]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.75035; runtime 0:00:08; BEST YET
Epoch 002: val_loss improved from 0.75035 to 0.60235; runtime 0:00:08; BEST YET
Epoch 003: val_loss improved from 0.60235 to 0.56547; runtime 0:00:07; BEST YET
Epoch 004: val_loss did not improve from 0.56547; runtime 0:00:07
Epoch 005: val_loss did not improve from 0.56547; runtime 0:00:07
Epoch 006: val_loss improved from 0.56547 to 0.51066; runtime 0:00:08; BEST YET
Epoch 007: val_loss did not improve from 0.51066; runtime 0:00:08
Epoch 008: val_loss improved from 0.51066 to 0.49469; runtime 0:00:08; BEST YET
Epoch 009: val_loss did not improve from 0.49469; runtime 0:00:08
Epoch 010: val_loss improved from 0.49469 to 0.46098; runtime 0:00:08; BEST YET
Epoch 011: val_loss improved from 0.46098 to 0.44561; runtime 0:00:07; BEST YET
Epoch 012: val_loss did not improve from 0.44561; runtime 0:00:08
Epoch 013: val_loss improved from 0.44561 to 0.44144; runtime 0:00:08; BEST YET
Epoch 014: val_loss did not improve from 0.44144; runtime 0:00:08
Epoch 015: val_loss did not improve from 0.44144; runtime 0:00:08
Epoch 016: val_loss improved from 0.44144 to 0.43424; runtime 0:00:08; BEST YET
Epoch 017: val_loss did not improve from 0.43424; runtime 0:00:08
Epoch 018: val_loss improved from 0.43424 to 0.43110; runtime 0:00:08; BEST YET
Epoch 019: val_loss did not improve from 0.43110; runtime 0:00:08
Epoch 020: val_loss did not improve from 0.43110; runtime 0:00:08
Epoch 021: val_loss did not improve from 0.43110; runtime 0:00:08
Fold 10 training runtime: 0:02:39

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.87      0.83       790
        HPL       0.92      0.73      0.82       563
        MWS       0.79      0.84      0.81       604

avg / total       0.83      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [687  24  79]
             HPL  [ 91 412  60]
             MWS  [ 83  11 510]
                    EAP  HPL  MWS
                  Predicted Labels

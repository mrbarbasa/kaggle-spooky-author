__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_layer (InputLayer)        (None, 128)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 128, 300)     8302800     input_layer[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 128, 128)     187392      spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] 
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
output_layer (Dense)            (None, 3)            771         concatenate_1[0][0]              
==================================================================================================
Total params: 8,490,963
Trainable params: 188,163
Non-trainable params: 8,302,800
__________________________________________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.73242; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.73242 to 0.71132; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.71132 to 0.67383; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.67383 to 0.60569; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.60569 to 0.56327; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.56327; runtime 0:00:02
Epoch 007: val_loss improved from 0.56327 to 0.55472; runtime 0:00:02; BEST YET
Epoch 008: val_loss did not improve from 0.55472; runtime 0:00:02
Epoch 009: val_loss improved from 0.55472 to 0.52908; runtime 0:00:02; BEST YET
Epoch 010: val_loss improved from 0.52908 to 0.49140; runtime 0:00:02; BEST YET
Epoch 011: val_loss did not improve from 0.49140; runtime 0:00:02
Epoch 012: val_loss did not improve from 0.49140; runtime 0:00:02
Epoch 013: val_loss improved from 0.49140 to 0.46334; runtime 0:00:02; BEST YET
Epoch 014: val_loss did not improve from 0.46334; runtime 0:00:02
Epoch 015: val_loss did not improve from 0.46334; runtime 0:00:02
Epoch 016: val_loss improved from 0.46334 to 0.45889; runtime 0:00:02; BEST YET
Epoch 017: val_loss improved from 0.45889 to 0.45171; runtime 0:00:02; BEST YET
Epoch 018: val_loss did not improve from 0.45171; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.45171; runtime 0:00:02
Epoch 020: val_loss did not improve from 0.45171; runtime 0:00:02
Fold 1 training runtime: 0:00:41

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.88      0.81       790
        HPL       0.91      0.68      0.78       564
        MWS       0.80      0.83      0.81       605

avg / total       0.82      0.81      0.80      1959

            ----- Confusion Matrix -----
True Labels  EAP  [692  26  72]
             HPL  [125 385  54]
             MWS  [ 94  10 501]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.73405; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.73405 to 0.67037; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.67037 to 0.60130; runtime 0:00:02; BEST YET
Epoch 004: val_loss did not improve from 0.60130; runtime 0:00:02
Epoch 005: val_loss improved from 0.60130 to 0.54912; runtime 0:00:02; BEST YET
Epoch 006: val_loss improved from 0.54912 to 0.51582; runtime 0:00:02; BEST YET
Epoch 007: val_loss improved from 0.51582 to 0.50210; runtime 0:00:02; BEST YET
Epoch 008: val_loss did not improve from 0.50210; runtime 0:00:02
Epoch 009: val_loss improved from 0.50210 to 0.47784; runtime 0:00:02; BEST YET
Epoch 010: val_loss improved from 0.47784 to 0.47093; runtime 0:00:02; BEST YET
Epoch 011: val_loss improved from 0.47093 to 0.46850; runtime 0:00:02; BEST YET
Epoch 012: val_loss improved from 0.46850 to 0.45231; runtime 0:00:02; BEST YET
Epoch 013: val_loss did not improve from 0.45231; runtime 0:00:02
Epoch 014: val_loss improved from 0.45231 to 0.43281; runtime 0:00:02; BEST YET
Epoch 015: val_loss did not improve from 0.43281; runtime 0:00:02
Epoch 016: val_loss improved from 0.43281 to 0.41268; runtime 0:00:02; BEST YET
Epoch 017: val_loss did not improve from 0.41268; runtime 0:00:02
Epoch 018: val_loss did not improve from 0.41268; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.41268; runtime 0:00:02
Fold 2 training runtime: 0:00:40

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.90      0.81       790
        HPL       0.96      0.67      0.79       564
        MWS       0.82      0.81      0.81       605

avg / total       0.82      0.81      0.80      1959

            ----- Confusion Matrix -----
True Labels  EAP  [709  12  69]
             HPL  [146 380  38]
             MWS  [111   5 489]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.78491; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.78491 to 0.72937; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.72937 to 0.65423; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.65423 to 0.63470; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.63470; runtime 0:00:02
Epoch 006: val_loss improved from 0.63470 to 0.63202; runtime 0:00:02; BEST YET
Epoch 007: val_loss improved from 0.63202 to 0.57364; runtime 0:00:02; BEST YET
Epoch 008: val_loss improved from 0.57364 to 0.54666; runtime 0:00:02; BEST YET
Epoch 009: val_loss did not improve from 0.54666; runtime 0:00:02
Epoch 010: val_loss did not improve from 0.54666; runtime 0:00:02
Epoch 011: val_loss improved from 0.54666 to 0.51685; runtime 0:00:02; BEST YET
Epoch 012: val_loss did not improve from 0.51685; runtime 0:00:02
Epoch 013: val_loss did not improve from 0.51685; runtime 0:00:02
Epoch 014: val_loss improved from 0.51685 to 0.49416; runtime 0:00:02; BEST YET
Epoch 015: val_loss improved from 0.49416 to 0.49197; runtime 0:00:02; BEST YET
Epoch 016: val_loss improved from 0.49197 to 0.47750; runtime 0:00:02; BEST YET
Epoch 017: val_loss did not improve from 0.47750; runtime 0:00:02
Epoch 018: val_loss did not improve from 0.47750; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.47750; runtime 0:00:02
Fold 3 training runtime: 0:00:40

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.75      0.87      0.81       790
        HPL       0.86      0.77      0.82       564
        MWS       0.84      0.74      0.79       605

avg / total       0.81      0.80      0.80      1959

            ----- Confusion Matrix -----
True Labels  EAP  [688  45  57]
             HPL  [ 96 437  31]
             MWS  [131  26 448]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.77056; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.77056 to 0.70275; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.70275 to 0.61735; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.61735 to 0.59042; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.59042; runtime 0:00:02
Epoch 006: val_loss improved from 0.59042 to 0.54863; runtime 0:00:02; BEST YET
Epoch 007: val_loss improved from 0.54863 to 0.53054; runtime 0:00:02; BEST YET
Epoch 008: val_loss improved from 0.53054 to 0.49468; runtime 0:00:02; BEST YET
Epoch 009: val_loss did not improve from 0.49468; runtime 0:00:02
Epoch 010: val_loss did not improve from 0.49468; runtime 0:00:02
Epoch 011: val_loss improved from 0.49468 to 0.46950; runtime 0:00:02; BEST YET
Epoch 012: val_loss did not improve from 0.46950; runtime 0:00:02
Epoch 013: val_loss improved from 0.46950 to 0.45583; runtime 0:00:02; BEST YET
Epoch 014: val_loss improved from 0.45583 to 0.43565; runtime 0:00:02; BEST YET
Epoch 015: val_loss did not improve from 0.43565; runtime 0:00:02
Epoch 016: val_loss did not improve from 0.43565; runtime 0:00:02
Epoch 017: val_loss did not improve from 0.43565; runtime 0:00:02
Fold 4 training runtime: 0:00:35

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.92      0.82       790
        HPL       0.94      0.66      0.78       564
        MWS       0.84      0.81      0.82       605

avg / total       0.83      0.81      0.81      1959

            ----- Confusion Matrix -----
True Labels  EAP  [723  18  49]
             HPL  [145 374  45]
             MWS  [112   4 489]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.70327; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.70327 to 0.70207; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.70207 to 0.62319; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.62319 to 0.59175; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.59175 to 0.54970; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.54970; runtime 0:00:02
Epoch 007: val_loss improved from 0.54970 to 0.52007; runtime 0:00:02; BEST YET
Epoch 008: val_loss improved from 0.52007 to 0.50136; runtime 0:00:02; BEST YET
Epoch 009: val_loss improved from 0.50136 to 0.49281; runtime 0:00:02; BEST YET
Epoch 010: val_loss did not improve from 0.49281; runtime 0:00:02
Epoch 011: val_loss did not improve from 0.49281; runtime 0:00:02
Epoch 012: val_loss improved from 0.49281 to 0.46606; runtime 0:00:02; BEST YET
Epoch 013: val_loss did not improve from 0.46606; runtime 0:00:02
Epoch 014: val_loss did not improve from 0.46606; runtime 0:00:02
Epoch 015: val_loss did not improve from 0.46606; runtime 0:00:02
Fold 5 training runtime: 0:00:32

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.92      0.82       790
        HPL       0.94      0.69      0.79       564
        MWS       0.85      0.79      0.82       604

avg / total       0.83      0.81      0.81      1958

            ----- Confusion Matrix -----
True Labels  EAP  [725  18  47]
             HPL  [136 387  41]
             MWS  [118   6 480]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.69073; runtime 0:00:03; BEST YET
Epoch 002: val_loss did not improve from 0.69073; runtime 0:00:02
Epoch 003: val_loss improved from 0.69073 to 0.60964; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.60964 to 0.59789; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.59789; runtime 0:00:02
Epoch 006: val_loss improved from 0.59789 to 0.54403; runtime 0:00:02; BEST YET
Epoch 007: val_loss improved from 0.54403 to 0.52345; runtime 0:00:02; BEST YET
Epoch 008: val_loss did not improve from 0.52345; runtime 0:00:02
Epoch 009: val_loss improved from 0.52345 to 0.49802; runtime 0:00:02; BEST YET
Epoch 010: val_loss improved from 0.49802 to 0.48488; runtime 0:00:02; BEST YET
Epoch 011: val_loss did not improve from 0.48488; runtime 0:00:02
Epoch 012: val_loss improved from 0.48488 to 0.46836; runtime 0:00:02; BEST YET
Epoch 013: val_loss improved from 0.46836 to 0.46280; runtime 0:00:02; BEST YET
Epoch 014: val_loss did not improve from 0.46280; runtime 0:00:02
Epoch 015: val_loss did not improve from 0.46280; runtime 0:00:02
Epoch 016: val_loss improved from 0.46280 to 0.45946; runtime 0:00:02; BEST YET
Epoch 017: val_loss improved from 0.45946 to 0.44948; runtime 0:00:02; BEST YET
Epoch 018: val_loss did not improve from 0.44948; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.44948; runtime 0:00:02
Epoch 020: val_loss did not improve from 0.44948; runtime 0:00:02
Fold 6 training runtime: 0:00:42

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.85      0.81       790
        HPL       0.93      0.71      0.80       563
        MWS       0.76      0.84      0.80       604

avg / total       0.82      0.81      0.81      1957

            ----- Confusion Matrix -----
True Labels  EAP  [669  22  99]
             HPL  [104 397  62]
             MWS  [ 87   7 510]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.76863; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.76863 to 0.71961; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.71961 to 0.66606; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.66606 to 0.65387; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.65387 to 0.60324; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.60324; runtime 0:00:02
Epoch 007: val_loss improved from 0.60324 to 0.59516; runtime 0:00:02; BEST YET
Epoch 008: val_loss improved from 0.59516 to 0.56213; runtime 0:00:02; BEST YET
Epoch 009: val_loss did not improve from 0.56213; runtime 0:00:02
Epoch 010: val_loss did not improve from 0.56213; runtime 0:00:02
Epoch 011: val_loss improved from 0.56213 to 0.51599; runtime 0:00:02; BEST YET
Epoch 012: val_loss did not improve from 0.51599; runtime 0:00:02
Epoch 013: val_loss improved from 0.51599 to 0.51466; runtime 0:00:02; BEST YET
Epoch 014: val_loss improved from 0.51466 to 0.49046; runtime 0:00:02; BEST YET
Epoch 015: val_loss did not improve from 0.49046; runtime 0:00:02
Epoch 016: val_loss improved from 0.49046 to 0.48413; runtime 0:00:02; BEST YET
Epoch 017: val_loss did not improve from 0.48413; runtime 0:00:02
Epoch 018: val_loss improved from 0.48413 to 0.47500; runtime 0:00:02; BEST YET
Epoch 019: val_loss did not improve from 0.47500; runtime 0:00:02
Epoch 020: val_loss improved from 0.47500 to 0.46546; runtime 0:00:02; BEST YET
Epoch 021: val_loss improved from 0.46546 to 0.45962; runtime 0:00:02; BEST YET
Epoch 022: val_loss improved from 0.45962 to 0.45228; runtime 0:00:02; BEST YET
Epoch 023: val_loss did not improve from 0.45228; runtime 0:00:02
Epoch 024: val_loss did not improve from 0.45228; runtime 0:00:02
Epoch 025: val_loss did not improve from 0.45228; runtime 0:00:02
Fold 7 training runtime: 0:00:52

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.90      0.82       790
        HPL       0.90      0.77      0.83       563
        MWS       0.85      0.76      0.80       604

avg / total       0.83      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [709  25  56]
             HPL  [103 432  28]
             MWS  [119  23 462]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.68466; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.68466 to 0.65756; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.65756 to 0.62902; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.62902 to 0.57710; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.57710; runtime 0:00:02
Epoch 006: val_loss improved from 0.57710 to 0.53888; runtime 0:00:02; BEST YET
Epoch 007: val_loss improved from 0.53888 to 0.51428; runtime 0:00:02; BEST YET
Epoch 008: val_loss improved from 0.51428 to 0.49827; runtime 0:00:02; BEST YET
Epoch 009: val_loss did not improve from 0.49827; runtime 0:00:02
Epoch 010: val_loss improved from 0.49827 to 0.49277; runtime 0:00:02; BEST YET
Epoch 011: val_loss improved from 0.49277 to 0.48912; runtime 0:00:02; BEST YET
Epoch 012: val_loss did not improve from 0.48912; runtime 0:00:02
Epoch 013: val_loss improved from 0.48912 to 0.45758; runtime 0:00:02; BEST YET
Epoch 014: val_loss improved from 0.45758 to 0.43423; runtime 0:00:02; BEST YET
Epoch 015: val_loss improved from 0.43423 to 0.42378; runtime 0:00:02; BEST YET
Epoch 016: val_loss did not improve from 0.42378; runtime 0:00:02
Epoch 017: val_loss improved from 0.42378 to 0.42211; runtime 0:00:02; BEST YET
Epoch 018: val_loss did not improve from 0.42211; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.42211; runtime 0:00:02
Epoch 020: val_loss did not improve from 0.42211; runtime 0:00:02
Fold 8 training runtime: 0:00:42

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.84      0.83       790
        HPL       0.78      0.87      0.83       563
        MWS       0.87      0.76      0.81       604

avg / total       0.82      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [660  79  51]
             HPL  [ 55 491  17]
             MWS  [ 91  56 457]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.73389; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.73389 to 0.64888; runtime 0:00:02; BEST YET
Epoch 003: val_loss did not improve from 0.64888; runtime 0:00:02
Epoch 004: val_loss improved from 0.64888 to 0.60784; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.60784 to 0.57654; runtime 0:00:02; BEST YET
Epoch 006: val_loss improved from 0.57654 to 0.53991; runtime 0:00:02; BEST YET
Epoch 007: val_loss did not improve from 0.53991; runtime 0:00:02
Epoch 008: val_loss improved from 0.53991 to 0.52355; runtime 0:00:02; BEST YET
Epoch 009: val_loss improved from 0.52355 to 0.50709; runtime 0:00:02; BEST YET
Epoch 010: val_loss did not improve from 0.50709; runtime 0:00:02
Epoch 011: val_loss did not improve from 0.50709; runtime 0:00:02
Epoch 012: val_loss improved from 0.50709 to 0.47445; runtime 0:00:02; BEST YET
Epoch 013: val_loss improved from 0.47445 to 0.46985; runtime 0:00:02; BEST YET
Epoch 014: val_loss improved from 0.46985 to 0.46099; runtime 0:00:02; BEST YET
Epoch 015: val_loss did not improve from 0.46099; runtime 0:00:02
Epoch 016: val_loss did not improve from 0.46099; runtime 0:00:02
Epoch 017: val_loss did not improve from 0.46099; runtime 0:00:02
Fold 9 training runtime: 0:00:36

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.74      0.89      0.81       790
        HPL       0.94      0.66      0.77       563
        MWS       0.80      0.80      0.80       604

avg / total       0.82      0.80      0.80      1957

            ----- Confusion Matrix -----
True Labels  EAP  [707  19  64]
             HPL  [135 371  57]
             MWS  [114   5 485]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.71555; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.71555 to 0.63233; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.63233 to 0.61423; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.61423 to 0.55871; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.55871 to 0.55292; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.55292; runtime 0:00:02
Epoch 007: val_loss improved from 0.55292 to 0.52261; runtime 0:00:02; BEST YET
Epoch 008: val_loss did not improve from 0.52261; runtime 0:00:02
Epoch 009: val_loss did not improve from 0.52261; runtime 0:00:02
Epoch 010: val_loss improved from 0.52261 to 0.49334; runtime 0:00:02; BEST YET
Epoch 011: val_loss did not improve from 0.49334; runtime 0:00:02
Epoch 012: val_loss improved from 0.49334 to 0.48869; runtime 0:00:02; BEST YET
Epoch 013: val_loss did not improve from 0.48869; runtime 0:00:02
Epoch 014: val_loss improved from 0.48869 to 0.46923; runtime 0:00:02; BEST YET
Epoch 015: val_loss did not improve from 0.46923; runtime 0:00:02
Epoch 016: val_loss improved from 0.46923 to 0.44789; runtime 0:00:02; BEST YET
Epoch 017: val_loss did not improve from 0.44789; runtime 0:00:02
Epoch 018: val_loss did not improve from 0.44789; runtime 0:00:02
Epoch 019: val_loss did not improve from 0.44789; runtime 0:00:02
Fold 10 training runtime: 0:00:39

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.76      0.90      0.83       790
        HPL       0.90      0.72      0.80       563
        MWS       0.82      0.77      0.79       604

avg / total       0.82      0.81      0.81      1957

            ----- Confusion Matrix -----
True Labels  EAP  [711  27  52]
             HPL  [102 408  53]
             MWS  [120  17 467]
                    EAP  HPL  MWS
                  Predicted Labels

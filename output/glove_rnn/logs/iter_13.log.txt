__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_layer (InputLayer)        (None, 128)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 128, 300)     8302800     input_layer[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 128, 600)     1444800     spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
spatial_dropout1d_2 (SpatialDro (None, 128, 600)     0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
bidirectional_2 (Bidirectional) (None, 128, 600)     2164800     spatial_dropout1d_2[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 600)          0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 600)          0           bidirectional_2[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 1200)         0           global_average_pooling1d_1[0][0] 
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
output_layer (Dense)            (None, 3)            3603        concatenate_1[0][0]              
==================================================================================================
Total params: 11,916,003
Trainable params: 3,613,203
Non-trainable params: 8,302,800
__________________________________________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.65698; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.65698 to 0.62318; runtime 0:00:20; BEST YET
Epoch 003: val_loss improved from 0.62318 to 0.50171; runtime 0:00:21; BEST YET
Epoch 004: val_loss improved from 0.50171 to 0.46972; runtime 0:00:21; BEST YET
Epoch 005: val_loss did not improve from 0.46972; runtime 0:00:20
Epoch 006: val_loss improved from 0.46972 to 0.43837; runtime 0:00:20; BEST YET
Epoch 007: val_loss improved from 0.43837 to 0.40878; runtime 0:00:21; BEST YET
Epoch 008: val_loss did not improve from 0.40878; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.40878; runtime 0:00:21
Epoch 010: val_loss did not improve from 0.40878; runtime 0:00:21
Fold 1 training runtime: 0:03:27

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.83      0.85      0.84       790
        HPL       0.89      0.79      0.84       564
        MWS       0.82      0.88      0.85       605

avg / total       0.85      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [672  45  73]
             HPL  [ 74 447  43]
             MWS  [ 60  11 534]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.62784; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.62784 to 0.57877; runtime 0:00:20; BEST YET
Epoch 003: val_loss improved from 0.57877 to 0.48416; runtime 0:00:20; BEST YET
Epoch 004: val_loss improved from 0.48416 to 0.45042; runtime 0:00:20; BEST YET
Epoch 005: val_loss improved from 0.45042 to 0.43671; runtime 0:00:20; BEST YET
Epoch 006: val_loss improved from 0.43671 to 0.40747; runtime 0:00:20; BEST YET
Epoch 007: val_loss did not improve from 0.40747; runtime 0:00:20
Epoch 008: val_loss did not improve from 0.40747; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.40747; runtime 0:00:21
Fold 2 training runtime: 0:03:06

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.85      0.84       790
        HPL       0.85      0.86      0.85       564
        MWS       0.86      0.81      0.83       605

avg / total       0.84      0.84      0.84      1959

            ----- Confusion Matrix -----
True Labels  EAP  [675  51  64]
             HPL  [ 64 484  16]
             MWS  [ 83  34 488]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.66840; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.66840 to 0.56890; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.56890 to 0.52877; runtime 0:00:21; BEST YET
Epoch 004: val_loss improved from 0.52877 to 0.48470; runtime 0:00:21; BEST YET
Epoch 005: val_loss improved from 0.48470 to 0.44911; runtime 0:00:21; BEST YET
Epoch 006: val_loss improved from 0.44911 to 0.43526; runtime 0:00:21; BEST YET
Epoch 007: val_loss did not improve from 0.43526; runtime 0:00:21
Epoch 008: val_loss did not improve from 0.43526; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.43526; runtime 0:00:21
Fold 3 training runtime: 0:03:08

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.84      0.83       790
        HPL       0.83      0.82      0.82       564
        MWS       0.82      0.80      0.81       605

avg / total       0.82      0.82      0.82      1959

            ----- Confusion Matrix -----
True Labels  EAP  [660  63  67]
             HPL  [ 61 463  40]
             MWS  [ 84  34 487]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.63844; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.63844 to 0.54772; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.54772 to 0.49100; runtime 0:00:21; BEST YET
Epoch 004: val_loss improved from 0.49100 to 0.44032; runtime 0:00:21; BEST YET
Epoch 005: val_loss improved from 0.44032 to 0.41977; runtime 0:00:21; BEST YET
Epoch 006: val_loss improved from 0.41977 to 0.38469; runtime 0:00:21; BEST YET
Epoch 007: val_loss did not improve from 0.38469; runtime 0:00:21
Epoch 008: val_loss did not improve from 0.38469; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.38469; runtime 0:00:21
Fold 4 training runtime: 0:03:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.88      0.85       790
        HPL       0.90      0.81      0.86       564
        MWS       0.85      0.85      0.85       605

avg / total       0.85      0.85      0.85      1959

            ----- Confusion Matrix -----
True Labels  EAP  [696  37  57]
             HPL  [ 75 459  30]
             MWS  [ 80  12 513]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.65321; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.65321 to 0.52509; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.52509 to 0.47778; runtime 0:00:21; BEST YET
Epoch 004: val_loss did not improve from 0.47778; runtime 0:00:21
Epoch 005: val_loss improved from 0.47778 to 0.42574; runtime 0:00:21; BEST YET
Epoch 006: val_loss improved from 0.42574 to 0.40653; runtime 0:00:21; BEST YET
Epoch 007: val_loss did not improve from 0.40653; runtime 0:00:21
Epoch 008: val_loss did not improve from 0.40653; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.40653; runtime 0:00:21
Fold 5 training runtime: 0:03:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.91      0.85       790
        HPL       0.92      0.82      0.87       564
        MWS       0.88      0.81      0.85       604

avg / total       0.86      0.85      0.85      1958

            ----- Confusion Matrix -----
True Labels  EAP  [717  31  42]
             HPL  [ 78 464  22]
             MWS  [102  12 490]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.63080; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.63080 to 0.54925; runtime 0:00:20; BEST YET
Epoch 003: val_loss improved from 0.54925 to 0.50698; runtime 0:00:21; BEST YET
Epoch 004: val_loss did not improve from 0.50698; runtime 0:00:20
Epoch 005: val_loss improved from 0.50698 to 0.45669; runtime 0:00:21; BEST YET
Epoch 006: val_loss improved from 0.45669 to 0.45047; runtime 0:00:21; BEST YET
Epoch 007: val_loss did not improve from 0.45047; runtime 0:00:21
Epoch 008: val_loss did not improve from 0.45047; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.45047; runtime 0:00:21
Fold 6 training runtime: 0:03:07

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.88      0.84       790
        HPL       0.87      0.83      0.85       563
        MWS       0.86      0.77      0.81       604

avg / total       0.84      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [698  40  52]
             HPL  [ 70 468  25]
             MWS  [112  27 465]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.64767; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.64767 to 0.58372; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.58372 to 0.56763; runtime 0:00:21; BEST YET
Epoch 004: val_loss improved from 0.56763 to 0.49579; runtime 0:00:21; BEST YET
Epoch 005: val_loss did not improve from 0.49579; runtime 0:00:21
Epoch 006: val_loss improved from 0.49579 to 0.46111; runtime 0:00:21; BEST YET
Epoch 007: val_loss improved from 0.46111 to 0.45386; runtime 0:00:21; BEST YET
Epoch 008: val_loss did not improve from 0.45386; runtime 0:00:21
Epoch 009: val_loss improved from 0.45386 to 0.44758; runtime 0:00:21; BEST YET
Epoch 010: val_loss did not improve from 0.44758; runtime 0:00:21
Epoch 011: val_loss did not improve from 0.44758; runtime 0:00:21
Epoch 012: val_loss did not improve from 0.44758; runtime 0:00:21
Fold 7 training runtime: 0:04:09

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.85      0.83       790
        HPL       0.91      0.78      0.84       563
        MWS       0.78      0.83      0.80       604

avg / total       0.83      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [670  23  97]
             HPL  [ 78 438  47]
             MWS  [ 84  19 501]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.64823; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.64823 to 0.55555; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.55555 to 0.49939; runtime 0:00:21; BEST YET
Epoch 004: val_loss improved from 0.49939 to 0.48655; runtime 0:00:21; BEST YET
Epoch 005: val_loss improved from 0.48655 to 0.43996; runtime 0:00:21; BEST YET
Epoch 006: val_loss did not improve from 0.43996; runtime 0:00:21
Epoch 007: val_loss improved from 0.43996 to 0.41686; runtime 0:00:21; BEST YET
Epoch 008: val_loss did not improve from 0.41686; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.41686; runtime 0:00:21
Epoch 010: val_loss did not improve from 0.41686; runtime 0:00:21
Fold 8 training runtime: 0:03:28

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.88      0.84       790
        HPL       0.91      0.76      0.83       563
        MWS       0.83      0.86      0.84       604

avg / total       0.84      0.84      0.84      1957

            ----- Confusion Matrix -----
True Labels  EAP  [698  32  60]
             HPL  [ 91 429  43]
             MWS  [ 76  11 517]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.63239; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.63239 to 0.60796; runtime 0:00:21; BEST YET
Epoch 003: val_loss improved from 0.60796 to 0.52193; runtime 0:00:20; BEST YET
Epoch 004: val_loss improved from 0.52193 to 0.45911; runtime 0:00:21; BEST YET
Epoch 005: val_loss did not improve from 0.45911; runtime 0:00:21
Epoch 006: val_loss did not improve from 0.45911; runtime 0:00:21
Epoch 007: val_loss improved from 0.45911 to 0.42115; runtime 0:00:21; BEST YET
Epoch 008: val_loss did not improve from 0.42115; runtime 0:00:21
Epoch 009: val_loss did not improve from 0.42115; runtime 0:00:21
Epoch 010: val_loss did not improve from 0.42115; runtime 0:00:21
Fold 9 training runtime: 0:03:27

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.88      0.83       790
        HPL       0.92      0.75      0.83       563
        MWS       0.83      0.84      0.84       604

avg / total       0.84      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [694  30  66]
             HPL  [103 422  38]
             MWS  [ 89   7 508]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.61743; runtime 0:00:22; BEST YET
Epoch 002: val_loss improved from 0.61743 to 0.57736; runtime 0:00:20; BEST YET
Epoch 003: val_loss improved from 0.57736 to 0.49632; runtime 0:00:20; BEST YET
Epoch 004: val_loss improved from 0.49632 to 0.49280; runtime 0:00:21; BEST YET
Epoch 005: val_loss improved from 0.49280 to 0.47426; runtime 0:00:20; BEST YET
Epoch 006: val_loss did not improve from 0.47426; runtime 0:00:21
Epoch 007: val_loss improved from 0.47426 to 0.43734; runtime 0:00:21; BEST YET
Epoch 008: val_loss improved from 0.43734 to 0.40172; runtime 0:00:20; BEST YET
Epoch 009: val_loss did not improve from 0.40172; runtime 0:00:20
Epoch 010: val_loss did not improve from 0.40172; runtime 0:00:21
Epoch 011: val_loss did not improve from 0.40172; runtime 0:00:20
Fold 10 training runtime: 0:03:47

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.81      0.86      0.83       790
        HPL       0.89      0.80      0.84       563
        MWS       0.80      0.82      0.81       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [676  35  79]
             HPL  [ 71 449  43]
             MWS  [ 84  22 498]
                    EAP  HPL  MWS
                  Predicted Labels

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_layer (InputLayer)        (None, 128)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 128, 300)     8302800     input_layer[0][0]                
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (None, 128, 300)     0           embedding_1[0][0]                
__________________________________________________________________________________________________
bidirectional_1 (Bidirectional) (None, 128, 128)     140544      spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] 
                                                                 global_max_pooling1d_1[0][0]     
__________________________________________________________________________________________________
output_layer (Dense)            (None, 3)            771         concatenate_1[0][0]              
==================================================================================================
Total params: 8,444,115
Trainable params: 141,315
Non-trainable params: 8,302,800
__________________________________________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.76137; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.76137 to 0.66706; runtime 0:00:01; BEST YET
Epoch 003: val_loss did not improve from 0.66706; runtime 0:00:01
Epoch 004: val_loss improved from 0.66706 to 0.60901; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.60901 to 0.58990; runtime 0:00:01; BEST YET
Epoch 006: val_loss did not improve from 0.58990; runtime 0:00:01
Epoch 007: val_loss did not improve from 0.58990; runtime 0:00:01
Epoch 008: val_loss did not improve from 0.58990; runtime 0:00:01
Fold 1 training runtime: 0:00:10

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.64      0.90      0.75       790
        HPL       0.92      0.52      0.67       564
        MWS       0.80      0.71      0.75       605

avg / total       0.77      0.73      0.73      1959

            ----- Confusion Matrix -----
True Labels  EAP  [710  13  67]
             HPL  [229 296  39]
             MWS  [166  12 427]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.75879; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.75879 to 0.68166; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.68166 to 0.62750; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.62750 to 0.61676; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.61676 to 0.58414; runtime 0:00:01; BEST YET
Epoch 006: val_loss did not improve from 0.58414; runtime 0:00:01
Epoch 007: val_loss improved from 0.58414 to 0.54147; runtime 0:00:01; BEST YET
Epoch 008: val_loss improved from 0.54147 to 0.54070; runtime 0:00:01; BEST YET
Epoch 009: val_loss did not improve from 0.54070; runtime 0:00:01
Epoch 010: val_loss did not improve from 0.54070; runtime 0:00:01
Epoch 011: val_loss did not improve from 0.54070; runtime 0:00:01
Fold 2 training runtime: 0:00:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.72      0.85      0.78       790
        HPL       0.94      0.57      0.71       564
        MWS       0.73      0.82      0.77       605

avg / total       0.79      0.76      0.76      1959

            ----- Confusion Matrix -----
True Labels  EAP  [675  15 100]
             HPL  [157 319  88]
             MWS  [101   6 498]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.79248; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.79248 to 0.72971; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.72971 to 0.65475; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.65475 to 0.64264; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.64264 to 0.62738; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.62738 to 0.59208; runtime 0:00:01; BEST YET
Epoch 007: val_loss did not improve from 0.59208; runtime 0:00:01
Epoch 008: val_loss did not improve from 0.59208; runtime 0:00:01
Epoch 009: val_loss improved from 0.59208 to 0.58426; runtime 0:00:01; BEST YET
Epoch 010: val_loss improved from 0.58426 to 0.55740; runtime 0:00:01; BEST YET
Epoch 011: val_loss improved from 0.55740 to 0.55126; runtime 0:00:01; BEST YET
Epoch 012: val_loss did not improve from 0.55126; runtime 0:00:01
Epoch 013: val_loss did not improve from 0.55126; runtime 0:00:01
Epoch 014: val_loss did not improve from 0.55126; runtime 0:00:01
Fold 3 training runtime: 0:00:16

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.87      0.79       790
        HPL       0.92      0.65      0.76       564
        MWS       0.78      0.79      0.78       605

avg / total       0.80      0.78      0.78      1959

            ----- Confusion Matrix -----
True Labels  EAP  [688  20  82]
             HPL  [143 365  56]
             MWS  [116  13 476]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.76932; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.76932 to 0.72280; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.72280 to 0.67784; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.67784 to 0.64820; runtime 0:00:01; BEST YET
Epoch 005: val_loss did not improve from 0.64820; runtime 0:00:01
Epoch 006: val_loss improved from 0.64820 to 0.60969; runtime 0:00:01; BEST YET
Epoch 007: val_loss improved from 0.60969 to 0.55143; runtime 0:00:01; BEST YET
Epoch 008: val_loss improved from 0.55143 to 0.53168; runtime 0:00:01; BEST YET
Epoch 009: val_loss did not improve from 0.53168; runtime 0:00:01
Epoch 010: val_loss did not improve from 0.53168; runtime 0:00:01
Epoch 011: val_loss improved from 0.53168 to 0.53117; runtime 0:00:01; BEST YET
Epoch 012: val_loss did not improve from 0.53117; runtime 0:00:01
Epoch 013: val_loss improved from 0.53117 to 0.50166; runtime 0:00:01; BEST YET
Epoch 014: val_loss improved from 0.50166 to 0.48381; runtime 0:00:01; BEST YET
Epoch 015: val_loss improved from 0.48381 to 0.46685; runtime 0:00:01; BEST YET
Epoch 016: val_loss did not improve from 0.46685; runtime 0:00:01
Epoch 017: val_loss did not improve from 0.46685; runtime 0:00:01
Epoch 018: val_loss did not improve from 0.46685; runtime 0:00:01
Fold 4 training runtime: 0:00:21

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.72      0.91      0.81       790
        HPL       0.95      0.65      0.77       564
        MWS       0.83      0.79      0.81       605

avg / total       0.82      0.80      0.80      1959

            ----- Confusion Matrix -----
True Labels  EAP  [719  14  57]
             HPL  [152 368  44]
             MWS  [124   4 477]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.74879; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.74879 to 0.71980; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.71980 to 0.63937; runtime 0:00:01; BEST YET
Epoch 004: val_loss did not improve from 0.63937; runtime 0:00:01
Epoch 005: val_loss improved from 0.63937 to 0.61799; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.61799 to 0.60119; runtime 0:00:01; BEST YET
Epoch 007: val_loss improved from 0.60119 to 0.56492; runtime 0:00:01; BEST YET
Epoch 008: val_loss did not improve from 0.56492; runtime 0:00:01
Epoch 009: val_loss improved from 0.56492 to 0.52769; runtime 0:00:01; BEST YET
Epoch 010: val_loss did not improve from 0.52769; runtime 0:00:01
Epoch 011: val_loss did not improve from 0.52769; runtime 0:00:01
Epoch 012: val_loss did not improve from 0.52769; runtime 0:00:01
Fold 5 training runtime: 0:00:14

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.87      0.79       790
        HPL       0.93      0.63      0.76       564
        MWS       0.77      0.81      0.79       604

avg / total       0.80      0.78      0.78      1958

            ----- Confusion Matrix -----
True Labels  EAP  [685  17  88]
             HPL  [146 358  60]
             MWS  [107   8 489]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.74597; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.74597 to 0.68184; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.68184 to 0.62971; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.62971 to 0.62595; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.62595 to 0.58733; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.58733 to 0.57819; runtime 0:00:01; BEST YET
Epoch 007: val_loss improved from 0.57819 to 0.56882; runtime 0:00:01; BEST YET
Epoch 008: val_loss improved from 0.56882 to 0.55237; runtime 0:00:01; BEST YET
Epoch 009: val_loss improved from 0.55237 to 0.55120; runtime 0:00:01; BEST YET
Epoch 010: val_loss improved from 0.55120 to 0.52628; runtime 0:00:01; BEST YET
Epoch 011: val_loss did not improve from 0.52628; runtime 0:00:01
Epoch 012: val_loss did not improve from 0.52628; runtime 0:00:01
Epoch 013: val_loss improved from 0.52628 to 0.52568; runtime 0:00:01; BEST YET
Epoch 014: val_loss improved from 0.52568 to 0.50733; runtime 0:00:01; BEST YET
Epoch 015: val_loss did not improve from 0.50733; runtime 0:00:01
Epoch 016: val_loss did not improve from 0.50733; runtime 0:00:01
Epoch 017: val_loss did not improve from 0.50733; runtime 0:00:01
Fold 6 training runtime: 0:00:20

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.70      0.90      0.79       790
        HPL       0.86      0.76      0.81       563
        MWS       0.88      0.65      0.75       604

avg / total       0.80      0.78      0.78      1957

            ----- Confusion Matrix -----
True Labels  EAP  [711  41  38]
             HPL  [119 429  15]
             MWS  [183  30 391]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.81759; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.81759 to 0.69071; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.69071 to 0.67947; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.67947 to 0.66632; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.66632 to 0.66290; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.66290 to 0.62965; runtime 0:00:01; BEST YET
Epoch 007: val_loss improved from 0.62965 to 0.62778; runtime 0:00:01; BEST YET
Epoch 008: val_loss improved from 0.62778 to 0.57287; runtime 0:00:01; BEST YET
Epoch 009: val_loss improved from 0.57287 to 0.55933; runtime 0:00:01; BEST YET
Epoch 010: val_loss did not improve from 0.55933; runtime 0:00:01
Epoch 011: val_loss did not improve from 0.55933; runtime 0:00:01
Epoch 012: val_loss did not improve from 0.55933; runtime 0:00:01
Fold 7 training runtime: 0:00:14

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.65      0.93      0.76       790
        HPL       0.93      0.55      0.69       563
        MWS       0.82      0.67      0.74       604

avg / total       0.78      0.74      0.73      1957

            ----- Confusion Matrix -----
True Labels  EAP  [732  12  46]
             HPL  [212 308  43]
             MWS  [187  10 407]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.75592; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.75592 to 0.75223; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.75223 to 0.64269; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.64269 to 0.62392; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.62392 to 0.58209; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.58209 to 0.56932; runtime 0:00:01; BEST YET
Epoch 007: val_loss did not improve from 0.56932; runtime 0:00:01
Epoch 008: val_loss improved from 0.56932 to 0.53089; runtime 0:00:01; BEST YET
Epoch 009: val_loss improved from 0.53089 to 0.51777; runtime 0:00:01; BEST YET
Epoch 010: val_loss did not improve from 0.51777; runtime 0:00:01
Epoch 011: val_loss improved from 0.51777 to 0.49593; runtime 0:00:01; BEST YET
Epoch 012: val_loss did not improve from 0.49593; runtime 0:00:01
Epoch 013: val_loss improved from 0.49593 to 0.48515; runtime 0:00:01; BEST YET
Epoch 014: val_loss improved from 0.48515 to 0.47541; runtime 0:00:01; BEST YET
Epoch 015: val_loss did not improve from 0.47541; runtime 0:00:01
Epoch 016: val_loss improved from 0.47541 to 0.46462; runtime 0:00:01; BEST YET
Epoch 017: val_loss did not improve from 0.46462; runtime 0:00:01
Epoch 018: val_loss did not improve from 0.46462; runtime 0:00:01
Epoch 019: val_loss improved from 0.46462 to 0.46178; runtime 0:00:01; BEST YET
Epoch 020: val_loss improved from 0.46178 to 0.46114; runtime 0:00:01; BEST YET
Epoch 021: val_loss did not improve from 0.46114; runtime 0:00:01
Epoch 022: val_loss did not improve from 0.46114; runtime 0:00:01
Epoch 023: val_loss improved from 0.46114 to 0.45721; runtime 0:00:01; BEST YET
Epoch 024: val_loss did not improve from 0.45721; runtime 0:00:01
Epoch 025: val_loss improved from 0.45721 to 0.44429; runtime 0:00:01; BEST YET
Epoch 026: val_loss improved from 0.44429 to 0.44162; runtime 0:00:01; BEST YET
Epoch 027: val_loss did not improve from 0.44162; runtime 0:00:01
Epoch 028: val_loss improved from 0.44162 to 0.42732; runtime 0:00:01; BEST YET
Epoch 029: val_loss did not improve from 0.42732; runtime 0:00:01
Epoch 030: val_loss did not improve from 0.42732; runtime 0:00:01
Epoch 031: val_loss did not improve from 0.42732; runtime 0:00:01
Fold 8 training runtime: 0:00:36

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.87      0.83       790
        HPL       0.90      0.75      0.82       563
        MWS       0.81      0.84      0.82       604

avg / total       0.83      0.82      0.82      1957

            ----- Confusion Matrix -----
True Labels  EAP  [684  31  75]
             HPL  [ 96 420  47]
             MWS  [ 84  14 506]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.74951; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.74951 to 0.69150; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.69150 to 0.68218; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.68218 to 0.64119; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.64119 to 0.63373; runtime 0:00:01; BEST YET
Epoch 006: val_loss did not improve from 0.63373; runtime 0:00:01
Epoch 007: val_loss improved from 0.63373 to 0.56610; runtime 0:00:01; BEST YET
Epoch 008: val_loss did not improve from 0.56610; runtime 0:00:01
Epoch 009: val_loss improved from 0.56610 to 0.54741; runtime 0:00:01; BEST YET
Epoch 010: val_loss did not improve from 0.54741; runtime 0:00:01
Epoch 011: val_loss improved from 0.54741 to 0.50805; runtime 0:00:01; BEST YET
Epoch 012: val_loss did not improve from 0.50805; runtime 0:00:01
Epoch 013: val_loss did not improve from 0.50805; runtime 0:00:01
Epoch 014: val_loss did not improve from 0.50805; runtime 0:00:01
Fold 9 training runtime: 0:00:16

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.86      0.79       790
        HPL       0.94      0.61      0.74       563
        MWS       0.76      0.83      0.79       604

avg / total       0.80      0.78      0.78      1957

            ----- Confusion Matrix -----
True Labels  EAP  [682  14  94]
             HPL  [158 342  63]
             MWS  [ 96   8 500]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 0.73384; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.73384 to 0.68619; runtime 0:00:01; BEST YET
Epoch 003: val_loss improved from 0.68619 to 0.64078; runtime 0:00:01; BEST YET
Epoch 004: val_loss improved from 0.64078 to 0.58233; runtime 0:00:01; BEST YET
Epoch 005: val_loss improved from 0.58233 to 0.57057; runtime 0:00:01; BEST YET
Epoch 006: val_loss improved from 0.57057 to 0.54420; runtime 0:00:01; BEST YET
Epoch 007: val_loss did not improve from 0.54420; runtime 0:00:01
Epoch 008: val_loss improved from 0.54420 to 0.53622; runtime 0:00:01; BEST YET
Epoch 009: val_loss did not improve from 0.53622; runtime 0:00:01
Epoch 010: val_loss improved from 0.53622 to 0.52134; runtime 0:00:01; BEST YET
Epoch 011: val_loss did not improve from 0.52134; runtime 0:00:01
Epoch 012: val_loss improved from 0.52134 to 0.48359; runtime 0:00:01; BEST YET
Epoch 013: val_loss did not improve from 0.48359; runtime 0:00:01
Epoch 014: val_loss did not improve from 0.48359; runtime 0:00:01
Epoch 015: val_loss did not improve from 0.48359; runtime 0:00:01
Fold 10 training runtime: 0:00:18

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.73      0.91      0.81       790
        HPL       0.91      0.69      0.78       563
        MWS       0.83      0.75      0.78       604

avg / total       0.81      0.79      0.79      1957

            ----- Confusion Matrix -----
True Labels  EAP  [716  22  52]
             HPL  [135 386  42]
             MWS  [136  18 450]
                    EAP  HPL  MWS
                  Predicted Labels

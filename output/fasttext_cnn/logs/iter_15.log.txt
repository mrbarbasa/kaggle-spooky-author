_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_layer (InputLayer)     (None, 128)               0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 128, 300)          8329800   
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 128, 256)          537856    
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 43, 256)           0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 43, 256)           0         
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 43, 256)           459008    
_________________________________________________________________
max_pooling1d_2 (MaxPooling1 (None, 15, 256)           0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 15, 256)           0         
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 15, 256)           459008    
_________________________________________________________________
max_pooling1d_3 (MaxPooling1 (None, 5, 256)            0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 5, 256)            0         
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 5, 256)            459008    
_________________________________________________________________
global_average_pooling1d_1 ( (None, 256)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 256)               0         
_________________________________________________________________
output_layer (Dense)         (None, 3)                 771       
=================================================================
Total params: 10,245,451
Trainable params: 1,915,651
Non-trainable params: 8,329,800
_________________________________________________________________


----- Fold 1 of 10 -----
Epoch 001: val_loss improved from inf to 0.78890; runtime 0:00:03; BEST YET
Epoch 002: val_loss improved from 0.78890 to 0.72757; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.72757 to 0.57259; runtime 0:00:02; BEST YET
Epoch 004: val_loss improved from 0.57259 to 0.52356; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.52356 to 0.50836; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.50836; runtime 0:00:02
Epoch 007: val_loss did not improve from 0.50836; runtime 0:00:02
Epoch 008: val_loss did not improve from 0.50836; runtime 0:00:01
Fold 1 training runtime: 0:00:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.79      0.87      0.83       790
        HPL       0.88      0.76      0.82       564
        MWS       0.83      0.83      0.83       605

avg / total       0.83      0.83      0.83      1959

            ----- Confusion Matrix -----
True Labels  EAP  [688  35  67]
             HPL  [ 98 428  38]
             MWS  [ 81  23 501]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 2 of 10 -----
Epoch 001: val_loss improved from inf to 0.84678; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.84678 to 0.65470; runtime 0:00:02; BEST YET
Epoch 003: val_loss did not improve from 0.65470; runtime 0:00:02
Epoch 004: val_loss improved from 0.65470 to 0.45433; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.45433; runtime 0:00:02
Epoch 006: val_loss did not improve from 0.45433; runtime 0:00:02
Epoch 007: val_loss did not improve from 0.45433; runtime 0:00:02
Fold 2 training runtime: 0:00:11

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.63      0.96      0.76       790
        HPL       0.92      0.63      0.75       564
        MWS       0.90      0.57      0.70       605

avg / total       0.80      0.74      0.74      1959

            ----- Confusion Matrix -----
True Labels  EAP  [755  19  16]
             HPL  [187 355  22]
             MWS  [248  10 347]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 3 of 10 -----
Epoch 001: val_loss improved from inf to 0.77487; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.77487 to 0.68647; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.68647 to 0.60011; runtime 0:00:02; BEST YET
Epoch 004: val_loss did not improve from 0.60011; runtime 0:00:02
Epoch 005: val_loss did not improve from 0.60011; runtime 0:00:02
Epoch 006: val_loss improved from 0.60011 to 0.57204; runtime 0:00:02; BEST YET
Epoch 007: val_loss did not improve from 0.57204; runtime 0:00:02
Epoch 008: val_loss did not improve from 0.57204; runtime 0:00:02
Epoch 009: val_loss did not improve from 0.57204; runtime 0:00:02
Fold 3 training runtime: 0:00:14

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.82      0.79      0.81       790
        HPL       0.82      0.81      0.81       564
        MWS       0.79      0.83      0.81       605

avg / total       0.81      0.81      0.81      1959

            ----- Confusion Matrix -----
True Labels  EAP  [627  69  94]
             HPL  [ 70 456  38]
             MWS  [ 69  31 505]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 4 of 10 -----
Epoch 001: val_loss improved from inf to 0.73086; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.73086 to 0.63895; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.63895 to 0.49820; runtime 0:00:01; BEST YET
Epoch 004: val_loss did not improve from 0.49820; runtime 0:00:02
Epoch 005: val_loss did not improve from 0.49820; runtime 0:00:01
Epoch 006: val_loss did not improve from 0.49820; runtime 0:00:02
Fold 4 training runtime: 0:00:10

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.88      0.54      0.67       790
        HPL       0.92      0.60      0.72       564
        MWS       0.53      0.98      0.69       605

avg / total       0.78      0.69      0.69      1959

            ----- Confusion Matrix -----
True Labels  EAP  [426  26 338]
             HPL  [ 48 337 179]
             MWS  [ 10   3 592]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 5 of 10 -----
Epoch 001: val_loss improved from inf to 0.63703; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.63703 to 0.56132; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.56132 to 0.48086; runtime 0:00:01; BEST YET
Epoch 004: val_loss did not improve from 0.48086; runtime 0:00:02
Epoch 005: val_loss improved from 0.48086 to 0.48077; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.48077; runtime 0:00:02
Epoch 007: val_loss did not improve from 0.48077; runtime 0:00:02
Epoch 008: val_loss did not improve from 0.48077; runtime 0:00:02
Fold 5 training runtime: 0:00:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.83      0.85      0.84       790
        HPL       0.85      0.84      0.84       564
        MWS       0.83      0.83      0.83       604

avg / total       0.84      0.84      0.84      1958

            ----- Confusion Matrix -----
True Labels  EAP  [668  52  70]
             HPL  [ 64 471  29]
             MWS  [ 73  32 499]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 6 of 10 -----
Epoch 001: val_loss improved from inf to 0.64165; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.64165 to 0.58998; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.58998 to 0.50991; runtime 0:00:02; BEST YET
Epoch 004: val_loss did not improve from 0.50991; runtime 0:00:02
Epoch 005: val_loss did not improve from 0.50991; runtime 0:00:01
Epoch 006: val_loss did not improve from 0.50991; runtime 0:00:02
Fold 6 training runtime: 0:00:10

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.62      0.94      0.75       790
        HPL       0.98      0.42      0.59       563
        MWS       0.83      0.69      0.75       604

avg / total       0.79      0.72      0.70      1957

            ----- Confusion Matrix -----
True Labels  EAP  [746   5  39]
             HPL  [275 239  49]
             MWS  [187   0 417]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 7 of 10 -----
Epoch 001: val_loss improved from inf to 0.70276; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.70276 to 0.69654; runtime 0:00:02; BEST YET
Epoch 003: val_loss did not improve from 0.69654; runtime 0:00:02
Epoch 004: val_loss improved from 0.69654 to 0.64768; runtime 0:00:02; BEST YET
Epoch 005: val_loss improved from 0.64768 to 0.52169; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.52169; runtime 0:00:02
Epoch 007: val_loss did not improve from 0.52169; runtime 0:00:02
Epoch 008: val_loss did not improve from 0.52169; runtime 0:00:02
Fold 7 training runtime: 0:00:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.82      0.81       790
        HPL       0.78      0.85      0.81       563
        MWS       0.85      0.77      0.81       604

avg / total       0.81      0.81      0.81      1957

            ----- Confusion Matrix -----
True Labels  EAP  [645  86  59]
             HPL  [ 66 476  21]
             MWS  [ 92  49 463]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 8 of 10 -----
Epoch 001: val_loss improved from inf to 0.69637; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.69637 to 0.60196; runtime 0:00:02; BEST YET
Epoch 003: val_loss did not improve from 0.60196; runtime 0:00:02
Epoch 004: val_loss improved from 0.60196 to 0.49857; runtime 0:00:02; BEST YET
Epoch 005: val_loss did not improve from 0.49857; runtime 0:00:02
Epoch 006: val_loss did not improve from 0.49857; runtime 0:00:02
Epoch 007: val_loss did not improve from 0.49857; runtime 0:00:02
Fold 8 training runtime: 0:00:11

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.91      0.64      0.75       790
        HPL       0.75      0.89      0.82       563
        MWS       0.72      0.87      0.79       604

avg / total       0.80      0.78      0.78      1957

            ----- Confusion Matrix -----
True Labels  EAP  [508 117 165]
             HPL  [ 24 502  37]
             MWS  [ 29  49 526]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 9 of 10 -----
Epoch 001: val_loss improved from inf to 0.67937; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 0.67937 to 0.65990; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.65990 to 0.64032; runtime 0:00:02; BEST YET
Epoch 004: val_loss did not improve from 0.64032; runtime 0:00:02
Epoch 005: val_loss improved from 0.64032 to 0.58505; runtime 0:00:02; BEST YET
Epoch 006: val_loss did not improve from 0.58505; runtime 0:00:02
Epoch 007: val_loss improved from 0.58505 to 0.53716; runtime 0:00:02; BEST YET
Epoch 008: val_loss did not improve from 0.53716; runtime 0:00:02
Epoch 009: val_loss did not improve from 0.53716; runtime 0:00:02
Epoch 010: val_loss did not improve from 0.53716; runtime 0:00:02
Fold 9 training runtime: 0:00:16

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.78      0.84      0.81       790
        HPL       0.88      0.73      0.80       563
        MWS       0.79      0.85      0.82       604

avg / total       0.81      0.81      0.81      1957

            ----- Confusion Matrix -----
True Labels  EAP  [660  36  94]
             HPL  [110 412  41]
             MWS  [ 73  20 511]
                    EAP  HPL  MWS
                  Predicted Labels


----- Fold 10 of 10 -----
Epoch 001: val_loss improved from inf to 1.13169; runtime 0:00:02; BEST YET
Epoch 002: val_loss improved from 1.13169 to 0.55494; runtime 0:00:02; BEST YET
Epoch 003: val_loss improved from 0.55494 to 0.53616; runtime 0:00:01; BEST YET
Epoch 004: val_loss did not improve from 0.53616; runtime 0:00:02
Epoch 005: val_loss improved from 0.53616 to 0.47430; runtime 0:00:01; BEST YET
Epoch 006: val_loss did not improve from 0.47430; runtime 0:00:01
Epoch 007: val_loss did not improve from 0.47430; runtime 0:00:02
Epoch 008: val_loss did not improve from 0.47430; runtime 0:00:02
Fold 10 training runtime: 0:00:13

            ----- Classification Report -----
             precision    recall  f1-score   support

        EAP       0.80      0.86      0.83       790
        HPL       0.86      0.80      0.83       563
        MWS       0.83      0.81      0.82       604

avg / total       0.83      0.83      0.83      1957

            ----- Confusion Matrix -----
True Labels  EAP  [680  52  58]
             HPL  [ 73 448  42]
             MWS  [ 93  22 489]
                    EAP  HPL  MWS
                  Predicted Labels

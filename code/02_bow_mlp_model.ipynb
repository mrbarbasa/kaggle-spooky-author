{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: Bag-of-Words MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # Set to empty string for CPU, '0' for the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/greenbeats/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/greenbeats/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/greenbeats/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/greenbeats/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't already have these packages, run this cell\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bow_mlp'\n",
    "\n",
    "INPUT_DIR = '../input/'\n",
    "TRAIN_FILE_PATH = f'{INPUT_DIR}train.csv'\n",
    "TEST_FILE_PATH = f'{INPUT_DIR}test.csv'\n",
    "SAMPLE_SUBMISSION_FILE_PATH = f'{INPUT_DIR}sample_submission.csv'\n",
    "\n",
    "OUTPUT_DIR = '../output/'\n",
    "OUTPUT_LOGS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/logs/'\n",
    "OUTPUT_MODELS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/models/'\n",
    "OUTPUT_SCORES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/scores/'\n",
    "OUTPUT_SUBMISSIONS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/submissions/'\n",
    "OUTPUT_SUMMARIES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/summaries/'\n",
    "\n",
    "# Create the output directories if they do not exist (the `_` is necessary\n",
    "# in order to create intermediate directories and is itself not created)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_LOGS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_MODELS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SCORES_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUBMISSIONS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUMMARIES_DIR}_'), exist_ok=True)\n",
    "\n",
    "MAX_FEATURES = 20000 # The max number of features (unigrams and bigrams)\n",
    "TOKEN_MODE = 'word' # Word-level tokenization; can also be set to 'char'\n",
    "NGRAM_RANGE = (1, 2) # Range of n-gram sizes (unigrams and bigrams)\n",
    "# A token will be discarded if it appears less than 2 times across documents in the corpus\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "N_SPLITS = 10\n",
    "\n",
    "# Fix a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, submission = load_data(TRAIN_FILE_PATH,\n",
    "                                    TEST_FILE_PATH,\n",
    "                                    SAMPLE_SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's \"Algorithm for Data Preparation and Model Building\"\n",
    "\n",
    "Let's follow Google's algorithm below to test our hypothesis that a bag-of-words multilayer perceptron model will perform better than our CNN and RNN models using pre-trained GloVe and fastText word embeddings.\n",
    "\n",
    "Source: https://developers.google.com/machine-learning/guides/text-classification/step-2-5\n",
    "\n",
    "```\n",
    "1. Calculate the number of samples/number of words per sample ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
    "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
    "flowchart below):\n",
    "  a. Split the samples into word n-grams; convert the n-grams into vectors.\n",
    "  b. Score the importance of the vectors and then select the top 20K using the scores.\n",
    "  c. Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
    "   sepCNN model to classify them (right branch in the flowchart below):\n",
    "  a. Split the samples into words; select the top 20K words based on their frequency.\n",
    "  b. Convert the samples into word sequence vectors.\n",
    "  c. If the original number of samples/number of words per sample ratio is less\n",
    "     than 15K, using a fine-tuned pre-trained embedding with the sepCNN\n",
    "     model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find\n",
    "   the best model configuration for the dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of samples/number of words per sample ratio\n",
    "From the same source URL above, Google has stated the following:\n",
    "\n",
    "> **From our experiments, we have observed that the ratio of “number of samples” (S) to “number of words per sample” (W) correlates with which model performs well.**\n",
    "\n",
    "> When the value for this ratio is small (<1500), small multi-layer perceptrons that take n-grams as input (which we'll call **Option A**) perform better or at least as well as sequence models. MLPs are simple to define and understand, and they take much less compute time than sequence models. When the value for this ratio is large (>= 1500), use a sequence model (**Option B**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated that this number is about `732.46` from the EDA notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text as word n-grams and convert them into vectors\n",
    "Since the calculated ratio of `732.46` is less than the `1500` threshold, we'll proceed with **Option A** (the n-gram inputs with an MLP model). First, we need to tokenize the text as n-grams and select only the top `MAX_FEATURES` most important features from the vector of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels: ['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "# The target classes need to be converted to integers so that\n",
    "# EAP --> 0\n",
    "# HPL --> 1\n",
    "# MWS --> 2\n",
    "y_train_integers = integer_encode_classes(train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target classes need to be one-hot encoded so that\n",
    "# EAP --> 0 --> [1, 0, 0]\n",
    "# HPL --> 1 --> [0, 1, 0]\n",
    "# MWS --> 2 --> [0, 0, 1]\n",
    "y_train_encoded = one_hot_encode_classes(y_train_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76279 unique unigrams and bigrams.\n",
      "But only kept the 20000 most important ones.\n"
     ]
    }
   ],
   "source": [
    "# Apply text preprocessing on each sentence\n",
    "X_train_sequences = list(train['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True, # Todo: Also try False\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "X_test_sequences = list(test['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True, # Todo: Also try False\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "\n",
    "# Vectorize the sentences and TF-IDF features\n",
    "X_train_tokenized, X_test_tokenized = vectorize_ngrams(X_train_sequences,\n",
    "                                                       X_test_sequences,\n",
    "                                                       y_train_integers,\n",
    "                                                       MAX_FEATURES,\n",
    "                                                       TOKEN_MODE,\n",
    "                                                       NGRAM_RANGE,\n",
    "                                                       MIN_DOCUMENT_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the input shape\n",
    "input_shape = X_train_tokenized.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 119 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Each model will be evaluated based on the logloss metric using either 5-fold or 10-fold cross validation; the lower the logloss, the better the model. This is the step in which we measure the model performance with different hyperparameter values to find the best model configuration for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model-dependent files\n",
    "from models import build_model_callbacks, save_model_summary\n",
    "from models import get_random_mlp_params as get_random_model_params\n",
    "from models import build_mlp_model as build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Iteration 1 of 1 -----\n",
      "Writing model params to file...\n",
      "\n",
      "----- Fold 1 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/1000\n",
      "17620/17620 [==============================] - 12s 665us/step - loss: 0.9187 - acc: 0.6241 - val_loss: 0.6860 - val_acc: 0.8188\n",
      "Epoch 2/1000\n",
      "17620/17620 [==============================] - 12s 696us/step - loss: 0.5061 - acc: 0.8631 - val_loss: 0.4367 - val_acc: 0.8581\n",
      "Epoch 3/1000\n",
      "17620/17620 [==============================] - 12s 705us/step - loss: 0.3234 - acc: 0.9064 - val_loss: 0.3523 - val_acc: 0.8765\n",
      "Epoch 4/1000\n",
      "17620/17620 [==============================] - 9s 538us/step - loss: 0.2478 - acc: 0.9261 - val_loss: 0.3115 - val_acc: 0.8836\n",
      "Epoch 5/1000\n",
      "17620/17620 [==============================] - 11s 615us/step - loss: 0.1952 - acc: 0.9430 - val_loss: 0.2902 - val_acc: 0.8887\n",
      "Epoch 6/1000\n",
      "17620/17620 [==============================] - 10s 577us/step - loss: 0.1664 - acc: 0.9483 - val_loss: 0.2779 - val_acc: 0.8954\n",
      "Epoch 7/1000\n",
      "17620/17620 [==============================] - 11s 637us/step - loss: 0.1380 - acc: 0.9603 - val_loss: 0.2698 - val_acc: 0.8918\n",
      "Epoch 8/1000\n",
      "17620/17620 [==============================] - 12s 658us/step - loss: 0.1230 - acc: 0.9628 - val_loss: 0.2675 - val_acc: 0.8938\n",
      "Epoch 9/1000\n",
      "17620/17620 [==============================] - 12s 683us/step - loss: 0.1106 - acc: 0.9673 - val_loss: 0.2697 - val_acc: 0.8928\n",
      "Epoch 10/1000\n",
      "17620/17620 [==============================] - 12s 708us/step - loss: 0.0954 - acc: 0.9715 - val_loss: 0.2712 - val_acc: 0.8928\n",
      "Epoch 11/1000\n",
      "17620/17620 [==============================] - 11s 611us/step - loss: 0.0892 - acc: 0.9730 - val_loss: 0.2736 - val_acc: 0.8913\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 2 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/1000\n",
      "17620/17620 [==============================] - 12s 706us/step - loss: 0.9062 - acc: 0.6428 - val_loss: 0.6609 - val_acc: 0.8438\n",
      "Epoch 2/1000\n",
      "17620/17620 [==============================] - 11s 611us/step - loss: 0.4973 - acc: 0.8621 - val_loss: 0.4040 - val_acc: 0.8882\n",
      "Epoch 3/1000\n",
      "17620/17620 [==============================] - 11s 607us/step - loss: 0.3245 - acc: 0.9028 - val_loss: 0.3161 - val_acc: 0.9005\n",
      "Epoch 4/1000\n",
      "17620/17620 [==============================] - 12s 709us/step - loss: 0.2456 - acc: 0.9233 - val_loss: 0.2760 - val_acc: 0.9066\n",
      "Epoch 5/1000\n",
      "17620/17620 [==============================] - 11s 647us/step - loss: 0.1958 - acc: 0.9419 - val_loss: 0.2530 - val_acc: 0.9132\n",
      "Epoch 6/1000\n",
      "17620/17620 [==============================] - 11s 641us/step - loss: 0.1636 - acc: 0.9497 - val_loss: 0.2418 - val_acc: 0.9086\n",
      "Epoch 7/1000\n",
      "17620/17620 [==============================] - 12s 707us/step - loss: 0.1406 - acc: 0.9581 - val_loss: 0.2353 - val_acc: 0.9117\n",
      "Epoch 8/1000\n",
      "17620/17620 [==============================] - 12s 655us/step - loss: 0.1202 - acc: 0.9656 - val_loss: 0.2280 - val_acc: 0.9148\n",
      "Epoch 9/1000\n",
      "17620/17620 [==============================] - 11s 651us/step - loss: 0.1094 - acc: 0.9678 - val_loss: 0.2286 - val_acc: 0.9096\n",
      "Epoch 10/1000\n",
      "17620/17620 [==============================] - 12s 669us/step - loss: 0.0983 - acc: 0.9698 - val_loss: 0.2314 - val_acc: 0.9086\n",
      "Epoch 11/1000\n",
      "17620/17620 [==============================] - 9s 513us/step - loss: 0.0898 - acc: 0.9742 - val_loss: 0.2295 - val_acc: 0.9112\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 3 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/1000\n",
      "17620/17620 [==============================] - 10s 570us/step - loss: 0.9094 - acc: 0.6663 - val_loss: 0.6713 - val_acc: 0.8244\n",
      "Epoch 2/1000\n",
      "17620/17620 [==============================] - 9s 519us/step - loss: 0.4967 - acc: 0.8699 - val_loss: 0.4251 - val_acc: 0.8714\n",
      "Epoch 3/1000\n",
      "17620/17620 [==============================] - 10s 567us/step - loss: 0.3225 - acc: 0.9061 - val_loss: 0.3413 - val_acc: 0.8872\n",
      "Epoch 4/1000\n",
      "17620/17620 [==============================] - 8s 472us/step - loss: 0.2424 - acc: 0.9285 - val_loss: 0.2977 - val_acc: 0.8959\n",
      "Epoch 5/1000\n",
      "17620/17620 [==============================] - 9s 484us/step - loss: 0.1948 - acc: 0.9411 - val_loss: 0.2780 - val_acc: 0.8918\n",
      "Epoch 6/1000\n",
      "17620/17620 [==============================] - 9s 530us/step - loss: 0.1643 - acc: 0.9520 - val_loss: 0.2638 - val_acc: 0.8928\n",
      "Epoch 7/1000\n",
      "17620/17620 [==============================] - 9s 501us/step - loss: 0.1417 - acc: 0.9563 - val_loss: 0.2572 - val_acc: 0.8974\n",
      "Epoch 8/1000\n",
      "17620/17620 [==============================] - 9s 529us/step - loss: 0.1188 - acc: 0.9662 - val_loss: 0.2544 - val_acc: 0.9015\n",
      "Epoch 9/1000\n",
      "17620/17620 [==============================] - 9s 537us/step - loss: 0.1091 - acc: 0.9678 - val_loss: 0.2545 - val_acc: 0.8984\n",
      "Epoch 10/1000\n",
      "17620/17620 [==============================] - 9s 524us/step - loss: 0.0994 - acc: 0.9696 - val_loss: 0.2546 - val_acc: 0.8984\n",
      "Epoch 11/1000\n",
      "17620/17620 [==============================] - 10s 581us/step - loss: 0.0921 - acc: 0.9714 - val_loss: 0.2529 - val_acc: 0.9010\n",
      "Epoch 12/1000\n",
      "17620/17620 [==============================] - 10s 568us/step - loss: 0.0848 - acc: 0.9729 - val_loss: 0.2567 - val_acc: 0.8984\n",
      "Epoch 13/1000\n",
      "17620/17620 [==============================] - 11s 612us/step - loss: 0.0785 - acc: 0.9750 - val_loss: 0.2565 - val_acc: 0.8984\n",
      "Epoch 14/1000\n",
      "17620/17620 [==============================] - 9s 516us/step - loss: 0.0752 - acc: 0.9767 - val_loss: 0.2555 - val_acc: 0.9020\n",
      "Epoch 00014: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 4 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/1000\n",
      "17620/17620 [==============================] - 9s 514us/step - loss: 0.9077 - acc: 0.6257 - val_loss: 0.6695 - val_acc: 0.8474\n",
      "Epoch 2/1000\n",
      "17620/17620 [==============================] - 9s 525us/step - loss: 0.4970 - acc: 0.8632 - val_loss: 0.4164 - val_acc: 0.8862\n",
      "Epoch 3/1000\n",
      "17620/17620 [==============================] - 10s 581us/step - loss: 0.3257 - acc: 0.9023 - val_loss: 0.3306 - val_acc: 0.8969\n",
      "Epoch 4/1000\n",
      "17620/17620 [==============================] - 10s 556us/step - loss: 0.2464 - acc: 0.9263 - val_loss: 0.2886 - val_acc: 0.9030\n",
      "Epoch 5/1000\n",
      "17620/17620 [==============================] - 9s 508us/step - loss: 0.1977 - acc: 0.9403 - val_loss: 0.2624 - val_acc: 0.9091\n",
      "Epoch 6/1000\n",
      "17620/17620 [==============================] - 10s 546us/step - loss: 0.1657 - acc: 0.9502 - val_loss: 0.2503 - val_acc: 0.9096\n",
      "Epoch 7/1000\n",
      "17620/17620 [==============================] - 8s 469us/step - loss: 0.1391 - acc: 0.9586 - val_loss: 0.2454 - val_acc: 0.9107\n",
      "Epoch 8/1000\n",
      "17620/17620 [==============================] - 8s 462us/step - loss: 0.1258 - acc: 0.9612 - val_loss: 0.2361 - val_acc: 0.9107\n",
      "Epoch 9/1000\n",
      "17620/17620 [==============================] - 8s 463us/step - loss: 0.1108 - acc: 0.9663 - val_loss: 0.2311 - val_acc: 0.9142\n",
      "Epoch 10/1000\n",
      "17620/17620 [==============================] - 8s 453us/step - loss: 0.0994 - acc: 0.9704 - val_loss: 0.2371 - val_acc: 0.9137\n",
      "Epoch 11/1000\n",
      "17620/17620 [==============================] - 9s 519us/step - loss: 0.0934 - acc: 0.9713 - val_loss: 0.2364 - val_acc: 0.9102\n",
      "Epoch 12/1000\n",
      "17620/17620 [==============================] - 9s 521us/step - loss: 0.0825 - acc: 0.9761 - val_loss: 0.2354 - val_acc: 0.9102\n",
      "Epoch 00012: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 5 of 10 -----\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/1000\n",
      "17621/17621 [==============================] - 11s 607us/step - loss: 0.9118 - acc: 0.6308 - val_loss: 0.6515 - val_acc: 0.8488\n",
      "Epoch 2/1000\n",
      "17621/17621 [==============================] - 9s 488us/step - loss: 0.4974 - acc: 0.8633 - val_loss: 0.3905 - val_acc: 0.8902\n",
      "Epoch 3/1000\n",
      "17621/17621 [==============================] - 8s 478us/step - loss: 0.3250 - acc: 0.9055 - val_loss: 0.3046 - val_acc: 0.9045\n",
      "Epoch 4/1000\n",
      "17621/17621 [==============================] - 9s 493us/step - loss: 0.2443 - acc: 0.9245 - val_loss: 0.2674 - val_acc: 0.9101\n",
      "Epoch 5/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - 8s 478us/step - loss: 0.1946 - acc: 0.9427 - val_loss: 0.2478 - val_acc: 0.9142\n",
      "Epoch 6/1000\n",
      "17621/17621 [==============================] - 8s 475us/step - loss: 0.1647 - acc: 0.9499 - val_loss: 0.2361 - val_acc: 0.9142\n",
      "Epoch 7/1000\n",
      "17621/17621 [==============================] - 8s 472us/step - loss: 0.1446 - acc: 0.9559 - val_loss: 0.2301 - val_acc: 0.9142\n",
      "Epoch 8/1000\n",
      "17621/17621 [==============================] - 9s 483us/step - loss: 0.1218 - acc: 0.9652 - val_loss: 0.2268 - val_acc: 0.9147\n",
      "Epoch 9/1000\n",
      "17621/17621 [==============================] - 8s 465us/step - loss: 0.1093 - acc: 0.9671 - val_loss: 0.2270 - val_acc: 0.9173\n",
      "Epoch 10/1000\n",
      "17621/17621 [==============================] - 8s 466us/step - loss: 0.0990 - acc: 0.9694 - val_loss: 0.2285 - val_acc: 0.9162\n",
      "Epoch 11/1000\n",
      "17621/17621 [==============================] - 8s 462us/step - loss: 0.0892 - acc: 0.9734 - val_loss: 0.2309 - val_acc: 0.9152\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 6 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/1000\n",
      "17622/17622 [==============================] - 8s 476us/step - loss: 0.9218 - acc: 0.6747 - val_loss: 0.6739 - val_acc: 0.8319\n",
      "Epoch 2/1000\n",
      "17622/17622 [==============================] - 8s 462us/step - loss: 0.5064 - acc: 0.8631 - val_loss: 0.4203 - val_acc: 0.8728\n",
      "Epoch 3/1000\n",
      "17622/17622 [==============================] - 9s 515us/step - loss: 0.3284 - acc: 0.9035 - val_loss: 0.3385 - val_acc: 0.8850\n",
      "Epoch 4/1000\n",
      "17622/17622 [==============================] - 12s 697us/step - loss: 0.2475 - acc: 0.9262 - val_loss: 0.3031 - val_acc: 0.8927\n",
      "Epoch 5/1000\n",
      "17622/17622 [==============================] - 12s 659us/step - loss: 0.1965 - acc: 0.9423 - val_loss: 0.2836 - val_acc: 0.8993\n",
      "Epoch 6/1000\n",
      "17622/17622 [==============================] - 10s 563us/step - loss: 0.1645 - acc: 0.9495 - val_loss: 0.2747 - val_acc: 0.9009\n",
      "Epoch 7/1000\n",
      "17622/17622 [==============================] - 11s 618us/step - loss: 0.1431 - acc: 0.9556 - val_loss: 0.2711 - val_acc: 0.9014\n",
      "Epoch 8/1000\n",
      "17622/17622 [==============================] - 9s 522us/step - loss: 0.1236 - acc: 0.9653 - val_loss: 0.2671 - val_acc: 0.9039\n",
      "Epoch 9/1000\n",
      "17622/17622 [==============================] - 10s 561us/step - loss: 0.1116 - acc: 0.9667 - val_loss: 0.2720 - val_acc: 0.9004\n",
      "Epoch 10/1000\n",
      "17622/17622 [==============================] - 10s 548us/step - loss: 0.0999 - acc: 0.9696 - val_loss: 0.2692 - val_acc: 0.9085\n",
      "Epoch 11/1000\n",
      "17622/17622 [==============================] - 8s 472us/step - loss: 0.0900 - acc: 0.9726 - val_loss: 0.2748 - val_acc: 0.9050\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 7 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/1000\n",
      "17622/17622 [==============================] - 9s 527us/step - loss: 0.9125 - acc: 0.6382 - val_loss: 0.6735 - val_acc: 0.8309\n",
      "Epoch 2/1000\n",
      "17622/17622 [==============================] - 8s 479us/step - loss: 0.4965 - acc: 0.8670 - val_loss: 0.4242 - val_acc: 0.8677\n",
      "Epoch 3/1000\n",
      "17622/17622 [==============================] - 8s 461us/step - loss: 0.3203 - acc: 0.9054 - val_loss: 0.3374 - val_acc: 0.8835\n",
      "Epoch 4/1000\n",
      "17622/17622 [==============================] - 9s 490us/step - loss: 0.2411 - acc: 0.9287 - val_loss: 0.2965 - val_acc: 0.8942\n",
      "Epoch 5/1000\n",
      "17622/17622 [==============================] - 8s 472us/step - loss: 0.1963 - acc: 0.9410 - val_loss: 0.2754 - val_acc: 0.8968\n",
      "Epoch 6/1000\n",
      "17622/17622 [==============================] - 9s 488us/step - loss: 0.1620 - acc: 0.9518 - val_loss: 0.2616 - val_acc: 0.8993\n",
      "Epoch 7/1000\n",
      "17622/17622 [==============================] - 9s 499us/step - loss: 0.1392 - acc: 0.9589 - val_loss: 0.2532 - val_acc: 0.9004\n",
      "Epoch 8/1000\n",
      "17622/17622 [==============================] - 8s 479us/step - loss: 0.1203 - acc: 0.9637 - val_loss: 0.2532 - val_acc: 0.8998\n",
      "Epoch 9/1000\n",
      "17622/17622 [==============================] - 8s 465us/step - loss: 0.1080 - acc: 0.9671 - val_loss: 0.2495 - val_acc: 0.9029\n",
      "Epoch 10/1000\n",
      "17622/17622 [==============================] - 8s 458us/step - loss: 0.1011 - acc: 0.9696 - val_loss: 0.2525 - val_acc: 0.8998\n",
      "Epoch 11/1000\n",
      "17622/17622 [==============================] - 8s 466us/step - loss: 0.0893 - acc: 0.9723 - val_loss: 0.2560 - val_acc: 0.8958\n",
      "Epoch 12/1000\n",
      "17622/17622 [==============================] - 10s 552us/step - loss: 0.0823 - acc: 0.9753 - val_loss: 0.2635 - val_acc: 0.8947\n",
      "Epoch 00012: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 8 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/1000\n",
      "17622/17622 [==============================] - 10s 590us/step - loss: 0.9139 - acc: 0.6418 - val_loss: 0.6696 - val_acc: 0.8365\n",
      "Epoch 2/1000\n",
      "17622/17622 [==============================] - 8s 479us/step - loss: 0.5058 - acc: 0.8658 - val_loss: 0.4175 - val_acc: 0.8779\n",
      "Epoch 3/1000\n",
      "17622/17622 [==============================] - 9s 521us/step - loss: 0.3321 - acc: 0.9023 - val_loss: 0.3285 - val_acc: 0.8958\n",
      "Epoch 4/1000\n",
      "17622/17622 [==============================] - 12s 691us/step - loss: 0.2487 - acc: 0.9244 - val_loss: 0.2875 - val_acc: 0.9034\n",
      "Epoch 5/1000\n",
      "17622/17622 [==============================] - 11s 619us/step - loss: 0.2016 - acc: 0.9392 - val_loss: 0.2645 - val_acc: 0.9065\n",
      "Epoch 6/1000\n",
      "17622/17622 [==============================] - 11s 613us/step - loss: 0.1714 - acc: 0.9472 - val_loss: 0.2516 - val_acc: 0.9070\n",
      "Epoch 7/1000\n",
      "17622/17622 [==============================] - 11s 602us/step - loss: 0.1432 - acc: 0.9569 - val_loss: 0.2437 - val_acc: 0.9070\n",
      "Epoch 8/1000\n",
      "17622/17622 [==============================] - 11s 597us/step - loss: 0.1265 - acc: 0.9613 - val_loss: 0.2381 - val_acc: 0.9131\n",
      "Epoch 9/1000\n",
      "17622/17622 [==============================] - 11s 601us/step - loss: 0.1102 - acc: 0.9679 - val_loss: 0.2383 - val_acc: 0.9136\n",
      "Epoch 10/1000\n",
      "17622/17622 [==============================] - 10s 594us/step - loss: 0.1016 - acc: 0.9684 - val_loss: 0.2379 - val_acc: 0.9096\n",
      "Epoch 11/1000\n",
      "17622/17622 [==============================] - 11s 604us/step - loss: 0.0930 - acc: 0.9729 - val_loss: 0.2385 - val_acc: 0.9106\n",
      "Epoch 12/1000\n",
      "17622/17622 [==============================] - 11s 604us/step - loss: 0.0864 - acc: 0.9742 - val_loss: 0.2397 - val_acc: 0.9070\n",
      "Epoch 13/1000\n",
      "17622/17622 [==============================] - 11s 597us/step - loss: 0.0811 - acc: 0.9753 - val_loss: 0.2431 - val_acc: 0.9075\n",
      "Epoch 00013: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 9 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/1000\n",
      "17622/17622 [==============================] - 12s 705us/step - loss: 0.9160 - acc: 0.6334 - val_loss: 0.6736 - val_acc: 0.8324\n",
      "Epoch 2/1000\n",
      "17622/17622 [==============================] - 10s 545us/step - loss: 0.5011 - acc: 0.8628 - val_loss: 0.4183 - val_acc: 0.8779\n",
      "Epoch 3/1000\n",
      "17622/17622 [==============================] - 9s 492us/step - loss: 0.3263 - acc: 0.9044 - val_loss: 0.3325 - val_acc: 0.8917\n",
      "Epoch 4/1000\n",
      "17622/17622 [==============================] - 9s 507us/step - loss: 0.2459 - acc: 0.9257 - val_loss: 0.2908 - val_acc: 0.9060\n",
      "Epoch 5/1000\n",
      "17622/17622 [==============================] - 9s 530us/step - loss: 0.1971 - acc: 0.9399 - val_loss: 0.2680 - val_acc: 0.9096\n",
      "Epoch 6/1000\n",
      "17622/17622 [==============================] - 9s 507us/step - loss: 0.1671 - acc: 0.9498 - val_loss: 0.2529 - val_acc: 0.9101\n",
      "Epoch 7/1000\n",
      "17622/17622 [==============================] - 8s 461us/step - loss: 0.1437 - acc: 0.9559 - val_loss: 0.2433 - val_acc: 0.9096\n",
      "Epoch 8/1000\n",
      "17622/17622 [==============================] - 9s 493us/step - loss: 0.1242 - acc: 0.9631 - val_loss: 0.2427 - val_acc: 0.9096\n",
      "Epoch 9/1000\n",
      "17622/17622 [==============================] - 8s 455us/step - loss: 0.1114 - acc: 0.9666 - val_loss: 0.2392 - val_acc: 0.9116\n",
      "Epoch 10/1000\n",
      "17622/17622 [==============================] - 8s 451us/step - loss: 0.1008 - acc: 0.9695 - val_loss: 0.2422 - val_acc: 0.9106\n",
      "Epoch 11/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17622/17622 [==============================] - 8s 458us/step - loss: 0.0907 - acc: 0.9729 - val_loss: 0.2397 - val_acc: 0.9055\n",
      "Epoch 12/1000\n",
      "17622/17622 [==============================] - 8s 456us/step - loss: 0.0832 - acc: 0.9751 - val_loss: 0.2403 - val_acc: 0.9070\n",
      "Epoch 00012: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 10 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/1000\n",
      "17622/17622 [==============================] - 9s 495us/step - loss: 0.9076 - acc: 0.6185 - val_loss: 0.6550 - val_acc: 0.8416\n",
      "Epoch 2/1000\n",
      "17622/17622 [==============================] - 9s 503us/step - loss: 0.5026 - acc: 0.8649 - val_loss: 0.3991 - val_acc: 0.8815\n",
      "Epoch 3/1000\n",
      "17622/17622 [==============================] - 8s 477us/step - loss: 0.3266 - acc: 0.9039 - val_loss: 0.3130 - val_acc: 0.8942\n",
      "Epoch 4/1000\n",
      "17622/17622 [==============================] - 9s 486us/step - loss: 0.2450 - acc: 0.9269 - val_loss: 0.2743 - val_acc: 0.8998\n",
      "Epoch 5/1000\n",
      "17622/17622 [==============================] - 9s 485us/step - loss: 0.1955 - acc: 0.9415 - val_loss: 0.2555 - val_acc: 0.9014\n",
      "Epoch 6/1000\n",
      "17622/17622 [==============================] - 9s 491us/step - loss: 0.1646 - acc: 0.9509 - val_loss: 0.2393 - val_acc: 0.9055\n",
      "Epoch 7/1000\n",
      "17622/17622 [==============================] - 8s 482us/step - loss: 0.1412 - acc: 0.9575 - val_loss: 0.2332 - val_acc: 0.9060\n",
      "Epoch 8/1000\n",
      "17622/17622 [==============================] - 9s 487us/step - loss: 0.1214 - acc: 0.9641 - val_loss: 0.2283 - val_acc: 0.9060\n",
      "Epoch 9/1000\n",
      "17622/17622 [==============================] - 9s 487us/step - loss: 0.1102 - acc: 0.9677 - val_loss: 0.2283 - val_acc: 0.9024\n",
      "Epoch 10/1000\n",
      "17622/17622 [==============================] - 9s 486us/step - loss: 0.0978 - acc: 0.9707 - val_loss: 0.2265 - val_acc: 0.9085\n",
      "Epoch 11/1000\n",
      "17622/17622 [==============================] - 9s 488us/step - loss: 0.0891 - acc: 0.9727 - val_loss: 0.2306 - val_acc: 0.9085\n",
      "Epoch 12/1000\n",
      "17622/17622 [==============================] - 9s 487us/step - loss: 0.0832 - acc: 0.9747 - val_loss: 0.2366 - val_acc: 0.9060\n",
      "Epoch 13/1000\n",
      "17622/17622 [==============================] - 9s 493us/step - loss: 0.0787 - acc: 0.9753 - val_loss: 0.2405 - val_acc: 0.9065\n",
      "Epoch 00013: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "Writing CV results and runtime summary to file...\n"
     ]
    }
   ],
   "source": [
    "# Random search\n",
    "\n",
    "training_num_epochs = 100 # With early stopping in place\n",
    "num_random_search_iter = 60 # The number of iterations\n",
    "offset_iter = 0 # How many iterations to offset the nth iteration by\n",
    "\n",
    "# Calculate the k-fold splits (same splits across all iterations)\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Randomly search for the best model using 10-fold cross validation for each iteration.\n",
    "# Note: Before re-running this cell, you might want to delete the existing files in the\n",
    "# `OUTPUT_SUMMARIES_DIR` folder, since the files within might not get overwritten.\n",
    "for nth_iter in range(1+offset_iter, num_random_search_iter+1+offset_iter):\n",
    "    \n",
    "    # Start the training timer for the current iteration\n",
    "    training_start = time()\n",
    "    \n",
    "    nth_iter_str = f'iter_{nth_iter:02d}'\n",
    "    print(f'\\n----- Iteration {nth_iter} of {num_random_search_iter+offset_iter} -----')\n",
    "\n",
    "    # Metrics to monitor and record\n",
    "    monitored_metric = 'val_loss'\n",
    "    other_metrics = ['val_acc', 'loss', 'acc']\n",
    "    all_metrics = ['val_loss', 'val_acc', 'loss', 'acc']\n",
    "    \n",
    "    # The number of training epochs that have passed at which\n",
    "    # the best validation loss is recorded\n",
    "    best_score_num_epochs = []\n",
    "    \n",
    "    # The metric scores recorded from the epoch number at which\n",
    "    # the best validation loss is recorded.\n",
    "    # Note that each list should be `N_SPLITS` in length.\n",
    "    best_scores_per_fold = { 'val_loss': [], 'val_acc': [], 'loss': [], 'acc': [] }\n",
    "    \n",
    "    # Runtime records\n",
    "    pred_runtimes = []\n",
    "    pred_runtime_strs = []\n",
    "\n",
    "    # Retrieve the random model params for the current iteration\n",
    "    random_model_params = get_random_model_params()\n",
    "    batch_size = random_model_params['batch_size']\n",
    "    \n",
    "    print('Writing model params to file...')\n",
    "    params_file_path = f'{OUTPUT_MODELS_DIR}{nth_iter_str}.params.json'\n",
    "    save_dictionary_to_file(random_model_params, params_file_path)    \n",
    "    \n",
    "    # The model summary, improvement log, and classification summary file path\n",
    "    log_file_path = f'{OUTPUT_LOGS_DIR}{nth_iter_str}.log.txt'\n",
    "    \n",
    "    # Prepare the generator\n",
    "    folds = kfold.split(X_train_tokenized, y_train_integers)\n",
    "    \n",
    "    # Begin 10-fold cross validation for the current set of random model params\n",
    "    for fold, (train_indices, valid_indices) in enumerate(folds):\n",
    "        nth_fold = fold + 1\n",
    "        nth_fold_str = f'fold_{nth_fold:02d}'\n",
    "        print(f'\\n----- Fold {nth_fold} of {N_SPLITS} -----')\n",
    "\n",
    "        # Prepare the splits of data\n",
    "        X_train, y_train = X_train_tokenized[train_indices], y_train_encoded[train_indices]\n",
    "        X_valid, y_valid = X_train_tokenized[valid_indices], y_train_encoded[valid_indices]\n",
    "\n",
    "        # Construct model callbacks, save the best models, and log metrics to file\n",
    "        logger_file_path = f'{OUTPUT_SCORES_DIR}{nth_iter_str}.{nth_fold_str}.scores.csv'\n",
    "        model_callbacks = build_model_callbacks(monitored_metric,\n",
    "                                                'min',\n",
    "                                                log_file_path, # progress\n",
    "                                                None, # model\n",
    "                                                logger_file_path, # logger\n",
    "                                                nth_fold,\n",
    "                                                N_SPLITS)\n",
    "        # Build the model\n",
    "        model = build_model(input_shape, random_model_params)\n",
    "        # Save the model summary to file on the first fold only\n",
    "        # since the models are identical across folds\n",
    "        if nth_fold == 1:\n",
    "            save_model_summary(model, log_file_path)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=training_num_epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=model_callbacks,\n",
    "                            validation_data=[X_valid, y_valid],\n",
    "                            shuffle=True)\n",
    "        \n",
    "        # Save the scores for later evaluation\n",
    "        monitored_metric_fold_scores = history.history[monitored_metric]\n",
    "        # Save only the best validation loss\n",
    "        epoch_index = np.argmin(monitored_metric_fold_scores)\n",
    "        best_score_num_epochs.append(int(epoch_index + 1))\n",
    "        best_monitored_metric_score = monitored_metric_fold_scores[epoch_index]\n",
    "        best_scores_per_fold[monitored_metric].append(best_monitored_metric_score)\n",
    "\n",
    "        for metric in other_metrics:\n",
    "            other_metric_fold_scores = history.history[metric]\n",
    "            # Save only the corresponding current metric score for the\n",
    "            # best validation loss epoch\n",
    "            same_epoch_score = other_metric_fold_scores[epoch_index]\n",
    "            best_scores_per_fold[metric].append(same_epoch_score)\n",
    "\n",
    "        # Construct a classification report and confusion matrix\n",
    "        print('Making predictions...')\n",
    "        pred_start = time()\n",
    "        y_pred = model.predict(X_valid, batch_size=batch_size, verbose=0)\n",
    "        pred_elapsed, pred_elapsed_str = get_time_elapsed(pred_start)\n",
    "        pred_runtimes.append(pred_elapsed)\n",
    "        pred_runtime_strs.append(pred_elapsed_str)\n",
    "\n",
    "        print('Writing classification summary to file...')\n",
    "        save_classification_summary(y_valid,\n",
    "                                    y_pred,\n",
    "                                    [0, 1, 2],\n",
    "                                    ['EAP', 'HPL', 'MWS'],\n",
    "                                    log_file_path,\n",
    "                                    mode='a')\n",
    "\n",
    "    print('Writing CV results and runtime summary to file...')\n",
    "    # Calculate the mean and standard deviation across all folds' best scores\n",
    "    summary_lines = 'CV Results Summary:'\n",
    "    for metric in all_metrics:\n",
    "        mean = np.mean(best_scores_per_fold[metric])\n",
    "        std = np.std(best_scores_per_fold[metric])\n",
    "        summary_lines += f'\\n- {metric} mean and std: {mean:.5f} (+/- {std:.5f})'\n",
    "    monitored_mean = np.mean(best_scores_per_fold[monitored_metric])\n",
    "    summary_file_path = (f'{OUTPUT_SUMMARIES_DIR}{monitored_mean:.5f}.'\n",
    "                         f'{nth_iter_str}.summary.txt')\n",
    "    \n",
    "    # Calculate the suggested number of epochs to train for\n",
    "    # using the entire training dataset\n",
    "    final_num_epochs = np.mean(best_score_num_epochs)\n",
    "    # We take the ceiling because it's better to train for\n",
    "    # a little longer than to underfit\n",
    "    final_num_epochs = int(np.ceil(final_num_epochs))\n",
    "    summary_lines += f'\\n\\nfinal_num_epochs = {final_num_epochs}\\n\\n'\n",
    "    \n",
    "    # Save the best epochs per fold\n",
    "    best_epochs_str = json.dumps(best_score_num_epochs)\n",
    "    summary_lines += f'best_score_num_epochs = {best_epochs_str}\\n\\n'\n",
    "    \n",
    "    # Save the best scores per fold\n",
    "    best_scores_str = json.dumps(best_scores_per_fold, indent=4)\n",
    "    summary_lines += f'best_scores_per_fold = {best_scores_str}\\n\\n'\n",
    "    \n",
    "    # Calculate the total and average runtimes across all folds\n",
    "    training_elapsed, training_elapsed_str = get_time_elapsed(training_start)\n",
    "    training_fold_elapsed_str = format_time_str(training_elapsed / N_SPLITS)\n",
    "    pred_elapsed = np.sum(pred_runtimes)\n",
    "    pred_elapsed_str = format_time_str(pred_elapsed)\n",
    "    pred_fold_elapsed_str = format_time_str(pred_elapsed / N_SPLITS)\n",
    "    summary_lines += (f'Total stratified {N_SPLITS}-fold loop runtime: '\n",
    "                      f'{training_elapsed_str}\\n'\n",
    "                      f'Average training runtime per fold: '\n",
    "                      f'{training_fold_elapsed_str}\\n\\n'\n",
    "                      f'Total stratified {N_SPLITS}-fold prediction runtime: '\n",
    "                      f'{pred_elapsed_str}\\n'\n",
    "                      f'Average prediction runtime per fold: '\n",
    "                      f'{pred_fold_elapsed_str}\\n')\n",
    "    for f, time_str in enumerate(pred_runtime_strs):\n",
    "        summary_lines += f'\\nFold {f + 1} prediction runtime: {time_str}'\n",
    "    \n",
    "    save_line_to_file(summary_lines, summary_file_path, 'w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to an existing output file\n",
    "nth_iter = 1\n",
    "nth_fold = 7\n",
    "iter_fold_scores = pd.read_csv(f'{OUTPUT_SCORES_DIR}iter_{nth_iter:02d}.'\n",
    "                               f'fold_{nth_fold:02d}.scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass in a metric (without the `val_` prefix) and a fold index to show\n",
    "# the training and validation error curves over the number of epochs\n",
    "display_metric_vs_epochs_plot(iter_fold_scores, 'loss', nth_iter, nth_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric_vs_epochs_plot(iter_fold_scores, 'acc', nth_iter, nth_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test data\n",
    "In order to make predictions on the Kaggle-provided unlabeled test data, we will need to submit our predictions to Kaggle. It would be best to train on the entire training set; this means that, this time, we won't provide a validation set to the Keras model.\n",
    "\n",
    "How do we know how many epochs to train for? To figure this out, we can use the results from the cross validation phase. Since we have recorded the number of epochs that each fold took to train the model before stopping, we can take the average number of epochs across all folds and use that as the number of epochs to train our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the best number of epochs based on the evaluation phase\n",
    "final_num_epochs = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model params based on the evaluation phase\n",
    "best_iter = 1\n",
    "final_model_params = load_dictionary_from_file(f'{OUTPUT_MODELS_DIR}'\n",
    "                                               f'iter_{best_iter:02d}.params.json')\n",
    "final_batch_size = final_model_params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the best model params\n",
    "model = build_model(input_shape, final_model_params)\n",
    "# Save the model architecture, weights, and optimizer state to file\n",
    "model.save(f'{OUTPUT_MODELS_DIR}final.model.hdf5')\n",
    "# Save the model summary to file\n",
    "save_model_summary(model, f'{OUTPUT_SUMMARIES_DIR}final.model_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "19579/19579 [==============================] - 11s 567us/step - loss: 0.8891 - acc: 0.6574\n",
      "Epoch 2/9\n",
      "19579/19579 [==============================] - 11s 578us/step - loss: 0.4625 - acc: 0.8723\n",
      "Epoch 3/9\n",
      "19579/19579 [==============================] - 11s 545us/step - loss: 0.3081 - acc: 0.9038\n",
      "Epoch 4/9\n",
      "19579/19579 [==============================] - 11s 538us/step - loss: 0.2325 - acc: 0.9290\n",
      "Epoch 5/9\n",
      "19579/19579 [==============================] - 10s 534us/step - loss: 0.1875 - acc: 0.9434\n",
      "Epoch 6/9\n",
      "19579/19579 [==============================] - 10s 536us/step - loss: 0.1556 - acc: 0.9511\n",
      "Epoch 7/9\n",
      "19579/19579 [==============================] - 10s 528us/step - loss: 0.1378 - acc: 0.9577\n",
      "Epoch 8/9\n",
      "19579/19579 [==============================] - 10s 535us/step - loss: 0.1181 - acc: 0.9643\n",
      "Epoch 9/9\n",
      "19579/19579 [==============================] - 11s 570us/step - loss: 0.1079 - acc: 0.9660\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_tokenized,\n",
    "                    y_train_encoded,\n",
    "                    batch_size=final_batch_size,\n",
    "                    epochs=final_num_epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 1s 98us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tokenized, batch_size=final_batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3719482e-03, 1.8796196e-02, 9.7883189e-01],\n",
       "       [9.9660289e-01, 3.2997099e-03, 9.7403281e-05],\n",
       "       [7.0439349e-03, 9.7891653e-01, 1.4039552e-02]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final submission values\n",
    "predictions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['EAP', 'HPL', 'MWS']] = predictions\n",
    "submission_num = 30\n",
    "submission_description = 'bow_google_mlp_after_tp_i'\n",
    "submission_filename = f'{submission_num:03d}_{submission_description}.csv'\n",
    "submission_file_path = f'{OUTPUT_SUBMISSIONS_DIR}{submission_filename}'\n",
    "submission.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the mean logloss of the Kaggle submission\n",
    "After submitting to Kaggle, we can calculate the mean logloss across the entire test dataset as follows:\n",
    "```\n",
    "Given:\n",
    "    n_test = 8392\n",
    "    %_private = 0.7\n",
    "    %_public = 0.3\n",
    "    private_logloss # Retrieve from Kaggle after submission\n",
    "    public_logloss # Retrieve from Kaggle after submission\n",
    "    \n",
    "Mean logloss = (private_logloss * n_private + public_logloss * n_public) / n_test\n",
    "             = (private_logloss * (%_private * n_test)\n",
    "                + public_logloss * (%_public * n_test))\n",
    "               / n_test\n",
    "               \n",
    "Where n_test = n_private + n_public\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Model\n",
    "Our benchmark model is the sample submission file provided by Kaggle, in which the probability distribution across the three authors is the exact same per sample, such that their probabilities are proportional to the number of samples of said author's work in the training dataset (EAP at about 40%, HPL at about 29%, and MWS at about 31%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 8392\n",
      "Number of private leaderboard samples: 5874.4\n",
      "Number of public leaderboard samples: 2517.6\n"
     ]
    }
   ],
   "source": [
    "n_test = len(submission)\n",
    "n_private = n_test * 0.7\n",
    "n_public = n_test * 0.3\n",
    "print(f'Total number of samples: {n_test}',\n",
    "      f'Number of private leaderboard samples: {n_private}',\n",
    "      f'Number of public leaderboard samples: {n_public}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logloss for the benchmark model: 1.088255\n"
     ]
    }
   ],
   "source": [
    "private_logloss = 1.09094\n",
    "public_logloss = 1.08199\n",
    "mean_logloss = (private_logloss * n_private + public_logloss * n_public) / n_test\n",
    "print(f'Mean logloss for the benchmark model: {mean_logloss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logloss: 0.37335\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_logloss(0.36698, 0.38822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

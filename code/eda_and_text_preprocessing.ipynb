{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: EDA and Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skimming the Text Data\n",
    "- There are some uncommon/misspelled words such as those used by HPL: \"It was all mud an' water, an' the sky was dark, an' the rain was wipin' aout all tracks abaout as fast as could be; but beginnin' at the glen maouth, whar the trees had moved, they was still some o' them awful prints big as bar'ls like he seen Monday.\"\n",
    "- There are some mentions of places like \"Windsor\" and names like \"Dr. Johnson\"\n",
    "- There are uses of uncommon words such as \"pursy\" or made-up words such as \"drest\" (meaning \"dressed\")\n",
    "- UK English is used, such as \"travellers\" or \"neighbour\"\n",
    "- There are some words with multiple spellings, such as \"until\" vs. \"untill\"\n",
    "- EAP sometimes writes in French: \"que tout notre raisonnement se rèduit à céder au sentiment;\"\n",
    "- There are no hyphenated words\n",
    "- Since the data is in CSV format, there are odd uses of punctuation marks like triple double quotes: \"\"\"The nitre\"\" I said: \"\"see, it increases.\"\n",
    "- There do not appear to be any numbers (if they exist, they're spelled out in words)\n",
    "- Punctuations used are: . , : ; ' \" ?\n",
    "- There do not appear to be any words in all caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marifel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marifel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't already have these packages, run this cell\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = '../input/train.csv'\n",
    "TEST_FILE_PATH = '../input/test.csv'\n",
    "SAMPLE_SUBMISSION_FILE_PATH = '../input/sample_submission.csv'\n",
    "\n",
    "train = pd.read_csv(TRAIN_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.groupby.groupby.DataFrameGroupBy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_groups = train.groupby('author') # Returns a list-like object of tuples\n",
    "type(author_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'str'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'str'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "for author, df in author_groups:\n",
    "    print(type(author))\n",
    "    print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample distribution per author out of 19,579 samples:\n",
      "EAP 7,900 samples or 40.35%\n",
      "HPL 5,635 samples or 28.78%\n",
      "MWS 6,044 samples or 30.87%\n"
     ]
    }
   ],
   "source": [
    "# How many samples are there per author?\n",
    "def get_sample_distribution(data, feature):\n",
    "    print('Sample distribution per {} out of {:,} samples:'.format(feature, len(data)))\n",
    "    unique_feature_values = data[feature].unique()\n",
    "    for value in unique_feature_values:\n",
    "        sample_count = len(data[data[feature]==value])\n",
    "        percentage = (sample_count / len(data)) * 100\n",
    "        print(value, '{:,} samples or {:.2f}%'.format(sample_count, percentage))\n",
    "        \n",
    "get_sample_distribution(train, 'author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character count of the text per author:\n",
      "EAP 1,131,484\n",
      "HPL 883,812\n",
      "MWS 922,675\n"
     ]
    }
   ],
   "source": [
    "# Let's concatenate all of the text entries per author\n",
    "def get_text_per_feature(data, feature, text_feature, sep=' '):\n",
    "    print('Character count of the text per {}:'.format(feature))\n",
    "    texts = []\n",
    "    unique_feature_values = data[feature].unique()\n",
    "    for value in unique_feature_values:\n",
    "        entire_text = data[data[feature]==value][text_feature].str.cat(sep=sep)\n",
    "        print(value, '{:,}'.format(len(entire_text)))\n",
    "        texts.append((value, entire_text))\n",
    "    return texts\n",
    "        \n",
    "author_texts = get_text_per_feature(train, 'author', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate basic statistics over all the text samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character length per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    231\n",
       "1     71\n",
       "2    200\n",
       "3    206\n",
       "4    174\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the character length of each sentence\n",
    "char_lens = train['text'].apply(lambda s: len(s))\n",
    "char_lens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19579.000000\n",
       "mean       149.057408\n",
       "std        106.800189\n",
       "min         21.000000\n",
       "25%         81.000000\n",
       "50%        128.000000\n",
       "75%        191.000000\n",
       "max       4663.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count: The number of sentences\n",
    "# mean: Mean length of a sentence\n",
    "# std: Standard deviation of the length of a sentence\n",
    "# min: Minimum length of a sentence\n",
    "# 25%: 25% of sentences are length 81 or shorter\n",
    "# 50%: Median length of a sentence\n",
    "# 75%: 75% of sentences are length 191 or shorter\n",
    "# max: Maximum length of a sentence\n",
    "char_lens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word length per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41\n",
       "1    14\n",
       "2    36\n",
       "3    34\n",
       "4    27\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the character length of each sentence\n",
    "word_lens = train['text'].apply(lambda s: len(s.split(' ')))\n",
    "word_lens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19579.000000\n",
       "mean        26.730477\n",
       "std         19.048353\n",
       "min          2.000000\n",
       "25%         15.000000\n",
       "50%         23.000000\n",
       "75%         34.000000\n",
       "max        861.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count: The number of sentences\n",
    "# mean: Mean word length of a sentence\n",
    "# std: Standard deviation of the word length of a sentence\n",
    "# min: Minimum word length of a sentence\n",
    "# 25%: 25% of sentences are word length 81 or shorter\n",
    "# 50%: Median word length of a sentence\n",
    "# 75%: 75% of sentences are word length 191 or shorter\n",
    "# max: Maximum word length of a sentence\n",
    "word_lens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      id22015\n",
       "text      \"PIQUANT EXPRESSIONS.\n",
       "author                      EAP\n",
       "Name: 6936, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The min and the max appear to be outliers; let's view what these sentences are\n",
    "train.iloc[word_lens.idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                  id27184\n",
       "text      Diotima approached the fountain seated herself...\n",
       "author                                                  MWS\n",
       "Name: 9215, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[word_lens.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732.4598189377784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio of \"number of samples\" (S) to \"number of words per sample\" (W)\n",
    "len(train) / word_lens.mean()\n",
    "# Since this is less than 1500, per Google's\n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-2-5\n",
    "# we should use n-gram inputs into an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the count features per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vocabulary(texts, construct_tokens):\n",
    "    vocab = { 'ALL': Counter() }\n",
    "    \n",
    "    # Word counts\n",
    "    print('Total word count of the text per author:')\n",
    "    for tup in texts:\n",
    "        author = tup[0]\n",
    "        text = tup[1]\n",
    "        tokens = construct_tokens(text)\n",
    "        print(author, '{:,}'.format(len(tokens)))\n",
    "        \n",
    "        # Calculate vocabulary word frequencies (how many times a word appears in the text)\n",
    "        vocab[author] = Counter()\n",
    "        for word in tokens:\n",
    "            vocab['ALL'][word] += 1\n",
    "            vocab[author][word] += 1\n",
    "\n",
    "    # Vocabulary size or unique word counts\n",
    "    print('\\nVocabulary size of the text per author:')\n",
    "    for author in vocab:\n",
    "        print(author, '{:,}'.format(len(vocab[author])))\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count of the text per author:\n",
      "EAP 200,995\n",
      "HPL 156,651\n",
      "MWS 165,710\n",
      "\n",
      "Vocabulary size of the text per author:\n",
      "ALL 47,556\n",
      "EAP 26,521\n",
      "HPL 22,527\n",
      "MWS 20,251\n"
     ]
    }
   ],
   "source": [
    "vocab = construct_vocabulary(author_texts, lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL \n",
      " [('the', 33296), ('of', 20851), ('and', 17059), ('to', 12615), ('I', 10382), ('a', 10359), ('in', 8787), ('was', 6440), ('that', 5988), ('my', 5037)] \n",
      "\n",
      "EAP \n",
      " [('the', 13927), ('of', 8930), ('and', 5222), ('to', 4625), ('a', 4514), ('in', 3750), ('I', 3598), ('was', 2109), ('that', 2085), ('my', 1666)] \n",
      "\n",
      "HPL \n",
      " [('the', 10330), ('and', 5908), ('of', 5792), ('a', 3230), ('to', 3219), ('I', 2629), ('in', 2558), ('was', 2126), ('that', 1902), ('had', 1744)] \n",
      "\n",
      "MWS \n",
      " [('the', 9039), ('of', 6129), ('and', 5929), ('to', 4771), ('I', 4155), ('a', 2615), ('in', 2479), ('my', 2469), ('was', 2205), ('that', 2001)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the most common words per author\n",
    "for author in vocab:\n",
    "    print(author, '\\n', vocab[author].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "As we can see from the results above, there is nothing too distinguishable between author vocabularies due to common words such as \"the\", \"of\", and \"and\"; these are called *stopwords*. Let's remove them and apply other text preprocessing techniques as we see fit. But beforehand, let's review the least common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL \n",
      " [('Ancona.', 1), ('Gulph;', 1), ('Adriatic', 1), ('Stress', 1), ('feel.\"', 1), ('legislators', 1), ('connection?', 1), ('domestic,', 1), ('curtain,', 1), ('praising', 1)] \n",
      "\n",
      "EAP \n",
      " [('received.\"', 1), ('outright.', 1), ('faints', 1), ('agir', 1), ('faut', 1), ('Mais', 1), ('spasm.', 1), ('brush.', 1), ('fancied,', 1), ('turning,', 1)] \n",
      "\n",
      "HPL \n",
      " [('mirth.', 1), ('noon.', 1), ('going,', 1), ('negative', 1), ('\"salt', 1), ('traditional', 1), ('Average', 1), ('flattening', 1), ('casements', 1), ('passageway.', 1)] \n",
      "\n",
      "MWS \n",
      " [('Ancona.', 1), ('Gulph;', 1), ('Adriatic', 1), ('Stress', 1), ('feel.\"', 1), ('pause,', 1), ('legislators', 1), ('connection?', 1), ('domestic,', 1), ('universe,', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the least common words per author\n",
    "for author in vocab:\n",
    "    print(author, '\\n', list(reversed(vocab[author].most_common()))[0:10], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "There are many words that only appear to occur once throughout the corpus or per author, so let's remove them from the vocabulary, along with other less frequent words, since they aren't very predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate infrequent occurrences (such as less than 5 or less than 10) of words from the vocabulary\n",
    "\n",
    "Tokenization: At the character and word levels.\n",
    "\n",
    "Removing punctuation\n",
    "vs.\n",
    "Keeping punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story', \"n't\", 'terrible', ',', 'thought', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_text(text,\n",
    "                 lower=True,\n",
    "                 exclude_punc=False,\n",
    "                 only_alpha=False, \n",
    "                 include_stopwords=False,\n",
    "                 stem=False):\n",
    "    # Convert to lowercase in order to treat \"the\" and \"The\" as the same word\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if exclude_punc:\n",
    "        regex_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        text = regex_punc.sub('', text)\n",
    "        \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove words that are not alphabetic\n",
    "    if only_alpha:\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "        \n",
    "    # Remove stopwords\n",
    "    if not include_stopwords:\n",
    "        nltk_stopwords = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if not t in nltk_stopwords]\n",
    "        \n",
    "    # Todo: Fix misspelled or uncommonly spelled words\n",
    "    \n",
    "    # Reduce words to their stem\n",
    "    if stem:\n",
    "        porter = PorterStemmer()\n",
    "        tokens = [porter.stem(t) for t in tokens]\n",
    "        \n",
    "    # Todo: Add the WordNet lemmatizer\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "t1 = \"The story wasn't so terrible, she thought.\"\n",
    "process_text(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'story', 'was', \"n't\", 'so', 'terrible', ',', 'she', 'thought', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(t1, include_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count of the text per author:\n",
      "EAP 127,478\n",
      "HPL 98,577\n",
      "MWS 101,892\n",
      "\n",
      "Vocabulary size of the text per author:\n",
      "ALL 25,236\n",
      "EAP 15,371\n",
      "HPL 14,416\n",
      "MWS 11,457\n"
     ]
    }
   ],
   "source": [
    "vocab = construct_vocabulary(author_texts, lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "Note that before text preprocessing, we had the following:\n",
    "```\n",
    "Total word count of the text per author:\n",
    "EAP 200,995\n",
    "HPL 156,651\n",
    "MWS 165,710\n",
    "\n",
    "Vocabulary size of the text per author:\n",
    "ALL 47,556\n",
    "EAP 26,521\n",
    "HPL 22,527\n",
    "MWS 20,251\n",
    "```\n",
    "\n",
    "Therefore, text preprocessing drastically reduced the total vocabulary size and word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL \n",
      " [(',', 38220), ('.', 19062), (';', 5159), ('``', 2762), (\"''\", 2207), ('one', 1623), ('upon', 1411), (\"'s\", 1355), ('could', 1330), ('would', 1258)] \n",
      "\n",
      "EAP \n",
      " [(',', 17594), ('.', 7655), ('``', 1628), (\"''\", 1359), (';', 1354), ('upon', 1025), ('one', 655), ('?', 510), ('could', 457), ('would', 416)] \n",
      "\n",
      "HPL \n",
      " [(',', 8581), ('.', 5699), (';', 1143), (\"'s\", 612), (\"'\", 547), ('one', 491), ('could', 490), ('old', 392), ('would', 367), ('``', 297)] \n",
      "\n",
      "MWS \n",
      " [(',', 12045), ('.', 5708), (';', 2662), ('``', 837), (\"''\", 632), ('one', 477), ('would', 475), ('?', 419), ('could', 383), (\"'s\", 352)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the most common words per author\n",
    "for author in vocab:\n",
    "    print(author, '\\n', vocab[author].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count of the text per author:\n",
      "EAP 96,133\n",
      "HPL 81,149\n",
      "MWS 78,800\n",
      "\n",
      "Vocabulary size of the text per author:\n",
      "ALL 25,275\n",
      "EAP 15,276\n",
      "HPL 14,576\n",
      "MWS 11,458\n"
     ]
    }
   ],
   "source": [
    "# Let's try removing punctuation\n",
    "vocab = construct_vocabulary(author_texts, lambda x: process_text(x, exclude_punc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL \n",
      " [('one', 1614), ('upon', 1411), ('could', 1316), ('would', 1241), ('man', 730), ('time', 729), ('yet', 715), ('said', 704), ('even', 701), ('might', 629)] \n",
      "\n",
      "EAP \n",
      " [('upon', 1025), ('one', 652), ('could', 453), ('would', 409), ('said', 356), ('little', 275), ('say', 267), ('well', 267), ('made', 263), ('even', 261)] \n",
      "\n",
      "HPL \n",
      " [('one', 488), ('could', 480), ('old', 392), ('would', 357), ('like', 273), ('seemed', 272), ('man', 260), ('night', 254), ('things', 239), ('time', 238)] \n",
      "\n",
      "MWS \n",
      " [('would', 475), ('one', 474), ('could', 383), ('life', 328), ('yet', 318), ('love', 273), ('us', 272), ('might', 269), ('heart', 262), ('raymond', 248)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the most common words per author\n",
    "for author in vocab:\n",
    "    print(author, '\\n', vocab[author].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size before applying count threshold of 3: 25275\n",
      "Vocab size after applying count threshold of 3: 12094\n"
     ]
    }
   ],
   "source": [
    "# Let's try excluding the least predictive words\n",
    "\n",
    "COUNT_THRESHOLD = 3 # Words should appear at least thrice to remain in the vocabulary\n",
    "\n",
    "print(f'Vocab size before applying count threshold of {COUNT_THRESHOLD}:', len(vocab['ALL']))\n",
    "all_vocab = Counter({ word: count for word, count in vocab['ALL'].items() if count >= COUNT_THRESHOLD })\n",
    "        \n",
    "print(f'Vocab size after applying count threshold of {COUNT_THRESHOLD}:', len(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size before applying count threshold of 2: 25275\n",
      "Vocab size after applying count threshold of 2: 15798\n"
     ]
    }
   ],
   "source": [
    "COUNT_THRESHOLD = 2 # Words should appear at least twice to remain in the vocabulary\n",
    "\n",
    "print(f'Vocab size before applying count threshold of {COUNT_THRESHOLD}:', len(vocab['ALL']))\n",
    "all_vocab = Counter({ word: count for word, count in vocab['ALL'].items() if count >= COUNT_THRESHOLD })\n",
    "        \n",
    "print(f'Vocab size after applying count threshold of {COUNT_THRESHOLD}:', len(all_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorid-test",
   "language": "python",
   "name": "authorid-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

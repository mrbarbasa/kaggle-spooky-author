{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: GloVe Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # '' to run on CPU, '0' to run on the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marifel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marifel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marifel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marifel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't already have these packages, run this cell\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'test_glove'\n",
    "\n",
    "INPUT_DIR = '../input/'\n",
    "TRAIN_FILE_PATH = f'{INPUT_DIR}train.csv'\n",
    "TEST_FILE_PATH = f'{INPUT_DIR}test.csv'\n",
    "SAMPLE_SUBMISSION_FILE_PATH = f'{INPUT_DIR}sample_submission.csv'\n",
    "\n",
    "EMBEDDINGS_DIR = f'{INPUT_DIR}embeddings/'\n",
    "EMBEDDINGS_FILE_PATH = f'{EMBEDDINGS_DIR}glove.840B.300d.txt'\n",
    "\n",
    "OUTPUT_DIR = '../output/'\n",
    "OUTPUT_LOGS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/logs/'\n",
    "OUTPUT_MODELS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/models/'\n",
    "OUTPUT_SCORES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/scores/'\n",
    "OUTPUT_SUBMISSIONS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/submissions/'\n",
    "OUTPUT_SUMMARIES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/summaries/'\n",
    "\n",
    "# Create the output directories if they do not exist (the `_` is necessary\n",
    "# in order to create intermediate directories and is itself not created)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_LOGS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_MODELS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SCORES_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUBMISSIONS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUMMARIES_DIR}_'), exist_ok=True)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_FEATURES = None # The top most common words if an integer; otherwise, all words are used\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "N_SPLITS = 10\n",
    "\n",
    "# Fix a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, submission = load_data(TRAIN_FILE_PATH, \n",
    "                                    TEST_FILE_PATH, \n",
    "                                    SAMPLE_SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27675 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply text preprocessing on each sentence\n",
    "X_train_sequences = list(train['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True,\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "X_test_sequences = list(test['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True,\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "\n",
    "# Tokenize and pad the sentences\n",
    "X_train_tokenized, X_test_tokenized, word_index = compute_word_index(X_train_sequences,\n",
    "                                                                     X_test_sequences,\n",
    "                                                                     MAX_FEATURES,\n",
    "                                                                     MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:05, 17430.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings(EMBEDDINGS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary words not found in the pre-trained embeddings: 1634 of 27675 (5.90%)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, vocab_size, num_unknown = construct_embedding_matrix(word_index, \n",
    "                                                                       embeddings_index, \n",
    "                                                                       EMBEDDING_DIM)\n",
    "# Here we subtract 1 from the vocab size because 1 has been added to the\n",
    "# actual number of tokens to account for masking in the embedding matrix\n",
    "unknown_word_percentage = (num_unknown / (vocab_size - 1)) * 100\n",
    "unknown_word_lines = ('Number of vocabulary words not found in the pre-trained embeddings: '\n",
    "                      f'{num_unknown} of {vocab_size - 1} '\n",
    "                      f'({unknown_word_percentage:.2f}%)')\n",
    "print(unknown_word_lines)\n",
    "preprocessing_file_path = f'{OUTPUT_LOGS_DIR}preprocessing.log.txt'\n",
    "save_line_to_file(unknown_word_lines, preprocessing_file_path, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels: ['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "# The target classes need to be converted to integers so that\n",
    "# EAP --> 0\n",
    "# HPL --> 1\n",
    "# MWS --> 2\n",
    "y_train_integers = integer_encode_classes(train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target classes need to be one-hot encoded so that\n",
    "# EAP --> 0 --> [1, 0, 0]\n",
    "# HPL --> 1 --> [0, 1, 0]\n",
    "# MWS --> 2 --> [0, 0, 1]\n",
    "y_train_encoded = one_hot_encode_classes(y_train_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model-dependent files\n",
    "from models import build_embedding_layer, build_model_callbacks, save_model_summary\n",
    "from models import get_random_cnn_params as get_random_model_params\n",
    "from models import build_cnn_model as build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the best number of epochs based on the evaluation phase\n",
    "final_num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model params based on the evaluation phase\n",
    "# CNN\n",
    "final_model_params = {\n",
    "    'batch_size': 64,\n",
    "    'filters': 250,\n",
    "    'kernel_size': 3,\n",
    "    'dropout_rate': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'use_special_arch': True,\n",
    "    'normal_arch_params': {}\n",
    "}\n",
    "# RNN\n",
    "# final_model_params = {\n",
    "#     'batch_size': 64,\n",
    "#     'use_gru_layer': True,\n",
    "#     'use_global_max_pooling_layer': True,\n",
    "#     'units': 128,\n",
    "#     'spatial_dropout_rate': 0.2,\n",
    "#     'optimizer': 'adam',\n",
    "#     'num_rnn_stacks': 1,\n",
    "# }\n",
    "final_batch_size = final_model_params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the embedding layer\n",
    "embedding_layer = build_embedding_layer(embedding_matrix, \n",
    "                                        vocab_size, \n",
    "                                        EMBEDDING_DIM, \n",
    "                                        MAX_SEQUENCE_LENGTH)\n",
    "# Build the model with the best model params\n",
    "model = build_model(embedding_layer, MAX_SEQUENCE_LENGTH, final_model_params)\n",
    "# Save the model architecture, weights, and optimizer state to file\n",
    "model.save(f'{OUTPUT_MODELS_DIR}final.model.hdf5')\n",
    "# Save the model summary to file\n",
    "save_model_summary(model, f'{OUTPUT_SUMMARIES_DIR}final.model_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "19579/19579 [==============================] - 28s 1ms/step - loss: 0.7302 - acc: 0.6792\n",
      "Epoch 2/3\n",
      "19579/19579 [==============================] - 29s 1ms/step - loss: 0.4955 - acc: 0.8008\n",
      "Epoch 3/3\n",
      "19579/19579 [==============================] - 27s 1ms/step - loss: 0.3777 - acc: 0.8473\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_tokenized,\n",
    "                    y_train_encoded,\n",
    "                    batch_size=final_batch_size,\n",
    "                    epochs=final_num_epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 4s 421us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tokenized, batch_size=final_batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05645953, 0.00371291, 0.9398276 ],\n",
       "       [0.97289103, 0.02237941, 0.00472952],\n",
       "       [0.00460288, 0.9932817 , 0.00211538]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final submission values\n",
    "predictions[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['EAP', 'HPL', 'MWS']] = predictions\n",
    "submission_num = 26\n",
    "submission_description = 'glove_best_manual_cnn_after_tp_i_2'\n",
    "submission_filename = f'{submission_num:03d}_{submission_description}.csv'\n",
    "submission_file_path = f'{OUTPUT_SUBMISSIONS_DIR}{submission_filename}'\n",
    "submission.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logloss: 0.47776\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_logloss(0.47272, 0.48951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorid-test",
   "language": "python",
   "name": "authorid-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

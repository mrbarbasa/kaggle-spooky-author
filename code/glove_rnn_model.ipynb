{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: GloVe RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # '' to run on CPU, '0' to run on the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marifel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marifel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marifel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/marifel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you don't already have these packages, run this cell\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'glove_rnn'\n",
    "\n",
    "INPUT_DIR = '../input/'\n",
    "TRAIN_FILE_PATH = f'{INPUT_DIR}train.csv'\n",
    "TEST_FILE_PATH = f'{INPUT_DIR}test.csv'\n",
    "SAMPLE_SUBMISSION_FILE_PATH = f'{INPUT_DIR}sample_submission.csv'\n",
    "\n",
    "EMBEDDINGS_DIR = f'{INPUT_DIR}embeddings/'\n",
    "EMBEDDINGS_FILE_PATH = f'{EMBEDDINGS_DIR}glove.840B.300d.txt'\n",
    "\n",
    "OUTPUT_DIR = '../output/'\n",
    "OUTPUT_LOGS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/logs/'\n",
    "OUTPUT_MODELS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/models/'\n",
    "OUTPUT_SCORES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/scores/'\n",
    "OUTPUT_SUBMISSIONS_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/submissions/'\n",
    "OUTPUT_SUMMARIES_DIR = f'{OUTPUT_DIR}{MODEL_NAME}/summaries/'\n",
    "\n",
    "# Create the output directories if they do not exist (the `_` is necessary\n",
    "# in order to create intermediate directories and is itself not created)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_LOGS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_MODELS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SCORES_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUBMISSIONS_DIR}_'), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(f'{OUTPUT_SUMMARIES_DIR}_'), exist_ok=True)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_FEATURES = None # The top most common words if an integer; otherwise, all words are used\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "N_SPLITS = 10\n",
    "\n",
    "# Fix a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, submission = load_data(TRAIN_FILE_PATH, \n",
    "                                    TEST_FILE_PATH, \n",
    "                                    SAMPLE_SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27675 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply text preprocessing on each sentence\n",
    "X_train_sequences = list(train['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True,\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "X_test_sequences = list(test['text'].apply(\n",
    "    lambda x: process_text(x,\n",
    "                           lower=False,\n",
    "                           remove_punc=False,\n",
    "                           normalize_spelling=True,\n",
    "                           stem=False,\n",
    "                           lemmatize=False,\n",
    "                           remove_stopwords=False)).values)\n",
    "\n",
    "# Tokenize and pad the sentences\n",
    "X_train_tokenized, X_test_tokenized, word_index = compute_word_index(X_train_sequences,\n",
    "                                                                     X_test_sequences,\n",
    "                                                                     MAX_FEATURES,\n",
    "                                                                     MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the embedding layer\n",
    "These results will later be used during stratified k-fold to construct the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [02:06, 17414.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings(EMBEDDINGS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary words not found in the pre-trained embeddings: 1634 of 27675 (5.90%)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, vocab_size, num_unknown = construct_embedding_matrix(word_index, \n",
    "                                                                       embeddings_index, \n",
    "                                                                       EMBEDDING_DIM,\n",
    "                                                                       MAX_FEATURES)\n",
    "# Here we subtract 1 from the vocab size because 1 has been added to the\n",
    "# actual number of tokens to account for masking in the embedding matrix\n",
    "unknown_word_percentage = (num_unknown / (vocab_size - 1)) * 100\n",
    "unknown_word_lines = ('Number of vocabulary words not found in the pre-trained embeddings: '\n",
    "                      f'{num_unknown} of {vocab_size - 1} '\n",
    "                      f'({unknown_word_percentage:.2f}%)')\n",
    "print(unknown_word_lines)\n",
    "preprocessing_file_path = f'{OUTPUT_LOGS_DIR}preprocessing.log.txt'\n",
    "save_line_to_file(unknown_word_lines, preprocessing_file_path, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Each model will be evaluated based on the logloss metric using either 5-fold or 10-fold cross validation; the lower the logloss, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels: ['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "# The target classes need to be converted to integers so that\n",
    "# EAP --> 0\n",
    "# HPL --> 1\n",
    "# MWS --> 2\n",
    "y_train_integers = integer_encode_classes(train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target classes need to be one-hot encoded so that\n",
    "# EAP --> 0 --> [1, 0, 0]\n",
    "# HPL --> 1 --> [0, 1, 0]\n",
    "# MWS --> 2 --> [0, 0, 1]\n",
    "y_train_encoded = one_hot_encode_classes(y_train_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model-dependent files\n",
    "from models import build_embedding_layer, build_model_callbacks, save_model_summary\n",
    "from models import get_random_rnn_params as get_random_model_params\n",
    "from models import build_rnn_model as build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Iteration 1 of 1 -----\n",
      "Writing model params to file...\n",
      "\n",
      "----- Fold 1 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/100\n",
      "17620/17620 [==============================] - 5s 302us/step - loss: 0.7312 - acc: 0.6857 - val_loss: 0.5950 - val_acc: 0.7524\n",
      "Epoch 2/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.5513 - acc: 0.7751 - val_loss: 0.5618 - val_acc: 0.7596\n",
      "Epoch 3/100\n",
      "17620/17620 [==============================] - 4s 236us/step - loss: 0.4668 - acc: 0.8158 - val_loss: 0.4649 - val_acc: 0.8055\n",
      "Epoch 4/100\n",
      "17620/17620 [==============================] - 4s 240us/step - loss: 0.4033 - acc: 0.8410 - val_loss: 0.4432 - val_acc: 0.8132\n",
      "Epoch 5/100\n",
      "17620/17620 [==============================] - 4s 238us/step - loss: 0.3510 - acc: 0.8658 - val_loss: 0.4314 - val_acc: 0.8188\n",
      "Epoch 6/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.2997 - acc: 0.8881 - val_loss: 0.4222 - val_acc: 0.8198\n",
      "Epoch 7/100\n",
      "17620/17620 [==============================] - 4s 236us/step - loss: 0.2595 - acc: 0.9075 - val_loss: 0.4190 - val_acc: 0.8321\n",
      "Epoch 8/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.2216 - acc: 0.9245 - val_loss: 0.4245 - val_acc: 0.8336\n",
      "Epoch 9/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.1883 - acc: 0.9345 - val_loss: 0.4763 - val_acc: 0.8244\n",
      "Epoch 10/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.1593 - acc: 0.9468 - val_loss: 0.4728 - val_acc: 0.8193\n",
      "Epoch 00010: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 2 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/100\n",
      "17620/17620 [==============================] - 6s 315us/step - loss: 0.7442 - acc: 0.6742 - val_loss: 0.6011 - val_acc: 0.7448\n",
      "Epoch 2/100\n",
      "17620/17620 [==============================] - 4s 237us/step - loss: 0.5603 - acc: 0.7694 - val_loss: 0.5005 - val_acc: 0.7999\n",
      "Epoch 3/100\n",
      "17620/17620 [==============================] - 4s 236us/step - loss: 0.4758 - acc: 0.8073 - val_loss: 0.4556 - val_acc: 0.8218\n",
      "Epoch 4/100\n",
      "17620/17620 [==============================] - 4s 239us/step - loss: 0.4064 - acc: 0.8389 - val_loss: 0.4435 - val_acc: 0.8208\n",
      "Epoch 5/100\n",
      "17620/17620 [==============================] - 4s 243us/step - loss: 0.3559 - acc: 0.8648 - val_loss: 0.3986 - val_acc: 0.8448\n",
      "Epoch 6/100\n",
      "17620/17620 [==============================] - 4s 237us/step - loss: 0.3079 - acc: 0.8841 - val_loss: 0.4550 - val_acc: 0.8173\n",
      "Epoch 7/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.2659 - acc: 0.9037 - val_loss: 0.3839 - val_acc: 0.8453\n",
      "Epoch 8/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.2242 - acc: 0.9193 - val_loss: 0.3825 - val_acc: 0.8520\n",
      "Epoch 9/100\n",
      "17620/17620 [==============================] - 4s 238us/step - loss: 0.1893 - acc: 0.9349 - val_loss: 0.3890 - val_acc: 0.8494\n",
      "Epoch 10/100\n",
      "17620/17620 [==============================] - 4s 240us/step - loss: 0.1584 - acc: 0.9470 - val_loss: 0.3862 - val_acc: 0.8540\n",
      "Epoch 11/100\n",
      "17620/17620 [==============================] - 4s 240us/step - loss: 0.1352 - acc: 0.9561 - val_loss: 0.4153 - val_acc: 0.8494\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 3 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/100\n",
      "17620/17620 [==============================] - 6s 315us/step - loss: 0.7330 - acc: 0.6806 - val_loss: 0.6007 - val_acc: 0.7514\n",
      "Epoch 2/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.5543 - acc: 0.7719 - val_loss: 0.5304 - val_acc: 0.7805\n",
      "Epoch 3/100\n",
      "17620/17620 [==============================] - 4s 237us/step - loss: 0.4699 - acc: 0.8120 - val_loss: 0.5336 - val_acc: 0.7897\n",
      "Epoch 4/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.4068 - acc: 0.8393 - val_loss: 0.4907 - val_acc: 0.8004\n",
      "Epoch 5/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.3474 - acc: 0.8682 - val_loss: 0.4484 - val_acc: 0.8121\n",
      "Epoch 6/100\n",
      "17620/17620 [==============================] - 4s 236us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.4529 - val_acc: 0.8203\n",
      "Epoch 7/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.2569 - acc: 0.9043 - val_loss: 0.4543 - val_acc: 0.8224\n",
      "Epoch 8/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.2218 - acc: 0.9220 - val_loss: 0.4480 - val_acc: 0.8295\n",
      "Epoch 9/100\n",
      "17620/17620 [==============================] - 4s 238us/step - loss: 0.1824 - acc: 0.9367 - val_loss: 0.4814 - val_acc: 0.8259\n",
      "Epoch 10/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.1537 - acc: 0.9472 - val_loss: 0.4635 - val_acc: 0.8239\n",
      "Epoch 11/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.1320 - acc: 0.9561 - val_loss: 0.4818 - val_acc: 0.8259\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 4 of 10 -----\n",
      "Train on 17620 samples, validate on 1959 samples\n",
      "Epoch 1/100\n",
      "17620/17620 [==============================] - 6s 322us/step - loss: 0.7404 - acc: 0.6804 - val_loss: 0.5817 - val_acc: 0.7580\n",
      "Epoch 2/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.5629 - acc: 0.7683 - val_loss: 0.5117 - val_acc: 0.7805\n",
      "Epoch 3/100\n",
      "17620/17620 [==============================] - 4s 238us/step - loss: 0.4766 - acc: 0.8085 - val_loss: 0.4627 - val_acc: 0.8050\n",
      "Epoch 4/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.4065 - acc: 0.8407 - val_loss: 0.4205 - val_acc: 0.8367\n",
      "Epoch 5/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.3547 - acc: 0.8642 - val_loss: 0.4155 - val_acc: 0.8321\n",
      "Epoch 6/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.3006 - acc: 0.8894 - val_loss: 0.3819 - val_acc: 0.8489\n",
      "Epoch 7/100\n",
      "17620/17620 [==============================] - 4s 236us/step - loss: 0.2606 - acc: 0.9014 - val_loss: 0.3800 - val_acc: 0.8479\n",
      "Epoch 8/100\n",
      "17620/17620 [==============================] - 4s 233us/step - loss: 0.2228 - acc: 0.9205 - val_loss: 0.4406 - val_acc: 0.8239\n",
      "Epoch 9/100\n",
      "17620/17620 [==============================] - 4s 233us/step - loss: 0.1882 - acc: 0.9379 - val_loss: 0.3728 - val_acc: 0.8509\n",
      "Epoch 10/100\n",
      "17620/17620 [==============================] - 4s 238us/step - loss: 0.1593 - acc: 0.9481 - val_loss: 0.3825 - val_acc: 0.8479\n",
      "Epoch 11/100\n",
      "17620/17620 [==============================] - 4s 234us/step - loss: 0.1353 - acc: 0.9568 - val_loss: 0.3990 - val_acc: 0.8464\n",
      "Epoch 12/100\n",
      "17620/17620 [==============================] - 4s 235us/step - loss: 0.1140 - acc: 0.9648 - val_loss: 0.4328 - val_acc: 0.8407\n",
      "Epoch 00012: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 5 of 10 -----\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 6s 336us/step - loss: 0.7420 - acc: 0.6749 - val_loss: 0.5731 - val_acc: 0.7697\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 4s 238us/step - loss: 0.5627 - acc: 0.7685 - val_loss: 0.4993 - val_acc: 0.7972\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 4s 233us/step - loss: 0.4778 - acc: 0.8046 - val_loss: 0.4813 - val_acc: 0.8023\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 4s 239us/step - loss: 0.4088 - acc: 0.8396 - val_loss: 0.4276 - val_acc: 0.8294\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 4s 238us/step - loss: 0.3545 - acc: 0.8635 - val_loss: 0.3985 - val_acc: 0.8412\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 4s 238us/step - loss: 0.3029 - acc: 0.8849 - val_loss: 0.3977 - val_acc: 0.8437\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 4s 237us/step - loss: 0.2669 - acc: 0.8998 - val_loss: 0.4003 - val_acc: 0.8488\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 4s 238us/step - loss: 0.2217 - acc: 0.9204 - val_loss: 0.3869 - val_acc: 0.8590\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 4s 240us/step - loss: 0.1901 - acc: 0.9342 - val_loss: 0.4009 - val_acc: 0.8473\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - 4s 239us/step - loss: 0.1629 - acc: 0.9448 - val_loss: 0.4097 - val_acc: 0.8473\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 4s 238us/step - loss: 0.1358 - acc: 0.9550 - val_loss: 0.3967 - val_acc: 0.8514\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 6 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/100\n",
      "17622/17622 [==============================] - 6s 350us/step - loss: 0.7315 - acc: 0.6843 - val_loss: 0.5739 - val_acc: 0.7660\n",
      "Epoch 2/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.5527 - acc: 0.7720 - val_loss: 0.5183 - val_acc: 0.7890\n",
      "Epoch 3/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.4659 - acc: 0.8145 - val_loss: 0.5226 - val_acc: 0.7854\n",
      "Epoch 4/100\n",
      "17622/17622 [==============================] - 4s 244us/step - loss: 0.3996 - acc: 0.8405 - val_loss: 0.4841 - val_acc: 0.8007\n",
      "Epoch 5/100\n",
      "17622/17622 [==============================] - 4s 237us/step - loss: 0.3503 - acc: 0.8658 - val_loss: 0.4451 - val_acc: 0.8258\n",
      "Epoch 6/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.2971 - acc: 0.8884 - val_loss: 0.4294 - val_acc: 0.8334\n",
      "Epoch 7/100\n",
      "17622/17622 [==============================] - 4s 238us/step - loss: 0.2512 - acc: 0.9090 - val_loss: 0.4600 - val_acc: 0.8273\n",
      "Epoch 8/100\n",
      "17622/17622 [==============================] - 4s 235us/step - loss: 0.2219 - acc: 0.9227 - val_loss: 0.4366 - val_acc: 0.8329\n",
      "Epoch 9/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.1825 - acc: 0.9370 - val_loss: 0.4540 - val_acc: 0.8263\n",
      "Epoch 00009: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 7 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/100\n",
      "17622/17622 [==============================] - 6s 346us/step - loss: 0.7280 - acc: 0.6855 - val_loss: 0.5967 - val_acc: 0.7501\n",
      "Epoch 2/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.5494 - acc: 0.7739 - val_loss: 0.5327 - val_acc: 0.7849\n",
      "Epoch 3/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.4720 - acc: 0.8089 - val_loss: 0.5081 - val_acc: 0.8017\n",
      "Epoch 4/100\n",
      "17622/17622 [==============================] - 4s 237us/step - loss: 0.3985 - acc: 0.8458 - val_loss: 0.4596 - val_acc: 0.8268\n",
      "Epoch 5/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.3448 - acc: 0.8684 - val_loss: 0.4434 - val_acc: 0.8298\n",
      "Epoch 6/100\n",
      "17622/17622 [==============================] - 4s 242us/step - loss: 0.3049 - acc: 0.8845 - val_loss: 0.4307 - val_acc: 0.8298\n",
      "Epoch 7/100\n",
      "17622/17622 [==============================] - 4s 242us/step - loss: 0.2567 - acc: 0.9084 - val_loss: 0.4768 - val_acc: 0.8191\n",
      "Epoch 8/100\n",
      "17622/17622 [==============================] - 4s 235us/step - loss: 0.2224 - acc: 0.9208 - val_loss: 0.4252 - val_acc: 0.8324\n",
      "Epoch 9/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.1831 - acc: 0.9374 - val_loss: 0.4373 - val_acc: 0.8385\n",
      "Epoch 10/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.1550 - acc: 0.9489 - val_loss: 0.4603 - val_acc: 0.8298\n",
      "Epoch 11/100\n",
      "17622/17622 [==============================] - 4s 245us/step - loss: 0.1284 - acc: 0.9594 - val_loss: 0.4733 - val_acc: 0.8258\n",
      "Epoch 00011: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 8 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/100\n",
      "17622/17622 [==============================] - 6s 365us/step - loss: 0.7385 - acc: 0.6776 - val_loss: 0.5744 - val_acc: 0.7639\n",
      "Epoch 2/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.5503 - acc: 0.7770 - val_loss: 0.5070 - val_acc: 0.7885\n",
      "Epoch 3/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.4713 - acc: 0.8114 - val_loss: 0.4518 - val_acc: 0.8145\n",
      "Epoch 4/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.4069 - acc: 0.8429 - val_loss: 0.4257 - val_acc: 0.8232\n",
      "Epoch 5/100\n",
      "17622/17622 [==============================] - 4s 243us/step - loss: 0.3550 - acc: 0.8631 - val_loss: 0.4076 - val_acc: 0.8360\n",
      "Epoch 6/100\n",
      "17622/17622 [==============================] - 4s 243us/step - loss: 0.2989 - acc: 0.8869 - val_loss: 0.3952 - val_acc: 0.8401\n",
      "Epoch 7/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.2589 - acc: 0.9063 - val_loss: 0.4027 - val_acc: 0.8390\n",
      "Epoch 8/100\n",
      "17622/17622 [==============================] - 4s 238us/step - loss: 0.2187 - acc: 0.9220 - val_loss: 0.3789 - val_acc: 0.8406\n",
      "Epoch 9/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.1887 - acc: 0.9344 - val_loss: 0.3998 - val_acc: 0.8554\n",
      "Epoch 10/100\n",
      "17622/17622 [==============================] - 4s 242us/step - loss: 0.1552 - acc: 0.9490 - val_loss: 0.3764 - val_acc: 0.8493\n",
      "Epoch 11/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.1296 - acc: 0.9589 - val_loss: 0.3845 - val_acc: 0.8544\n",
      "Epoch 12/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.1124 - acc: 0.9637 - val_loss: 0.4387 - val_acc: 0.8513\n",
      "Epoch 13/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.0994 - acc: 0.9682 - val_loss: 0.4103 - val_acc: 0.8600\n",
      "Epoch 00013: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 9 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/100\n",
      "17622/17622 [==============================] - 6s 358us/step - loss: 0.7348 - acc: 0.6807 - val_loss: 0.6066 - val_acc: 0.7578\n",
      "Epoch 2/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.5519 - acc: 0.7745 - val_loss: 0.5298 - val_acc: 0.7890\n",
      "Epoch 3/100\n",
      "17622/17622 [==============================] - 4s 236us/step - loss: 0.4669 - acc: 0.8145 - val_loss: 0.4704 - val_acc: 0.8094\n",
      "Epoch 4/100\n",
      "17622/17622 [==============================] - 4s 238us/step - loss: 0.4044 - acc: 0.8405 - val_loss: 0.4639 - val_acc: 0.8186\n",
      "Epoch 5/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.3465 - acc: 0.8683 - val_loss: 0.4279 - val_acc: 0.8314\n",
      "Epoch 6/100\n",
      "17622/17622 [==============================] - 4s 236us/step - loss: 0.2964 - acc: 0.8885 - val_loss: 0.4479 - val_acc: 0.8304\n",
      "Epoch 7/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.2543 - acc: 0.9053 - val_loss: 0.4138 - val_acc: 0.8411\n",
      "Epoch 8/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.2176 - acc: 0.9220 - val_loss: 0.4162 - val_acc: 0.8421\n",
      "Epoch 9/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.1810 - acc: 0.9375 - val_loss: 0.5094 - val_acc: 0.8181\n",
      "Epoch 10/100\n",
      "17622/17622 [==============================] - 4s 238us/step - loss: 0.1526 - acc: 0.9505 - val_loss: 0.4571 - val_acc: 0.8360\n",
      "Epoch 00010: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "\n",
      "----- Fold 10 of 10 -----\n",
      "Train on 17622 samples, validate on 1957 samples\n",
      "Epoch 1/100\n",
      "17622/17622 [==============================] - 6s 366us/step - loss: 0.7456 - acc: 0.6773 - val_loss: 0.5654 - val_acc: 0.7762\n",
      "Epoch 2/100\n",
      "17622/17622 [==============================] - 4s 245us/step - loss: 0.5584 - acc: 0.7735 - val_loss: 0.5509 - val_acc: 0.7680\n",
      "Epoch 3/100\n",
      "17622/17622 [==============================] - 4s 243us/step - loss: 0.4786 - acc: 0.8088 - val_loss: 0.4820 - val_acc: 0.8012\n",
      "Epoch 4/100\n",
      "17622/17622 [==============================] - 4s 244us/step - loss: 0.4078 - acc: 0.8413 - val_loss: 0.4325 - val_acc: 0.8217\n",
      "Epoch 5/100\n",
      "17622/17622 [==============================] - 4s 245us/step - loss: 0.3490 - acc: 0.8653 - val_loss: 0.4266 - val_acc: 0.8304\n",
      "Epoch 6/100\n",
      "17622/17622 [==============================] - 4s 241us/step - loss: 0.3021 - acc: 0.8877 - val_loss: 0.4197 - val_acc: 0.8329\n",
      "Epoch 7/100\n",
      "17622/17622 [==============================] - 4s 242us/step - loss: 0.2565 - acc: 0.9055 - val_loss: 0.4120 - val_acc: 0.8380\n",
      "Epoch 8/100\n",
      "17622/17622 [==============================] - 4s 244us/step - loss: 0.2219 - acc: 0.9208 - val_loss: 0.4143 - val_acc: 0.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "17622/17622 [==============================] - 4s 240us/step - loss: 0.1868 - acc: 0.9361 - val_loss: 0.4138 - val_acc: 0.8385\n",
      "Epoch 10/100\n",
      "17622/17622 [==============================] - 4s 239us/step - loss: 0.1534 - acc: 0.9493 - val_loss: 0.4234 - val_acc: 0.8390\n",
      "Epoch 00010: early stopping\n",
      "Making predictions...\n",
      "Writing classification summary to file...\n",
      "Writing CV results and runtime summary to file...\n"
     ]
    }
   ],
   "source": [
    "# Random search\n",
    "\n",
    "training_num_epochs = 100 # With early stopping in place\n",
    "num_random_search_iter = 10 # The number of iterations\n",
    "offset_iter = 0 # How many iterations to offset the nth iteration by\n",
    "\n",
    "# Calculate the k-fold splits (same splits across all iterations)\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Randomly search for the best model using 10-fold cross validation for each iteration.\n",
    "# Note: Before re-running this cell, you might want to delete the existing files in the\n",
    "# `OUTPUT_SUMMARIES_DIR` folder, since the files within might not get overwritten.\n",
    "for nth_iter in range(1+offset_iter, num_random_search_iter+1+offset_iter):\n",
    "    \n",
    "    # Start the training timer for the current iteration\n",
    "    training_start = time()\n",
    "    \n",
    "    nth_iter_str = f'iter_{nth_iter:02d}'\n",
    "    print(f'\\n----- Iteration {nth_iter} of {num_random_search_iter+offset_iter} -----')\n",
    "\n",
    "    # Metrics to monitor and record\n",
    "    monitored_metric = 'val_loss'\n",
    "    other_metrics = ['val_acc', 'loss', 'acc']\n",
    "    all_metrics = ['val_loss', 'val_acc', 'loss', 'acc']\n",
    "    \n",
    "    # The number of training epochs that have passed at which\n",
    "    # the best validation loss is recorded\n",
    "    best_score_num_epochs = []\n",
    "    \n",
    "    # The metric scores recorded from the epoch number at which\n",
    "    # the best validation loss is recorded.\n",
    "    # Note that each list should be `N_SPLITS` in length.\n",
    "    best_scores_per_fold = { 'val_loss': [], 'val_acc': [], 'loss': [], 'acc': [] }\n",
    "    \n",
    "    # Runtime records\n",
    "    pred_runtimes = []\n",
    "    pred_runtime_strs = []\n",
    "\n",
    "    # Retrieve the random model params for the current iteration\n",
    "    random_model_params = get_random_model_params()\n",
    "    batch_size = random_model_params['batch_size']\n",
    "    \n",
    "    print('Writing model params to file...')\n",
    "    params_file_path = f'{OUTPUT_MODELS_DIR}{nth_iter_str}.params.json'\n",
    "    save_dictionary_to_file(random_model_params, params_file_path)    \n",
    "    \n",
    "    # The model summary, improvement log, and classification summary file path\n",
    "    log_file_path = f'{OUTPUT_LOGS_DIR}{nth_iter_str}.log.txt'\n",
    "    \n",
    "    # Prepare the generator\n",
    "    folds = kfold.split(X_train_tokenized, y_train_integers)\n",
    "    \n",
    "    # Begin 10-fold cross validation for the current set of random model params\n",
    "    for fold, (train_indices, valid_indices) in enumerate(folds):\n",
    "        nth_fold = fold + 1\n",
    "        nth_fold_str = f'fold_{nth_fold:02d}'\n",
    "        print(f'\\n----- Fold {nth_fold} of {N_SPLITS} -----')\n",
    "\n",
    "        # Prepare the splits of data\n",
    "        X_train, y_train = X_train_tokenized[train_indices], y_train_encoded[train_indices]\n",
    "        X_valid, y_valid = X_train_tokenized[valid_indices], y_train_encoded[valid_indices]\n",
    "\n",
    "        # Build the embedding layer\n",
    "        embedding_layer = build_embedding_layer(embedding_matrix, \n",
    "                                                vocab_size, \n",
    "                                                EMBEDDING_DIM, \n",
    "                                                MAX_SEQUENCE_LENGTH)\n",
    "        # Construct model callbacks, save the best models, and log metrics to file\n",
    "        logger_file_path = f'{OUTPUT_SCORES_DIR}{nth_iter_str}.{nth_fold_str}.scores.csv'\n",
    "        model_callbacks = build_model_callbacks(monitored_metric,\n",
    "                                                'min',\n",
    "                                                log_file_path, # progress\n",
    "                                                None, # model\n",
    "                                                logger_file_path, # logger\n",
    "                                                nth_fold,\n",
    "                                                N_SPLITS)\n",
    "        # Build the model\n",
    "        model = build_model(embedding_layer, MAX_SEQUENCE_LENGTH, random_model_params)    \n",
    "        # Save the model summary to file on the first fold only\n",
    "        # since the models are identical across folds\n",
    "        if nth_fold == 1:\n",
    "            save_model_summary(model, log_file_path)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=training_num_epochs,\n",
    "                            verbose=1,\n",
    "                            callbacks=model_callbacks,\n",
    "                            validation_data=[X_valid, y_valid],\n",
    "                            shuffle=True)\n",
    "\n",
    "        # Save the scores for later evaluation\n",
    "        monitored_metric_fold_scores = history.history[monitored_metric]\n",
    "        # Save only the best validation loss\n",
    "        epoch_index = np.argmin(monitored_metric_fold_scores)\n",
    "        best_score_num_epochs.append(int(epoch_index + 1))\n",
    "        best_monitored_metric_score = monitored_metric_fold_scores[epoch_index]\n",
    "        best_scores_per_fold[monitored_metric].append(best_monitored_metric_score)\n",
    "\n",
    "        for metric in other_metrics:\n",
    "            other_metric_fold_scores = history.history[metric]\n",
    "            # Save only the corresponding current metric score for the\n",
    "            # best validation loss epoch\n",
    "            same_epoch_score = other_metric_fold_scores[epoch_index]\n",
    "            best_scores_per_fold[metric].append(same_epoch_score)\n",
    "\n",
    "        # Construct a classification report and confusion matrix\n",
    "        print('Making predictions...')\n",
    "        pred_start = time()\n",
    "        y_pred = model.predict(X_valid, batch_size=batch_size, verbose=0)\n",
    "        pred_elapsed, pred_elapsed_str = get_time_elapsed(pred_start)\n",
    "        pred_runtimes.append(pred_elapsed)\n",
    "        pred_runtime_strs.append(pred_elapsed_str)\n",
    "\n",
    "        print('Writing classification summary to file...')\n",
    "        save_classification_summary(y_valid,\n",
    "                                    y_pred,\n",
    "                                    [0, 1, 2],\n",
    "                                    ['EAP', 'HPL', 'MWS'],\n",
    "                                    log_file_path,\n",
    "                                    mode='a')\n",
    "\n",
    "    print('Writing CV results and runtime summary to file...')\n",
    "    # Calculate the mean and standard deviation across all folds' best scores\n",
    "    summary_lines = 'CV Results Summary:'\n",
    "    for metric in all_metrics:\n",
    "        mean = np.mean(best_scores_per_fold[metric])\n",
    "        std = np.std(best_scores_per_fold[metric])\n",
    "        summary_lines += f'\\n- {metric} mean and std: {mean:.5f} (+/- {std:.5f})'\n",
    "    monitored_mean = np.mean(best_scores_per_fold[monitored_metric])\n",
    "    summary_file_path = (f'{OUTPUT_SUMMARIES_DIR}{monitored_mean:.5f}.'\n",
    "                         f'{nth_iter_str}.summary.txt')\n",
    "    \n",
    "    # Calculate the suggested number of epochs to train for\n",
    "    # using the entire training dataset\n",
    "    final_num_epochs = np.mean(best_score_num_epochs)\n",
    "    # We take the ceiling because it's better to train for\n",
    "    # a little longer than to underfit\n",
    "    final_num_epochs = int(np.ceil(final_num_epochs))\n",
    "    summary_lines += f'\\n\\nfinal_num_epochs = {final_num_epochs}\\n\\n'\n",
    "    \n",
    "    # Save the best epochs per fold\n",
    "    best_epochs_str = json.dumps(best_score_num_epochs)\n",
    "    summary_lines += f'best_score_num_epochs = {best_epochs_str}\\n\\n'\n",
    "    \n",
    "    # Save the best scores per fold\n",
    "    best_scores_str = json.dumps(best_scores_per_fold, indent=4)\n",
    "    summary_lines += f'best_scores_per_fold = {best_scores_str}\\n\\n'\n",
    "    \n",
    "    # Calculate the total and average runtimes across all folds\n",
    "    training_elapsed, training_elapsed_str = get_time_elapsed(training_start)\n",
    "    training_fold_elapsed_str = format_time_str(training_elapsed / N_SPLITS)\n",
    "    pred_elapsed = np.sum(pred_runtimes)\n",
    "    pred_elapsed_str = format_time_str(pred_elapsed)\n",
    "    pred_fold_elapsed_str = format_time_str(pred_elapsed / N_SPLITS)\n",
    "    summary_lines += (f'Total stratified {N_SPLITS}-fold loop runtime: '\n",
    "                      f'{training_elapsed_str}\\n'\n",
    "                      f'Average training runtime per fold: '\n",
    "                      f'{training_fold_elapsed_str}\\n\\n'\n",
    "                      f'Total stratified {N_SPLITS}-fold prediction runtime: '\n",
    "                      f'{pred_elapsed_str}\\n'\n",
    "                      f'Average prediction runtime per fold: '\n",
    "                      f'{pred_fold_elapsed_str}\\n')\n",
    "    for f, time_str in enumerate(pred_runtime_strs):\n",
    "        summary_lines += f'\\nFold {f + 1} prediction runtime: {time_str}'\n",
    "    \n",
    "    save_line_to_file(summary_lines, summary_file_path, 'w')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these variables to an existing output file\n",
    "nth_iter = 1\n",
    "nth_fold = 7\n",
    "iter_fold_scores = pd.read_csv(f'{OUTPUT_SCORES_DIR}iter_{nth_iter:02d}.'\n",
    "                               f'fold_{nth_fold:02d}.scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass in a metric (without the `val_` prefix) and a fold index to show\n",
    "# the training and validation error curves over the number of epochs\n",
    "display_metric_vs_epochs_plot(iter_fold_scores, 'loss', nth_iter, nth_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metric_vs_epochs_plot(iter_fold_scores, 'acc', nth_iter, nth_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test data\n",
    "In order to make predictions on the Kaggle-provided unlabeled test data, we will need to submit our predictions to Kaggle. It would be best to train on the entire training set; this means that, this time, we won't provide a validation set to the Keras model.\n",
    "\n",
    "How do we know how many epochs to train for? To figure this out, we can use the results from the cross validation phase. Since we have recorded the number of epochs that each fold took to train the model before stopping, we can take the average number of epochs across all folds and use that as the number of epochs to train our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the best number of epochs based on the evaluation phase\n",
    "final_num_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"batch_size\": 64,\n",
      "    \"use_gru_layer\": true,\n",
      "    \"use_global_max_pooling_layer\": true,\n",
      "    \"units\": 300,\n",
      "    \"spatial_dropout_rate\": 0.2,\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"num_rnn_stacks\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Select the best model params based on the evaluation phase\n",
    "best_iter = 58\n",
    "final_model_params = load_dictionary_from_file(f'{OUTPUT_MODELS_DIR}'\n",
    "                                               f'iter_{best_iter:02d}.params.json')\n",
    "final_batch_size = final_model_params['batch_size']\n",
    "print(json.dumps(final_model_params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the embedding layer\n",
    "embedding_layer = build_embedding_layer(embedding_matrix, \n",
    "                                        vocab_size, \n",
    "                                        EMBEDDING_DIM, \n",
    "                                        MAX_SEQUENCE_LENGTH)\n",
    "# Build the model with the best model params\n",
    "model = build_model(embedding_layer, MAX_SEQUENCE_LENGTH, final_model_params)\n",
    "# Save the model architecture, weights, and optimizer state to file\n",
    "model.save(f'{OUTPUT_MODELS_DIR}final.iter_{best_iter:02d}.model.hdf5')\n",
    "# Save the model summary to file\n",
    "save_model_summary(model,\n",
    "                   f'{OUTPUT_SUMMARIES_DIR}final.iter_{best_iter:02d}.model_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "19579/19579 [==============================] - 6s 331us/step - loss: 0.6939 - acc: 0.7001\n",
      "Epoch 2/6\n",
      "19579/19579 [==============================] - 6s 306us/step - loss: 0.4952 - acc: 0.8009\n",
      "Epoch 3/6\n",
      "19579/19579 [==============================] - 6s 303us/step - loss: 0.3915 - acc: 0.8467\n",
      "Epoch 4/6\n",
      "19579/19579 [==============================] - 6s 303us/step - loss: 0.3114 - acc: 0.8868\n",
      "Epoch 5/6\n",
      "19579/19579 [==============================] - 6s 304us/step - loss: 0.2430 - acc: 0.9142\n",
      "Epoch 6/6\n",
      "19579/19579 [==============================] - 6s 305us/step - loss: 0.1907 - acc: 0.9362\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_tokenized,\n",
    "                    y_train_encoded,\n",
    "                    batch_size=final_batch_size,\n",
    "                    epochs=final_num_epochs,\n",
    "                    verbose=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392/8392 [==============================] - 1s 129us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tokenized, batch_size=final_batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1481910e-03, 2.4586748e-03, 9.9539316e-01],\n",
       "       [9.9939013e-01, 2.1623643e-04, 3.9367584e-04],\n",
       "       [8.5466972e-04, 9.9823111e-01, 9.1420597e-04]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final submission values\n",
    "predictions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['EAP', 'HPL', 'MWS']] = predictions\n",
    "submission_num = 47\n",
    "submission_description = 'glove_rnn_iter_58_first'\n",
    "submission_filename = f'{submission_num:03d}_{submission_description}.csv'\n",
    "submission_file_path = f'{OUTPUT_SUBMISSIONS_DIR}{submission_filename}'\n",
    "submission.to_csv(submission_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the mean logloss of the Kaggle submission\n",
    "After submitting to Kaggle, we can calculate the mean logloss across the entire test dataset as follows:\n",
    "```\n",
    "Given:\n",
    "    n_test = 8392\n",
    "    %_private = 0.7\n",
    "    %_public = 0.3\n",
    "    private_logloss # Retrieve from Kaggle after submission\n",
    "    public_logloss # Retrieve from Kaggle after submission\n",
    "    \n",
    "Mean logloss = (private_logloss * n_private + public_logloss * n_public) / n_test\n",
    "             = (private_logloss * (%_private * n_test)\n",
    "                + public_logloss * (%_public * n_test))\n",
    "               / n_test\n",
    "               \n",
    "Where n_test = n_private + n_public\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Model\n",
    "Our benchmark model is the sample submission file provided by Kaggle, in which the probability distribution across the three authors is the exact same per sample, such that their probabilities are proportional to the number of samples of said author's work in the training dataset (EAP at about 40%, HPL at about 29%, and MWS at about 31%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 8392\n",
      "Number of private leaderboard samples: 5874.4\n",
      "Number of public leaderboard samples: 2517.6\n"
     ]
    }
   ],
   "source": [
    "n_test = len(submission)\n",
    "n_private = n_test * 0.7\n",
    "n_public = n_test * 0.3\n",
    "print(f'Total number of samples: {n_test}',\n",
    "      f'Number of private leaderboard samples: {n_private}',\n",
    "      f'Number of public leaderboard samples: {n_public}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logloss for the benchmark model: 1.088255\n"
     ]
    }
   ],
   "source": [
    "private_logloss = 1.09094\n",
    "public_logloss = 1.08199\n",
    "mean_logloss = (private_logloss * n_private + public_logloss * n_public) / n_test\n",
    "print(f'Mean logloss for the benchmark model: {mean_logloss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logloss: 0.47776\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_logloss(0.47272, 0.48951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "authorid-test",
   "language": "python",
   "name": "authorid-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: GloVe Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/'\n",
    "EMBEDDINGS_DIR = f'{INPUT_DIR}embeddings/'\n",
    "TRAIN_FILE_PATH = f'{INPUT_DIR}train.csv'\n",
    "TEST_FILE_PATH = f'{INPUT_DIR}test.csv'\n",
    "SAMPLE_SUBMISSION_FILE_PATH = f'{INPUT_DIR}sample_submission.csv'\n",
    "GLOVE_EMBEDDINGS_FILE_PATH = f'{EMBEDDINGS_DIR}glove.6B.300d.txt' # Try `glove.840B.300d.txt`\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_FEATURES = 12000 # The top most common words\n",
    "MAX_SEQUENCE_LENGTH = 900 # Since max number of words in a sentence is 861; try 34 as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, submission = load_data(TRAIN_FILE_PATH, \n",
    "                                    TEST_FILE_PATH, \n",
    "                                    SAMPLE_SUBMISSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25943 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Todo: Decide whether or not to perform custom text preprocessing beforehand\n",
    "# X_train_sequences = list(train['text'].apply(lambda x: process_text(x)).values)\n",
    "# X_test_sequences = list(test['text'].apply(lambda x: process_text(x)).values)\n",
    "X_train_sequences = list(train['text'].values)\n",
    "X_test_sequences = list(test['text'].values)\n",
    "\n",
    "# Tokenize and pad the sentences\n",
    "X_train_tokenized, X_test_tokenized, word_index = compute_word_index(X_train_sequences,\n",
    "                                                                     X_test_sequences,\n",
    "                                                                     MAX_FEATURES,\n",
    "                                                                     MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:37, 10712.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_glove_embeddings(GLOVE_EMBEDDINGS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vocab_size = construct_embedding_matrix(word_index, \n",
    "                                                          embeddings_index, \n",
    "                                                          EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_embedding_layer, build_cnn_model\n",
    "\n",
    "# Fix a random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target classes need to be converted to integers so that\n",
    "# EAP --> 0\n",
    "# HPL --> 1\n",
    "# MWS --> 2\n",
    "y_train_integers = integer_encode_classes(train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13051 samples, validate on 6528 samples\n",
      "Epoch 1/1\n",
      "13051/13051 [==============================] - 23s 2ms/step - loss: 0.9089 - acc: 0.5780 - val_loss: 0.8613 - val_acc: 0.6134\n",
      "Train on 13053 samples, validate on 6526 samples\n",
      "Epoch 1/1\n",
      "13053/13053 [==============================] - 23s 2ms/step - loss: 0.9101 - acc: 0.5778 - val_loss: 0.8400 - val_acc: 0.6338\n",
      "Train on 13054 samples, validate on 6525 samples\n",
      "Epoch 1/1\n",
      "13054/13054 [==============================] - 24s 2ms/step - loss: 0.9031 - acc: 0.5811 - val_loss: 0.8577 - val_acc: 0.6244\n",
      "val_loss mean and std: 0.8530 (+/- 0.0093)\n",
      "val_acc mean and std: 0.6238 (+/- 0.0083)\n",
      "loss mean and std: 0.9074 (+/- 0.0031)\n",
      "acc mean and std: 0.5790 (+/- 0.0015)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "metrics = ['val_loss', 'val_acc', 'loss', 'acc']\n",
    "cv_scores = {\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'loss': [],\n",
    "    'acc': [],\n",
    "}\n",
    "best_model = None\n",
    "\n",
    "for train_index, test_index in kfold.split(X_train_tokenized, y_train_integers):\n",
    "    # The target classes need to be one-hot encoded so that\n",
    "    # EAP --> 0 --> [1, 0, 0]\n",
    "    # HPL --> 1 --> [0, 1, 0]\n",
    "    # MWS --> 2 --> [0, 0, 1]\n",
    "    y_train_encoded = one_hot_encode_classes(y_train_integers)\n",
    "    \n",
    "    # Prepare the splits of data\n",
    "    X_train, y_train = X_train_tokenized[train_index], y_train_encoded[train_index]\n",
    "    X_valid, y_valid = X_train_tokenized[test_index], y_train_encoded[test_index]\n",
    "    \n",
    "    # Build the embedding layer\n",
    "    embedding_layer = build_embedding_layer(embedding_matrix, \n",
    "                                            vocab_size, \n",
    "                                            EMBEDDING_DIM, \n",
    "                                            MAX_SEQUENCE_LENGTH)\n",
    "    # Build the model\n",
    "    model = build_cnn_model(embedding_layer, MAX_SEQUENCE_LENGTH)\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=64, # 32, 64, 128, 256, 512\n",
    "                        epochs=1, # 100\n",
    "                        verbose=1,\n",
    "#                         callbacks=[stopper],\n",
    "                        validation_data=[X_valid, y_valid],\n",
    "                        shuffle=True)\n",
    "    # Todo: Save the best model thus far\n",
    "    best_model = model\n",
    "    # Save the scores for later evaluation\n",
    "    for metric in metrics:\n",
    "        cv_score = history.history[metric][0]\n",
    "        cv_scores[metric].append(cv_score)\n",
    "\n",
    "# Calculate mean and standard deviation across all folds' scores\n",
    "for metric in metrics:\n",
    "    mean = np.mean(cv_scores[metric])\n",
    "    std = np.std(cv_scores[metric])\n",
    "    print('{} mean and std: {:.4f} (+/- {:.4f})'.format(metric, mean, std))\n",
    "\n",
    "# stopper = EarlyStopping(monitor='val_loss',\n",
    "#                         min_delta=0,\n",
    "#                         patience=3,\n",
    "#                         verbose=0,\n",
    "#                         mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8470147880375157"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history\n",
    "\"\"\"\n",
    "{'acc': [0.9771225346821195, 0.982747603137452], # Score per epoch\n",
    " 'loss': [0.06964566658561674, 0.04653985751459787],\n",
    " 'val_acc': [0.9809708705834689, 0.981837727556194],\n",
    " 'val_loss': [0.05274372364098244, 0.0492371146594362]}\n",
    "\"\"\"\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1) # range(1, 3)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 7123478\n",
    "# estimator = KerasClassifier(build_fn=create_lstm_model, epochs=2, batch_size=32)\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# results = cross_val_score(estimator, X_train, y, cv=kfold)\n",
    "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = model.predict([X_test_padded], batch_size=1024)\n",
    "# model.load_weights('pre_trained_glove_model.h5')\n",
    "y_test_metrics = model.evaluate(X_test_padded, y_test_final, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_metrics # [loss, accuracy] # Result in percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logloss: {:.3f}'.format(calculate_logloss(y_test, y_test_final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that submission is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final submission values\n",
    "y_test_final[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final.shape == submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_SUBMISSION_FILE_PATH = '../input/temp_submission.csv'\n",
    "temp_submission = pd.read_csv(TEMP_SUBMISSION_FILE_PATH)\n",
    "\n",
    "temp_submission[['EAP', 'HPL', 'MWS']] = y_test_final\n",
    "temp_submission.to_csv('../submissions/001_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plain RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RNN model with LSTM layers from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RNN model with GRU layers from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Validation\n",
    "\n",
    "Each model will be evaluated based on the logloss metric using either 5-fold or 10-fold cross validation; the lower the logloss, the better the model. Apart from evaluation, for either the top two performing algorithms or for each algorithm, I plan to run random search to tune certain hyperparameters for each algorithm for at least 60 iterations (but I’ll lower that number if it ends up taking way too long) in order to find the best model for this multiclass classification problem of authorship attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-Form Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

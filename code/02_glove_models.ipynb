{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification: GloVe Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, submission = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "# Fit and return the encoded labels\n",
    "le = LabelEncoder()\n",
    "y_values = le.fit_transform(train['author'].values)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and testing splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['text'].values,\n",
    "                                                    y_values,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still perform custom text processing beforehand\n",
    "# list_sentences_train = train['text'].apply(lambda x: process_text(x)) # Series (19579,)\n",
    "# list_sentences_test = test['text'].apply(lambda x: process_text(x))\n",
    "list_sentences_train = list(X_train) # ndarray (15663,)\n",
    "list_sentences_test = list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23677 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_EMBEDDINGS_FILE_PATH = '../input/embeddings/glove.6B.300d.txt' # Use `glove.840B.300d.txt`\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_FEATURES = 12000 # The features are words, in this case\n",
    "# count    19579.000000\n",
    "# mean        26.730477\n",
    "# std         19.048353\n",
    "# min          2.000000\n",
    "# 25%         15.000000\n",
    "# 50%         23.000000\n",
    "# 75%         34.000000\n",
    "# max        861.000000\n",
    "# Pad documents to a max length of 900 since the max length is 861.\n",
    "# These input sequences should be padded so that they all\n",
    "# have the same length in a batch of input data.\n",
    "MAX_SEQUENCE_LENGTH = 900 # 70\n",
    "\n",
    "# Only include the top `num_words` most common words\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "# Build the word index, requiring a list argument\n",
    "tokenizer.fit_on_texts(list_sentences_train)\n",
    "\n",
    "# Turn strings into a list of lists of integer indices such as [[688, 75, 1], [...]]\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "# `data` will later be used in `model.fit` call\n",
    "# sequences is a list of lists\n",
    "# Pad with 0.0 BEFORE each sequence\n",
    "# Remove values, BEFORE each sequence, from sequences larger than `maxlen`\n",
    "# Turn a list of integers into a 2D integer tensor of shape (samples, maxlen)\n",
    "# --> (159571, 100) such as array([[0, 0, 0, 688], [0, 0, 0, 589]])\n",
    "X_train_padded = pad_sequences(list_tokenized_train, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                               padding='pre', \n",
    "                               truncating='pre', \n",
    "                               value=0.0)\n",
    "X_test_padded = pad_sequences(list_tokenized_test, \n",
    "                              maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                              padding='pre', \n",
    "                              truncating='pre', \n",
    "                              value=0.0)\n",
    "\n",
    "# Recover the computed word index, which appears as {'necessary': 1234, ...}\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:47, 8482.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the GloVe embeddings into a dictionary\n",
    "# This maps words (as strings) to their vector representation (as float vectors)\n",
    "embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove_filename'))\n",
    "f = open(GLOVE_EMBEDDINGS_FILE_PATH, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Only use this if creating a randomly initialized embedding matrix\n",
    "# np.stack() --> \n",
    "#   array([[0.32, 0.7 ],\n",
    "#          [0.42, 0.1 ]], dtype=float32)\n",
    "# all_embeddings = np.stack(embeddings_index.values())\n",
    "# embedding_mean, embedding_std = all_embeddings.mean(), all_embeddings.std()\n",
    "# print(embedding_mean, embedding_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embedding matrix using our training words `word_index` and\n",
    "# the pre-trained embeddings `embeddings_index`\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "# Create an embedding matrix with random initialization for words that aren't in GloVe,\n",
    "#   using the mean and stdev of the GloVe embeddings\n",
    "# embedding_matrix = np.random.normal(loc=embedding_mean,\n",
    "#                                     scale=embedding_std,\n",
    "#                                     size=(vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "# Loop over each of the first `MAX_FEATURES` words of the `word_index` built from\n",
    "# the dataset and retrieve its embedding vector from the GloVe `embeddings_index`\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    # Words not found in the `embeddings_index` will have their vectors in `embedding_matrix`\n",
    "    # remain as all zeros\n",
    "    # -- or --\n",
    "    # remain as a random normalization of the mean and stdev of the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Sequences of integers with input shape: (samples, indices)\n",
    "# Output: A 3D tensor of shape (samples, sequence_length, embedding_dim)\n",
    "# Layer is frozen so that its weights, the embedding vectors,\n",
    "# will not be updated during training.\n",
    "# Note: May remove `weights` and `trainable` to train the embedding\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load this embedding matrix into an Embedding layer.\n",
    "# Note that we set trainable=False to prevent the weights from being updated during training.\n",
    "\n",
    "# Custom metrics can be passed at the compilation step.\n",
    "# The function would need to take (y_true, y_pred) as arguments and return \n",
    "# a single tensor value.\n",
    "# def mean_pred(y_true, y_pred):\n",
    "#     return K.mean(y_pred)\n",
    "\n",
    "def create_cnn_model(embedding_layer):\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_layer')\n",
    "    x = embedding_layer(input_layer)\n",
    "    \n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "#     x = Conv1D(128, 5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(5)(x)\n",
    "#     x = Conv1D(128, 5, activation='relu')(x)\n",
    "#     x = MaxPooling1D(5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    output_layer = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', # Try 'rmsprop'\n",
    "                  metrics=['accuracy'])\n",
    "#                   metrics=['accuracy', mean_pred])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# Sentence vectors are passed in here\n",
    "# X_train_scaled = scaler.fit_transform(X_train) # Then converted to a numpy array with\n",
    "# X_test_scaled = scaler.transform(X_test) # np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target classes need to be one-hot encoded\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 900)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 900, 300)          7103400   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 896, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 179, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 22912)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2932864   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 10,228,779\n",
      "Trainable params: 3,125,379\n",
      "Non-trainable params: 7,103,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_cnn_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12530 samples, validate on 3133 samples\n",
      "Epoch 1/2\n",
      "12530/12530 [==============================] - 322s 26ms/step - loss: 0.8611 - acc: 0.6026 - val_loss: 0.7168 - val_acc: 0.6981\n",
      "Epoch 2/2\n",
      "12530/12530 [==============================] - 312s 25ms/step - loss: 0.5932 - acc: 0.7555 - val_loss: 0.7108 - val_acc: 0.6993\n"
     ]
    }
   ],
   "source": [
    "# stopper = EarlyStopping(monitor='val_loss',\n",
    "#                         min_delta=0,\n",
    "#                         patience=3,\n",
    "#                         verbose=0,\n",
    "#                         mode='auto')\n",
    "\n",
    "history = model.fit(X_train_padded,\n",
    "#                     X_train_scaled, # Or should it be scaled?\n",
    "                    y_train_encoded,\n",
    "                    batch_size=64, # 32, 128, 512\n",
    "                    epochs=2, # 2, 5, 100\n",
    "                    verbose=1,\n",
    "#                     callbacks=[stopper],\n",
    "                    validation_split=0.2,\n",
    "#                     validation_data=[X_test_scaled, y_test_encoded], # Overrides split\n",
    "                    shuffle=True)\n",
    "model.save_weights('./models/cnn_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.history\n",
    "\"\"\"\n",
    "{'acc': [0.9771225346821195, 0.982747603137452], # Score per epoch\n",
    " 'loss': [0.06964566658561674, 0.04653985751459787],\n",
    " 'val_acc': [0.9809708705834689, 0.981837727556194],\n",
    " 'val_loss': [0.05274372364098244, 0.0492371146594362]}\n",
    "\"\"\"\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1) # range(1, 3)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 7123478\n",
    "# estimator = KerasClassifier(build_fn=create_lstm_model, epochs=2, batch_size=32)\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# results = cross_val_score(estimator, X_train, y, cv=kfold)\n",
    "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final = model.predict([X_test_padded], batch_size=1024)\n",
    "# model.load_weights('pre_trained_glove_model.h5')\n",
    "y_test_metrics = model.evaluate(X_test_padded, y_test_final, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_metrics # [loss, accuracy] # Result in percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logloss: {:.3f}'.format(calculate_logloss(y_test, y_test_final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that submission is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final submission values\n",
    "y_test_final[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded[0:6]\n",
    "# [1, 0, 0] --> EAP\n",
    "# [0, 1, 0] --> HPL\n",
    "# [0, 0, 1] --> MWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_SUBMISSION_FILE_PATH = '../input/temp_submission.csv'\n",
    "temp_submission = pd.read_csv(TEMP_SUBMISSION_FILE_PATH)\n",
    "\n",
    "temp_submission[['EAP', 'HPL', 'MWS']] = y_test_final\n",
    "temp_submission.to_csv('../submissions/001_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plain RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RNN model with LSTM layers from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RNN model with GRU layers from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Validation\n",
    "\n",
    "Each model will be evaluated based on the logloss metric using either 5-fold or 10-fold cross validation; the lower the logloss, the better the model. Apart from evaluation, for either the top two performing algorithms or for each algorithm, I plan to run random search to tune certain hyperparameters for each algorithm for at least 60 iterations (but I’ll lower that number if it ends up taking way too long) in order to find the best model for this multiclass classification problem of authorship attribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-Form Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
